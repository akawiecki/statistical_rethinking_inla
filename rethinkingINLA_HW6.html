<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 6 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 6 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="intro-to-link-functions-from-statistical-rethinking-2nd-edition-chapter.10" class="section level1">
<h1>Intro to link functions from Statistical Rethinking 2nd edition Chapter.10</h1>
<div id="logit-link" class="section level2">
<h2>Logit link:</h2>
<p>The logit link maps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this:</p>
<p>yi ∼ Binomial(n, pi)</p>
<p>logit(pi) = α + βxi</p>
<p>And the logit function itself is defined as the log-odds:</p>
<p>logit(pi) = log (pi/(1 − pi))</p>
<p>The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is:</p>
<p>log (pi/(1 − pi)) = α + βxi</p>
<p>So to figure out the definition of pi implied here, just do a little algebra and solve the above equation for pi:</p>
<p>pi = exp(α + βxi) / 1 + exp(α + βxi)</p>
<p>The above function is usually called the <strong>logistic</strong>. In this context, it is also commonly called the <strong>inverse-logit</strong>, because it inverts the logit transform.</p>
<p>No longer does a unit change in a predictor variable produce a constant change in the mean of the outcome variable. Instead, a unit change in xi may produce a larger or smaller change in the probability pi, depending upon how far from zero the log-odds are. For example, in Figure 10.7, when x = 0 the linear model has a value of zero on the log-odds scale. A half- unit increase in x results in about a 0.25 increase in probability. But each addition half-unit will produce less and less of an increase in probability, until any increase is vanishingly small.</p>
<p>The key lesson for now is just that no regression coefficient, such as β, from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 7) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change. More generally, every predic- tor variable effectively interacts with every other predictor variable, whether you explicitly model them as interactions or not. This fact makes the visualization of counter-factual pre- dictions even more important for understanding what the model is telling you.</p>
</div>
<div id="log-link" class="section level2">
<h2>Log link:</h2>
<p>This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation σ of a Gaussian distribution so it is a function of a predictor variable x. The parameter σ must be positive, because a standard deviation cannot be negative nor can it be zero. The model might look like:</p>
<p>yi ∼ Normal(μ, σi)</p>
<p>log(σi) = α + βxi</p>
<p>In this model, the mean μ is constant, but the standard deviation scales with the value xi. A log link is both conventional and useful in this situation. It prevents σ from taking on a negative value. What the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving log(σi) = α + βxi for σi yields the inverse link:</p>
<p>σi = exp(α + βxi)</p>
<p>Using a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right). Another way to think of this relationship is to remember that logarithms are magnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the un- transformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot of Figure 10.8. While using a log link does solve the problem of constraining the parameter to be posi- tive, it may also create a problem when the model is asked to predict well outside the range of data used to fit it. Exponential relationships grow, well, exponentially. Just like a lin- ear model cannot be linear forever, an exponential model cannot be exponential forever. Human height cannot be linearly related to weight forever, because very heavy people stop getting taller and start getting wider. Likewise, the property damage caused by a hurricane may be approximately exponentially related to wind speed for smaller storms. But for very big storms, damage may be capped by the fact that everything gets destroyed.</p>
</div>
</div>
<div id="section" class="section level1">
<h1>1.</h1>
<p><strong>The data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 2010–2012 (see van der Lee and Ellemers <a href="doi:10.1073/pnas.1510159112" class="uri">doi:10.1073/pnas.1510159112</a>). These data have a very similar structure to the UCBAdmit data discussed in Chapter 11. I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWO’s goal is to equalize rates of funding between the genders, what type of intervention would be most effective?</strong></p>
<p>The implied DAG is:</p>
<pre class="r"><code>hw6.1dag &lt;- dagitty(&quot;dag{
                  A &lt;- G
                  A&lt;- D
                  D &lt;- G
                  }&quot;)
plot(hw6.1dag)</code></pre>
<pre><code>## Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.1%20dag-1.png" width="672" /></p>
<p>G is gender, D is discipline, and A is award. The direct causal effect of gender is the path G → A. The total effect includes that path and the indirect path G → D → A.</p>
<div id="a-total-causal-effect-of-gender" class="section level2">
<h2>1.a total causal effect of gender</h2>
<p>We can estimate the total causal influence (assuming this DAG is correct) with a model that conditions only on gender. I’ll use a N(-1,1) prior for the intercepts, because we know from domain knowledge that less than half of applicants get awards.</p>
<div id="a-rethinking" class="section level3">
<h3>1.a rethinking</h3>
<pre class="r"><code>data(NWOGrants)
d &lt;- NWOGrants
dat_list &lt;- list(
awards = as.integer(d$awards),
apps = as.integer(d$applications),
gid = ifelse( d$gender==&quot;m&quot; , 1L , 2L )
)

m1_total &lt;- ulam(
alist(
awards ~ binomial( apps , p ), 
logit(p) &lt;- a[gid],
a[gid] ~ normal(-1,1)
), data=dat_list , chains=4 ) </code></pre>
<pre><code>## recompiling to avoid crashing R session</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;81eae820e39db5841689425b0c0bd23c&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.9e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.010002 seconds (Warm-up)
## Chain 1:                0.009117 seconds (Sampling)
## Chain 1:                0.019119 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;81eae820e39db5841689425b0c0bd23c&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.010624 seconds (Warm-up)
## Chain 2:                0.009798 seconds (Sampling)
## Chain 2:                0.020422 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;81eae820e39db5841689425b0c0bd23c&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.00959 seconds (Warm-up)
## Chain 3:                0.010232 seconds (Sampling)
## Chain 3:                0.019822 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;81eae820e39db5841689425b0c0bd23c&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.011859 seconds (Warm-up)
## Chain 4:                0.009348 seconds (Sampling)
## Chain 4:                0.021207 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>precis(m1_total,2)</code></pre>
<pre><code>##           mean         sd      5.5%     94.5%    n_eff    Rhat4
## a[1] -1.534215 0.06419862 -1.636220 -1.433608 1270.353 1.001674
## a[2] -1.739403 0.08338316 -1.876282 -1.605273 1458.263 1.002103</code></pre>
<p>Gender 1 here is male and 2 is female. So males have higher rates of award, on average. How big is the difference? Let’s look at the contrast on absolute (penguin) scale:</p>
<pre class="r"><code>post &lt;- extract.samples(m1_total)
diff &lt;- inv_logit( post$a[,1] ) - inv_logit(post$a[,2]) 
precis( list( diff=diff ) )</code></pre>
<pre><code>##            mean         sd        5.5%      94.5% histogram
## diff 0.02787365 0.01386206 0.005582129 0.05019898 ▁▁▂▅▇▇▃▁▁</code></pre>
<p>So a small 3% difference on average. Still, with such low funding rates (in some disciplines), 3% is a big advantage.</p>
</div>
<div id="a-inla" class="section level3">
<h3>1.a inla</h3>
<pre class="r"><code>data(NWOGrants)
d &lt;- NWOGrants

d1.i &lt;- d %&gt;% 
  mutate(g_val= 1, 
         d= paste(&quot;d&quot;, as.integer(discipline), sep = &quot;.&quot;),
         d_val= 1) %&gt;% 
  spread(gender, g_val) %&gt;% 
  spread(d, d_val)

# number of trials is d1.i$applications

m1a.i &lt;- inla(awards ~ -1 + m + f , data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = applications, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean= -1, 
        prec= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m1a.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = awards ~ -1 + m + f, family = \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = applications, 
##    control.compute = list(config = T), &quot;, &quot; control.predictor = list(link = 1, compute = T), control.family = 
##    list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = -1, prec = 1))&quot;) 
## Time used:
##     Pre = 1.18, Running = 0.12, Post = 0.145, Total = 1.45 
## Fixed effects:
##     mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## m -1.532 0.065     -1.660   -1.531     -1.407 -1.530   0
## f -1.738 0.081     -1.899   -1.737     -1.581 -1.735   0
## 
## Expected number of effective parameters(stdev): 1.99(0.00)
## Number of equivalent replicates : 9.04 
## 
## Marginal log-Likelihood:  -66.79 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>#computed in the outcome scale 
m1a.i$summary.fitted.values</code></pre>
<pre><code>##                          mean          sd 0.025quant  0.5quant 0.975quant      mode
## fitted.Predictor.01 0.1499131 0.010317651  0.1301956 0.1497340  0.1706421 0.1493748
## fitted.Predictor.02 0.1779058 0.009440579  0.1597389 0.1777855  0.1967474 0.1775440
## fitted.Predictor.03 0.1499091 0.010316723  0.1301934 0.1497302  0.1706361 0.1493710
## fitted.Predictor.04 0.1779085 0.009440072  0.1597425 0.1777883  0.1967490 0.1775469
## fitted.Predictor.05 0.1499154 0.010316754  0.1301995 0.1497365  0.1706424 0.1493774
## fitted.Predictor.06 0.1778922 0.009438767  0.1597286 0.1777720  0.1967300 0.1775307
## fitted.Predictor.07 0.1499140 0.010317386  0.1301970 0.1497350  0.1706424 0.1493758
## fitted.Predictor.08 0.1778933 0.009439873  0.1597277 0.1777730  0.1967335 0.1775316
## fitted.Predictor.09 0.1779015 0.009439025  0.1597374 0.1777813  0.1967398 0.1775400
## fitted.Predictor.10 0.1499020 0.010315236  0.1301889 0.1497232  0.1706259 0.1493642
## fitted.Predictor.11 0.1499123 0.010317606  0.1301949 0.1497333  0.1706412 0.1493740
## fitted.Predictor.12 0.1779011 0.009439942  0.1597354 0.1777808  0.1967414 0.1775394
## fitted.Predictor.13 0.1499103 0.010317738  0.1301928 0.1497313  0.1706396 0.1493720
## fitted.Predictor.14 0.1779047 0.009440672  0.1597377 0.1777845  0.1967466 0.1775430
## fitted.Predictor.15 0.1498987 0.010313838  0.1301880 0.1497199  0.1706195 0.1493611
## fitted.Predictor.16 0.1778898 0.009437012  0.1597294 0.1777697  0.1967239 0.1775286
## fitted.Predictor.17 0.1499127 0.010317445  0.1301957 0.1497337  0.1706413 0.1493745
## fitted.Predictor.18 0.1778960 0.009439275  0.1597316 0.1777758  0.1967349 0.1775345</code></pre>
<pre class="r"><code>#contrast between male and female

n.samples = 1000

#pi = exp(α + βxi) / 1 + exp(α + βxi)

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

m1a.i.samples = inla.posterior.sample(n.samples, result = m1a.i)

m1a.i.contrast1 &lt;- inla.posterior.sample.eval(function(...) {
    inverse_logit(m) - 
    inverse_logit(f)
},
   m1a.i.samples)
summary(as.vector(m1a.i.contrast1))</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.02657  0.01748  0.02724  0.02733  0.03771  0.07954</code></pre>
</div>
</div>
<div id="b-direct-effect-of-gender" class="section level2">
<h2>1.b direct effect of gender</h2>
<p>Now for the direct influence of gender, we condition on discipline as well:</p>
<div id="b-rethinking" class="section level3">
<h3>1.b rethinking</h3>
<pre class="r"><code>dat_list$disc &lt;- as.integer(d$discipline) 

m1_direct &lt;- ulam(
alist(
awards ~ binomial( apps , p ), 
logit(p) &lt;- a[gid] + d[disc], 
a[gid] ~ normal(-1,1),
d[disc] ~ normal(0,1)
),
data=dat_list , chains=4 , cores=4 , iter=3000 ) 

precis(m1_direct,2)</code></pre>
<pre><code>##              mean        sd       5.5%       94.5%    n_eff    Rhat4
## a[1] -1.360704267 0.3065706 -1.8587853 -0.86263022 586.7668 1.003143
## a[2] -1.501544865 0.3128893 -2.0028811 -0.99711205 611.3386 1.002769
## d[1]  0.342608770 0.3559347 -0.2262837  0.90898810 768.4841 1.002029
## d[2]  0.020314490 0.3303936 -0.5055576  0.55236030 691.4877 1.003546
## d[3] -0.213318382 0.3260893 -0.7367684  0.31247347 705.1018 1.002649
## d[4] -0.252491812 0.3552531 -0.8202482  0.31097573 820.0334 1.001536
## d[5] -0.315338973 0.3256869 -0.8271814  0.21124122 644.0385 1.002919
## d[6]  0.001451087 0.3458133 -0.5616136  0.55976413 812.7953 1.001885
## d[7]  0.312613425 0.3807893 -0.2961343  0.92932199 977.8970 1.000619
## d[8] -0.435891933 0.3182044 -0.9471851  0.09015475 592.5001 1.002909
## d[9] -0.190623298 0.3346838 -0.7231056  0.35118272 708.5796 1.002368</code></pre>
<p>Those chains didn’t sample very efficiently. This likely because the model is over- parameterized—it has more parameters than absolutely necessary. This doesn’t break it. It just makes the sampling less efficient. Anyway, now we can compute the gender difference again. On the relative scale:</p>
<pre class="r"><code>post &lt;- extract.samples(m1_direct) 
diff_a &lt;- post$a[,1] - post$a[,2] 
precis( list( diff_a=diff_a ) )</code></pre>
<pre><code>##             mean        sd       5.5%     94.5% histogram
## diff_a 0.1408406 0.1083074 -0.0312027 0.3181228 ▁▁▂▅▇▅▁▁▁</code></pre>
<p>Still an advantage for the males, but reduced and overlapping zero a bit. To see this difference on the absolute scale, we need to account for the base rates in each discipline as well. If you look at the postcheck(m1_direct) display, you’ll see the predictive difference is very small. There are also several disciplines that reverse the advantage. If there is a direct influence of gender here, it is small, much smaller than before we accounted for discipline. Why? Because again the disciplines have different funding rates and women apply more to the disciplines with lower funding rates. But it would be hasty, I think, to conclude there are no other influences. There are after all lots of unmeasured confounds…</p>
<pre class="r"><code>postcheck(m1_direct)</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.1.b%20postcheck-1.png" width="672" /></p>
</div>
<div id="b-inla" class="section level3">
<h3>1.b inla</h3>
<pre class="r"><code>data(NWOGrants)
d &lt;- NWOGrants

d1.i &lt;- d %&gt;% 
  mutate(g_val= 1, 
         d= paste(&quot;d&quot;, as.integer(discipline), sep = &quot;.&quot;),
         d_val= 1) %&gt;% 
  spread(gender, g_val) %&gt;% 
  spread(d, d_val)

# number of trials is d1.i$applications

d_names &lt;- paste(&quot;d&quot;, 1:9, sep = &quot;.&quot;)

m1b.i &lt;- inla(awards ~ -1 + m + f + d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9, data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = applications, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean= list(m=-1,f=-1, d.1=0, d.2=0, d.3=0, d.4=0,d.5=0,d.6=0,d.7=0,d.8=0,d.9),
        prec= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m1b.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = awards ~ -1 + m + f + d.1 + d.2 + d.3 + d.4 + &quot;, &quot; d.5 + d.6 + d.7 + d.8 + d.9, family = 
##    \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = applications, control.compute = list(config = T), &quot;, &quot; control.predictor = 
##    list(link = 1, compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = 
##    list(mean = list(m = -1, f = -1, d.1 = 0, &quot;, &quot; d.2 = 0, d.3 = 0, d.4 = 0, d.5 = 0, d.6 = 0, d.7 = 0, &quot;, &quot; d.8 = 0, 
##    d.9), prec = 1))&quot;) 
## Time used:
##     Pre = 1.81, Running = 0.14, Post = 0.235, Total = 2.18 
## Fixed effects:
##       mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## m   -1.348 0.308     -1.953   -1.348     -0.745 -1.348   0
## f   -1.488 0.313     -2.102   -1.488     -0.875 -1.488   0
## d.1  0.336 0.356     -0.365    0.337      1.033  0.339   0
## d.2  0.008 0.333     -0.647    0.009      0.661  0.009   0
## d.3 -0.224 0.329     -0.870   -0.223      0.420 -0.223   0
## d.4 -0.263 0.354     -0.961   -0.262      0.428 -0.259   0
## d.5 -0.328 0.326     -0.967   -0.327      0.310 -0.327   0
## d.6 -0.008 0.349     -0.696   -0.007      0.674 -0.006   0
## d.7  0.304 0.383     -0.453    0.306      1.050  0.310   0
## d.8 -0.448 0.319     -1.074   -0.447      0.178 -0.447   0
## d.9 -0.196 0.340     -0.866   -0.196      0.469 -0.195   0
## 
## Expected number of effective parameters(stdev): 9.75(0.00)
## Number of equivalent replicates : 1.85 
## 
## Marginal log-Likelihood:  -71.14 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>#contrast between male and female

n.samples = 1000

m1b.i.samples = inla.posterior.sample(n.samples, result = m1b.i)

#On the relative scale:
m1b.i.contrast1 &lt;- inla.posterior.sample.eval(function(...) {
    m - f
},
   m1b.i.samples)
summary(as.vector(m1b.i.contrast1))</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.24428  0.05866  0.12881  0.12994  0.20067  0.53197</code></pre>
</div>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p><strong>Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from Problem 1. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the back-door criterion. Hint: This is structurally a lot like the grandparents-parents- children-neighborhoods example from a previous week. If you have trouble thinking this though, try simulating fake data, assuming your DAG is true. Then analyze it using the model from Problem 1. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence?</strong></p>
<pre class="r"><code>hw6.2dag &lt;- dagitty(&quot;dag{
                  A &lt;- G
                  A&lt;- D
                  D &lt;- G
                  D &lt;- S
                  A &lt;- S
                  }&quot;)
plot(hw6.2dag)</code></pre>
<pre><code>## Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.2%20dag-1.png" width="672" /></p>
<p>S is stage of career (unobserved). This DAG has the same structure as the grandparents-parents-children-neighborhoods example from earlier in the course. When we condition on discipline D it opens a backdoor path through S to A.</p>
<p><strong>It is not possible here to get an unconfounded estimate of gender on awards.</strong></p>
<p>Here’s a simulation to demonstrate the potential issue: This code simulates 1000 applicants. There are 2 genders (G 0/1), 2 stages of career (S 0/1), and 2 disciplines (D 0/1). Discipline 1 is chosen more by gender 1 and career stage 1. So that could mean more by males and later stage of career. Then awards A have a consistent bias towards gender 1, and discipline 1 has a higher award rate, and stage 1 also a higher award rate. If we analyze these data:</p>
<div id="a-rethinking-1" class="section level3">
<h3>2.a rethinking</h3>
<pre class="r"><code>set.seed(1913)
N &lt;- 1000
G &lt;- rbern(N)
S &lt;- rbern(N)
D &lt;- rbern( N , p=inv_logit( G + S ) )
A &lt;- rbern( N , p=inv_logit( 0.25*G + D + 2*S - 2 ) ) 
dat_sim &lt;- list( G=G , D=D , A=A )

m2_sim &lt;- ulam( alist(
A ~ bernoulli(p), 
logit(p) &lt;- a + d*D + g*G, 
c(a,d,g) ~ normal(0,1)
), data=dat_sim , chains=4 , cores=4 ) 

precis(m2_sim)</code></pre>
<pre><code>##         mean        sd       5.5%      94.5%    n_eff    Rhat4
## g  0.3396301 0.1301321  0.1257989  0.5480141 826.6339 1.003988
## d  1.2227536 0.1588039  0.9707247  1.4763537 731.3829 1.004335
## a -1.0780455 0.1421107 -1.3062138 -0.8520954 729.2983 1.004582</code></pre>
<p>The parameter g is the advantage of gender 1. It is smaller than the true advantage and the estimate straddles zero quite a lot, even with 1000 applicants.</p>
</div>
<div id="a-inla-1" class="section level3">
<h3>2.a INLA</h3>
<p>Reference: <a href="https://rpubs.com/corey_sparks/431920" class="uri">https://rpubs.com/corey_sparks/431920</a></p>
<pre class="r"><code>set.seed(1913)
N &lt;- 1000
G &lt;- rbern(N)
S &lt;- rbern(N)
D &lt;- rbern( N , p=inv_logit( G + S ) )
A &lt;- rbern( N , p=inv_logit( 0.25*G + D + 2*S - 2 ) ) 
dat_sim &lt;- list( G=G , D=D , A=A )

m2a.i &lt;- inla(A ~ D+G, data= dat_sim, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials=1 for bernoulli, logistic regression
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m2a.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = A ~ D + G, family = \&quot;binomial\&quot;, data = dat_sim, &quot;, &quot; Ntrials = 1, control.compute = list(config = 
##    T), control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1))&quot;) 
## Time used:
##     Pre = 1.18, Running = 0.239, Post = 0.183, Total = 1.6 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -1.073 0.143     -1.358   -1.072     -0.798 -1.069   0
## D            1.216 0.155      0.914    1.214      1.523  1.212   0
## G            0.345 0.132      0.085    0.345      0.604  0.345   0
## 
## Expected number of effective parameters(stdev): 2.94(0.00)
## Number of equivalent replicates : 340.19 
## 
## Marginal log-Likelihood:  -657.91 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<p><strong>It is also possible to have no gender influence and infer it by accident.</strong> Try these settings: ### 2.b rethinking</p>
<pre class="r"><code>set.seed(1913)
N &lt;- 1000
G &lt;- rbern(N)
S &lt;- rbern(N)
D &lt;- rbern( N , p=inv_logit( 2*G - S ) )
A &lt;- rbern( N , p=inv_logit( 0*G + D + S - 2 ) ) 
dat_sim2 &lt;- list( G=G , D=D , A=A )
m2_sim_2 &lt;- ulam( m2_sim , data=dat_sim2 , chains=4 , cores=4 ) 

precis(m2_sim_2,2)</code></pre>
<pre><code>##         mean        sd         5.5%      94.5%     n_eff     Rhat4
## g  0.2500214 0.1468850  0.007746612  0.4856631  931.1880 0.9999269
## d  0.2719462 0.1543328  0.023773557  0.5164615  962.4796 0.9987312
## a -1.1241445 0.1196152 -1.313980496 -0.9409501 1070.1102 1.0003390</code></pre>
<p>Now it looks like gender 1 has a consistent advantage, but in fact there is no advan- tage in the simulation.</p>
</div>
<div id="b-inla-1" class="section level3">
<h3>2.b INLA</h3>
<pre class="r"><code>set.seed(1913)
N &lt;- 1000
G &lt;- rbern(N)
S &lt;- rbern(N)
D &lt;- rbern( N , p=inv_logit( 2*G - S ) )
A &lt;- rbern( N , p=inv_logit( 0*G + D + S - 2 ) ) 
dat_sim2 &lt;- list( G=G , D=D , A=A )

m2b.i &lt;- inla(A ~ D+G, data= dat_sim2, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials=1 for bernoulli, logistic regression
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m2b.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = A ~ D + G, family = \&quot;binomial\&quot;, data = dat_sim2, &quot;, &quot; Ntrials = 1, control.compute = list(config 
##    = T), control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1))&quot;) 
## Time used:
##     Pre = 1.21, Running = 0.242, Post = 0.183, Total = 1.64 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -1.120 0.119     -1.358   -1.119     -0.889 -1.116   0
## D            0.269 0.156     -0.036    0.269      0.576  0.268   0
## G            0.251 0.151     -0.044    0.251      0.548  0.251   0
## 
## Expected number of effective parameters(stdev): 2.94(0.00)
## Number of equivalent replicates : 340.13 
## 
## Marginal log-Likelihood:  -616.04 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="section-2" class="section level1">
<h1>3.</h1>
<p><strong>The data in data(Primates301) were first introduced at the end of Chapter7. In this problem, you will consider how brain size is associated with social learning. There are three parts.</strong></p>
<div id="section-3" class="section level2">
<h2>3.1</h2>
<p><strong>First, model the number of observations of social_learning for each species as a function of the log brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the resulting posterior.</strong></p>
<p>Now we first want a model with social learning as the outcome and brain size as a predictor. For this Poisson GLM, I’m going to use a N(0,1) prior on the intercept, since we know the counts should be small.</p>
<div id="a-rethinking-2" class="section level3">
<h3>3.1.a rethinking</h3>
<pre class="r"><code>library(rethinking)
data(Primates301)
d &lt;- Primates301
d3 &lt;- d[ complete.cases( d$social_learning , d$brain , d$research_effort),]

dat &lt;- list(
soc_learn = d3$social_learning,
log_brain = standardize( log(d3$brain) ), log_effort = log(d3$research_effort)
)

m3_1 &lt;- ulam( alist(
soc_learn ~ poisson( lambda ), log(lambda) &lt;- a + bb*log_brain, a ~ normal(0,1),
bb ~ normal(0,0.5)
), data=dat , chains=4 , cores=4 ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis( m3_1 )</code></pre>
<pre><code>##        mean        sd      5.5%      94.5%    n_eff    Rhat4
## a  -1.18137 0.1197506 -1.375183 -0.9890849 362.9303 1.001218
## bb  2.76474 0.0762447  2.644873  2.8864750 380.3295 1.001877</code></pre>
<pre class="r"><code>postcheck(m3_1,window=50)</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.3.1%20re-1.png" width="672" /><img src="rethinkingINLA_HW6_files/figure-html/6.3.1%20re-2.png" width="672" /><img src="rethinkingINLA_HW6_files/figure-html/6.3.1%20re-3.png" width="672" /></p>
</div>
<div id="a-inla-2" class="section level3">
<h3>3.1.a INLA</h3>
<pre class="r"><code>library(rethinking)
data(Primates301)
d &lt;- Primates301
d3.i &lt;- d[ complete.cases( d$social_learning , d$brain , d$research_effort),] %&gt;% 
  mutate(soc_learn = social_learning,
log_brain = standardize( log(brain) ), 
log_effort = log(research_effort), 
case= row_number())

m3.1.i &lt;- inla( soc_learn~ log_brain, family=&#39;poisson&#39;,
   data=d3.i,
   control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(0.5^2)),
   control.family=list(link=&#39;log&#39;),
   control.predictor=list(link=1, compute=TRUE),
   control.compute=list(dic=TRUE, cpo=TRUE, waic=TRUE))

summary(m3.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = soc_learn ~ log_brain, family = \&quot;poisson\&quot;, data = d3.i, &quot;, &quot; control.compute = list(dic = TRUE, 
##    cpo = TRUE, waic = TRUE), &quot;, &quot; control.predictor = list(link = 1, compute = TRUE), control.family = list(link = 
##    \&quot;log\&quot;), &quot;, &quot; control.fixed = list(mean = 0, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1/(0.5^2)))&quot;) 
## Time used:
##     Pre = 1.25, Running = 0.161, Post = 0.148, Total = 1.56 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -1.190 0.116     -1.421   -1.188     -0.966 -1.185   0
## log_brain    2.778 0.074      2.634    2.778      2.924  2.777   0
## 
## Expected number of effective parameters(stdev): 1.94(0.00)
## Number of equivalent replicates : 77.20 
## 
## Deviance Information Criterion (DIC) ...............: 1237.72
## Deviance Information Criterion (DIC, saturated) ....: 1123.01
## Effective number of parameters .....................: 1.94
## 
## Watanabe-Akaike information criterion (WAIC) ...: 1405.31
## Effective number of parameters .................: 138.86
## 
## Marginal log-Likelihood:  -628.73 
## CPO and PIT are computed
## 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<p>inla postcheck</p>
<pre class="r"><code>m3.1.i.postcheck &lt;- bind_cols(d3.i[, c(&quot;case&quot;, &quot;soc_learn&quot;)], m3.1.i$summary.fitted.values)

names(m3.1.i.postcheck) &lt;- c(&quot;case&quot;, &quot;soc_learn&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;LCI&quot;, &quot;MCI&quot;, 
&quot;UCI&quot;, &quot;mode&quot;)

m3.1.i.postcheck.plot &lt;- ggplot() +
  geom_point(data= m3.1.i.postcheck, aes(x= case, y= soc_learn), color= &quot;blue&quot;, alpha= 0.5)+
  geom_pointrange(data= m3.1.i.postcheck, aes(x= case, y= mean, ymin= LCI, ymax= UCI ), color= &quot;red&quot;, alpha= 0.3, size= 0.15)+
  theme_bw()
  

m3.1.i.postcheck.plot </code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/inla%206.3.1%20postcheck-1.png" width="672" /></p>
<p>The blue points are the raw data, recall. These are not great posterior predictions. Clearly other factors are in play.</p>
<p>Let’s try the research effort variable now.</p>
</div>
</div>
<div id="section-4" class="section level2">
<h2>3.2</h2>
<p><strong>Second, some species are studied much more than others. So the number of reported instances of social_learning could be a product of research effort. Use the research_effort variable, specifically its logarithm, as an additional predictor variable. Interpret the coefficient for log research_effort. Does this model disagree with the previous one?</strong></p>
<div id="rethinking" class="section level3">
<h3>3.2 rethinking</h3>
<pre class="r"><code>library(rethinking)
data(Primates301)
d &lt;- Primates301
d3 &lt;- d[ complete.cases( d$social_learning , d$brain , d$research_effort),]

dat &lt;- list(
soc_learn = d3$social_learning,
log_brain = standardize( log(d3$brain) ), log_effort = log(d3$research_effort)
)

m3_2 &lt;- ulam( alist(
soc_learn ~ poisson( lambda ),
log(lambda) &lt;- a + be*log_effort + bb*log_brain, 
a ~ normal(0,1),
c(bb,be) ~ normal(0,0.5)
), data=dat , chains=4 , cores=4 ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis( m3_2 )</code></pre>
<pre><code>##          mean         sd       5.5%      94.5%    n_eff    Rhat4
## a  -5.9369282 0.28669021 -6.3947105 -5.4762540 412.4261 1.007162
## be  1.5264157 0.06024052  1.4260593  1.6230606 374.9833 1.006619
## bb  0.4649816 0.07766680  0.3441619  0.5863793 586.3908 1.004803</code></pre>
<pre class="r"><code>precis( m3_2)</code></pre>
<pre><code>##          mean         sd       5.5%      94.5%    n_eff    Rhat4
## a  -5.9369282 0.28669021 -6.3947105 -5.4762540 412.4261 1.007162
## be  1.5264157 0.06024052  1.4260593  1.6230606 374.9833 1.006619
## bb  0.4649816 0.07766680  0.3441619  0.5863793 586.3908 1.004803</code></pre>
<pre class="r"><code>postcheck(m3_2,window=50)</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.3.2%20rethinking-1.png" width="672" /><img src="rethinkingINLA_HW6_files/figure-html/6.3.2%20rethinking-2.png" width="672" /><img src="rethinkingINLA_HW6_files/figure-html/6.3.2%20rethinking-3.png" width="672" /></p>
<p>Brain size bb is still positively associated, but much less. Research effort be is strongly associated. To see how these models disagree, let’s use pointwise WAIC to see which cases each predicts well.</p>
</div>
<div id="inla" class="section level3">
<h3>3.2 INLA</h3>
<pre class="r"><code>library(rethinking)
data(Primates301)
d &lt;- Primates301
d3.i &lt;- d[ complete.cases( d$social_learning , d$brain , d$research_effort),] %&gt;% 
  mutate(soc_learn = social_learning,
log_brain = standardize( log(brain) ), 
log_effort = log(research_effort), 
case= row_number())

m3.2.i &lt;- inla( soc_learn~ log_brain + log_effort, family=&#39;poisson&#39;,
   data=d3.i,
   control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(0.5^2)),
   control.family=list(link=&#39;log&#39;),
   control.predictor=list(link=1, compute=TRUE),
   control.compute=list(dic=TRUE, cpo=TRUE, waic=TRUE))

summary(m3.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = soc_learn ~ log_brain + log_effort, family = \&quot;poisson\&quot;, &quot;, &quot; data = d3.i, control.compute = 
##    list(dic = TRUE, cpo = TRUE, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = TRUE), &quot;, &quot; 
##    control.family = list(link = \&quot;log\&quot;), control.fixed = list(mean = 0, &quot;, &quot; prec = 1, mean.intercept = 0, 
##    prec.intercept = 1/(0.5^2)))&quot; ) 
## Time used:
##     Pre = 1.42, Running = 0.216, Post = 0.155, Total = 1.79 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -4.833 0.239     -5.310   -4.831     -4.372 -4.825   0
## log_brain    0.614 0.080      0.461    0.613      0.774  0.611   0
## log_effort   1.300 0.054      1.195    1.300      1.406  1.299   0
## 
## Expected number of effective parameters(stdev): 2.77(0.00)
## Number of equivalent replicates : 54.26 
## 
## Deviance Information Criterion (DIC) ...............: 529.44
## Deviance Information Criterion (DIC, saturated) ....: 414.73
## Effective number of parameters .....................: 2.76
## 
## Watanabe-Akaike information criterion (WAIC) ...: 600.90
## Effective number of parameters .................: 59.68
## 
## Marginal log-Likelihood:  -317.98 
## CPO and PIT are computed
## 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>waic.i1 &lt;- m3.1.i$waic$local.waic
waic.i2 &lt;- m3.2.i$waic$local.waic

d3.i &lt;- d3.i %&gt;% 
  mutate(waic.i1= waic.i1, 
         waic.i2= waic.i2, 
         waic.diff= waic.i1-waic.i2)

waic.i.3plot &lt;- ggplot(data= d3.i, aes(x= waic.diff, y= log_effort, label= genus))+
  geom_point()+
  geom_text(aes(label=genus),hjust=0, vjust=0)+
  geom_vline(xintercept=0, linetype=&#39;longdash&#39;) +
    theme_bw()
  
  
waic.i.3plot</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/inla%206.3%20waic%20compare-1.png" width="672" /></p>
<p>Species on the right of the vertical line fit better for model m3_2, the model with research effort. These are mostly species that are studied a lot, like chimpanzees (Pan) and macaques (Macaca). The genus Pan especially has been a focus on social learning research, and its counts are inflated by this. This is a good example of how the nature of measurement influences inference. There are likely a lot of false zeros in these data, species that are not studied often enough to get a good idea of their learning tendencies. Meanwhile every time a chimpanzee sneezes, someone writes a social learning paper.</p>
</div>
<div id="section-5" class="section level3">
<h3>3.3</h3>
<p><strong>3. Third, draw a DAG to represent how you think the variables social_learning, brain, and research_effort interact. Justify the DAG with the measured associations in the two models above (and any other models you used).</strong></p>
<p>The implied DAG is:</p>
<pre class="r"><code>hw6.3dag &lt;- dagitty(&quot;dag{
                  S &lt;- E
                  S&lt;- B
                  E &lt;- B
                  }&quot;)
plot(hw6.3dag)</code></pre>
<pre><code>## Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.</code></pre>
<p><img src="rethinkingINLA_HW6_files/figure-html/6.3.3%20dag-1.png" width="672" /></p>
<p>B is brain size, E is research effort, and S is social learning. Research effort doesn’t actually influence social learning, but it does influence the value of the variable. The model results above are consistent with this DAG in the sense that including E reduced the association with B, which we would expect when we close the indirect path through E. If researchers choose to look for social learning in species with large brains, this leads to an exaggerated estimate of the association between brains and social learning</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
