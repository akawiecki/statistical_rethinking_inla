<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking Homework 2 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking Homework 2 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="intro-to-linear-prediction-from-statistical-rethinking-2nd-edition-chapter-4." class="section level1">
<h1>Intro to linear prediction from Statistical Rethinking 2nd edition Chapter 4.</h1>
<p><strong>Finding the posterior distribution</strong></p>
<p>Bayesian updating will allow us to consider every possible combination of values for μ and σ and to score each combination by its relative plausibility, in light of the data. These relative plausibilities are the posterior probabilities of each combination of values μ, σ. Posterior plausibility provides a measure of the logical compatibility of each possible distribution with the data and model.</p>
<p>The thing to worry about is keeping in mind that the “estimate” here will be the entire posterior distribution, not any point within it. And as a result, the posterior distribution will be a distribution of Gaussian distributions. Yes, a distribution of distributions.</p>
<p>The prior for μ is a broad Gaussian prior, centered on 178cm, with 95% of probability between 178 ± 40.</p>
<p>The weights that interest us are all adult weights, so we can analyze only the adults and make an okay linear approximation.</p>
<pre class="r"><code>data(Howell1)
d &lt;- Howell1
d2 &lt;- d[ d$age &gt;= 18 , ] 
xbar &lt;- mean(d2$weight) </code></pre>
<p>The golem is assuming that the average height (not each individual height) is almost certainly between 140 cm and 220 cm</p>
<pre class="r"><code>#plot mean prior
 curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.12-1.png" width="672" /> The σ prior is a truly flat prior, a uniform one, that functions just to constrain σ to have positive probability between zero and 50cm. A standard deviation of 50cm would imply that 95% of individual heights lie within 100cm of the average height. That’s a very large range.</p>
<pre class="r"><code>#plot sd prior
  curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.13-1.png" width="672" /></p>
<p>The prior predictive simulation is an essential part of your modeling. Once you’ve chosen priors for h, μ, and σ, these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices. Lots of conventional choices are indeed bad ones, and we’ll be able to see this through prior predictive simulations.</p>
<p>Okay, so how to do this? You can quickly simulate heights by sampling from the prior.</p>
<pre class="r"><code>sample_mu &lt;- rnorm( 1e4 , 178 , 20 )
sample_sigma &lt;- runif( 1e4 , 0 , 50 )
prior_h &lt;- rnorm( 1e4 , sample_mu , sample_sigma )
#prior_h 
dens( prior_h )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.15-1.png" width="672" /></p>
<p>This is the expected distribution of heights, averaged over the prior. Notice that the prior probability distribution of height is not itself Gaussian. This is okay. The distribution you see is not an empirical expectation, but rather the distribution of relative plausibilities of different heights, before seeing the data.</p>
<p>μ i = α + β ( x i − ̄x )</p>
<p>What this tells the regression golem is that you are asking two questions about the mean of the outcome.</p>
<ol style="list-style-type: decimal">
<li><p>What is the expected height when xi = ̄x? The parameter α answers this question, because when xi = ̄x, μi = α. For this reason, α is often called the intercept. But we should think not in terms of some abstract line, but rather in terms of the meaning with respect to the observable variables.</p></li>
<li><p>What is the change in expected height, when xi changes by 1 unit? The parameter β answers this question. It is often called a “slope,” again because of the abstract line. Better to think of it as a rate of change in expectation. Jointly these two parameters ask the golem to find a line that relates x to h, a line that passes through α when xi = ̄x and has slope β. That is a task that golems are very good at. It’s up to you, though, to be sure it’s a good question.</p></li>
</ol>
<p>The goal is to simulate heights from the model, using only the priors. First, let’s consider a range of weight values to simulate over. The range of observed weights will do fine. Then we need to simulate a bunch of lines, the lines implied by the priors for α and β. Now we have 100 pairs of α and β values. Now to plot the lines:</p>
<pre class="r"><code>set.seed(2971)
N &lt;- 100                   # 100 lines
a &lt;- rnorm( N , 178 , 20 )
b &lt;- rnorm( N , 0 , 10 )
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
    xlab=&quot;weight&quot; , ylab=&quot;height&quot; )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( &quot;b ~ dnorm(0,10)&quot; )
xbar &lt;- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
    from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
    col=col.alpha(&quot;black&quot;,0.2) )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.38-1.png" width="672" /></p>
<p>If the logarithm of β is normal, then β itself is strictly positive. The reason is that exp(x) is greater than zero for any real number x. This is the reason that Log-Normal priors are commonplace. They are an easy way to enforce positive relationships</p>
<pre class="r"><code>set.seed(2971)
N &lt;- 100                   # 100 lines
a &lt;- rnorm( N , 178 , 20 )
b &lt;- rlnorm( N , 0 , 1 )
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
    xlab=&quot;weight&quot; , ylab=&quot;height&quot; )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( &quot;b ~ dnorm(0,10)&quot; )
xbar &lt;- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
    from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
    col=col.alpha(&quot;black&quot;,0.2) )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.41-1.png" width="672" /></p>
<p><strong>Overthinking: Logs and exps, oh my.</strong> My experience is that many natural and social scientists have naturally forgotten whatever they once knew about logarithms. Logarithms appear all the time in applied statistics. You can usefully think of y = log(x) as assigning to y the order of magnitude of x. The function x = exp(y) is the reverse, turning a magnitude into a value. These definitions will make a mathematician shriek. But much of our computational work relies only on these intuitions. These definitions allow the Log-Normal prior for β to be coded another way. Instead of defining a parameter β, we define a parameter that is the logarithm of β and then assign it a normal distribution. Then we can reverse the logarithm inside the linear model. It looks like this:</p>
<pre class="r"><code>m4.3b &lt;- quap( alist(
height ~ dnorm( mu , sigma ) ,
mu &lt;- a + exp(log_b)*( weight - xbar ), a ~ dnorm( 178 , 20 ) ,
log_b ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )</code></pre>
<p>Note the exp(log_b) in the definition of mu. This is the same model as m4.3. It will make the same predictions. But instead of β in the posterior distribution, you get log(β). It is easy to translate between the two, because β = exp(log(β)). In code form: b &lt;- exp(log_b).</p>
<p><strong>Interpreting the posterior distribution</strong>. One trouble with statistical models is that they are hard to understand. Once you’ve fit the model, it can only report posterior distribution. This is the right answer to the question you asked. But it’s your responsibility to process the answer and make sense of it.</p>
<pre class="r"><code>m4.3 &lt;- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu &lt;- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
), data=d2 )
precis(m4.3)</code></pre>
<pre><code>##              mean        sd        5.5%       94.5%
## a     154.6013713 0.2703075 154.1693677 155.0333748
## b       0.9032763 0.0419236   0.8362742   0.9702783
## sigma   5.0718775 0.1911545   4.7663757   5.3773792</code></pre>
<p>The first row gives the quadratic approximation for α, the second the approximation for β, and the third approximation for σ. Let’s try to make some sense of them.</p>
<p>Let’s focus on b (β), because it’s the new parameter. Since β is a slope, the value 0.90 can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. That suggests that β values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.</p>
<p>You can see the covariances among the parameters with vcov:</p>
<pre class="r"><code>round( vcov( m4.3 ) , 3 )</code></pre>
<pre><code>##           a     b sigma
## a     0.073 0.000 0.000
## b     0.000 0.002 0.000
## sigma 0.000 0.000 0.037</code></pre>
<pre class="r"><code># shows both the marginal posteriors and the covariance.
pairs(m4.3)</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.45-1.png" width="672" /></p>
<p>Very little covariation among the parameters in this case. The lack of covariance among the parameters results from centering.</p>
<p><strong>Plotting posterior inference against the data</strong>. It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified. But even if you only treat plots as a way to help in interpreting the posterior, they are invaluable.</p>
<p>Each point in this plot is a single individual. The black line is defined by the mean slope β and mean intercept α = the posterior mean line. It looks highly plausible. But there an infinite number of other highly plausible lines near it. Let’s draw those too.</p>
<pre class="r"><code>plot( height ~ weight , data=d2 , col=rangi2 )
post &lt;- extract.samples( m4.3 )
a_map &lt;- mean(post$a)
b_map &lt;- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.46-1.png" width="672" /> post &lt;- extract.samples( m4.3 ) = Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The paired values of a and b on each row define a line. The average of very many of these lines is the posterior mean line. But the scatter around that average is meaningful, because it alters our confidence in the relationship between the predictor and the outcome.</p>
<p>Let’s display a bunch of these lines, so you can see the scatter. This lesson will be easier to appreciate, if we use only some of the data to begin. Then you can see how adding in more data changes the scatter of the lines. So we’ll begin with just the first 10 cases in d2. The following code extracts the first 10 cases and re-estimates the model:</p>
<pre class="r"><code>N &lt;- 10
dN &lt;- d2[ 1:N , ]
mN &lt;- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu &lt;- a + b*( weight - mean(weight) ) ,
        a ~ dnorm( 178 , 20 ) ,
        b ~ dlnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 50 )
) , data=dN )</code></pre>
<p>Now let’s plot 20 of these lines, to see what the uncertainty looks like.</p>
<pre class="r"><code># extract 20 samples from the posterior
post &lt;- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab=&quot;weight&quot; , ylab=&quot;height&quot; )
mtext(concat(&quot;N = &quot;,N))
# plot the lines, with transparency
for ( i in 1:20 )
    curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
        col=col.alpha(&quot;black&quot;,0.3) , add=TRUE )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.49-1.png" width="672" /> Increae the amounts of data. Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean.</p>
<pre class="r"><code>N &lt;- 352
dN &lt;- d2[ 1:N , ]
mN &lt;- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu &lt;- a + b*( weight - mean(weight) ) ,
        a ~ dnorm( 178 , 20 ) ,
        b ~ dlnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 50 )
) , data=dN )
# extract 20 samples from the posterior
post &lt;- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab=&quot;weight&quot; , ylab=&quot;height&quot; )
mtext(concat(&quot;N = &quot;,N))
# plot the lines, with transparency
for ( i in 1:20 )
    curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
        col=col.alpha(&quot;black&quot;,0.3) , add=TRUE )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.50-1.png" width="672" /></p>
<p>Focus for the moment on a single weight value, say 50 kilograms. You can quickly make a list of 10,000 values of μ (height)for an individual who weighs 50 kilograms, by using your samples from the posterior.</p>
<p>μ i = α + β ( x i − ̄x ) The value of xi in this case is 50.</p>
<p>mu_at_50 is a vector of predicted means, one for each random sample from the posterior. Since joint a and b went into computing each, the variation across those means incorporates the uncertainty in and correlation between both parameters. It might be helpful at this point to actually plot the density for this vector of means.</p>
<p>Since the components of μ have distributions, so too does μ. And since the distributions of α and β are Gaussian, so to is the distribution of μ (adding Gaussian distributions always produces a Gaussian distribution). Since the posterior for μ is a distribution, you can find intervals for it, just like for any posterior distribution. The central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.</p>
<pre class="r"><code>post &lt;- extract.samples( m4.3 )
mu_at_50 &lt;- post$a + post$b * ( 50 - xbar )
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab=&quot;mu|weight=50&quot; )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.50%20dens-1.png" width="672" /></p>
<pre class="r"><code>PI( mu_at_50 , prob=0.89 )</code></pre>
<pre><code>##       5%      94% 
## 158.5677 159.6695</code></pre>
<p>That’s good so far, but we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the average slope.</p>
<p>This is made simple by strategic use of the link function, a part of the rethinking package. What link will do is take your quap approximation, sample from the posterior distribution, and then compute μ for each case in the data and sample from the posterior distribution. Here’s what it looks like for the data you used to fit the model:</p>
<pre class="r"><code>mu &lt;- link( m4.3 )
str(mu)</code></pre>
<pre><code>##  num [1:1000, 1:352] 157 157 157 157 157 ...</code></pre>
<p>You end up with a big matrix of values of μ. Each row is a sample from the posterior distribu- tion. There are 352 rows in d2, corresponding to 352 individuals. So there are 352 columns in the matrix mu above. link takes 1000 samples of the posterior dist. for every value in the data (of weight in this case).</p>
<div id="this-is-what-the-rethinkinglink-function-does" class="section level3">
<h3><strong>this is what the rethinking::link function does</strong>:</h3>
<pre class="r"><code>post &lt;- extract.samples(m4.3)
mu.link &lt;- function(weight) post$a + post$b*( weight - xbar )
weight.seq &lt;- seq( from=25 , to=70 , by=1 )
mu &lt;- sapply( weight.seq , mu.link )
mu.mean &lt;- apply( mu , 2 , mean )
mu.CI &lt;- apply( mu , 2 , PI , prob=0.89 )</code></pre>
<p>The function link provides a posterior distribution of μ for each case we feed it. So above we have a distribution of μ for each individual in the original data. We actually want something slightly different: a distribution of μ for each unique weight value on the horizontal axis.</p>
<pre class="r"><code># define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq &lt;- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu &lt;- link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)</code></pre>
<pre><code>##  num [1:1000, 1:46] 137 136 138 136 136 ...</code></pre>
<p>And now there are only 46 columns in mu, because we fed it 46 different values for weight.</p>
<p>To visualize what you’ve got here, let’s plot the distribution of μ values at each height.</p>
<pre class="r"><code># use type=&quot;n&quot; to hide raw data
plot( height ~ weight , d2 ) #, type=&quot;n&quot; )
# loop over samples and plot each mu value
for ( i in 1:100 )
    points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.55-1.png" width="672" /></p>
<p>At each weight value in weight.seq, a pile of computed μ values are shown. Each of these piles is a Gaussian distribution, the amount of uncertainty in μ depends upon the value of weight.</p>
<p>The final step is to summarize the distribution for each weight value. We’ll use apply, which applies a function of your choice to a matrix.</p>
<p>mu.mean contains the average μ at each weight value, and mu.PI contains 89% lower and upper bounds for each weight value.</p>
<pre class="r"><code># summarize the distribution of mu
#compute the mean of each column (dimension “2”) of the matrix mu.
mu.mean &lt;- apply( mu , 2 , mean )
mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 )
# plot raw data
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89% PI
shade( mu.PI , weight.seq )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.56-1.png" width="672" /></p>
<p>To summarize, here’s the recipe for generating predictions and intervals from the poste- rior of a fit model.</p>
<ol style="list-style-type: decimal">
<li><p>Use link to generate distributions of posterior values for μ. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.</p></li>
<li><p>Use summary functions like mean or PI to find averages and lower and upper bounds of μ for each value of the predictor variable.</p></li>
<li><p>Finally,use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It’s really up to you.</p></li>
</ol>
<p>This recipe works for every model we fit in the book. As long as you know the structure of the model—how parameters relate to the data—you can use samples from the posterior to describe any aspect of the model’s behavior.</p>
<p><strong>Prediction intervals</strong> Now let’s walk through generating an 89% prediction in- terval for actual heights, not just the average height, μ. This means we’ll incorporate the standard deviation σ and its uncertainty as well. Remember, the first line of the statistical model here is:</p>
<p>hi ∼ Normal(μi, σ)</p>
<p>What you’ve done so far is just use samples from the posterior to visualize the uncertainty in μi, the linear model of the mean. But actual predictions of heights depend also upon the distribution in the first line. The Gaussian distribution on the first line tells us that the model expects observed heights to be distributed around μ, not right on top of it. And the spread around μ is governed by σ. All of this suggests we need to incorporate σ in the predictions somehow.</p>
<p>Here’s how you do it. Imagine simulating heights. For any unique weight value, you sample from a Gaussian distribution with the correct mean μ for that weight, using the correct value of σ sampled from the same posterior distribution. If you do this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this.</p>
<p>This matrix is much like the earlier one, mu, but it contains simulated heights, not distributions of plausible average height, μ.</p>
<p>We can summarize these simulated heights in the same way we summarized the distributions of μ, by using the PI function over apply.</p>
<pre class="r"><code>sim.height &lt;- sim( m4.3 , data=list(weight=weight.seq) )
#This matrix is much like the earlier one, mu, but it contains simulated heights, not distributions of plausible average height, μ.
str(sim.height)</code></pre>
<pre><code>##  num [1:1000, 1:46] 145 126 137 137 142 ...</code></pre>
<pre class="r"><code>#height.PI contains the 89% posterior prediction interval of observable (according to the model) heights, across the values of weight in weight.seq
height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 )</code></pre>
<p>Let’s plot everything we’ve built up: (1) the average line (2) the shaded region of 89% plausible μ (3) the boundaries of the simulated heights the model expects.</p>
<pre class="r"><code># plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line : the average line of all the mean heights expected by the linear model for each weight
#mu.link &lt;- function(weight) post$a + post$b*( weight - xbar )
#weight.seq &lt;- seq( from=25 , to=70 , by=1 )
#mu &lt;- sapply( weight.seq , mu.link )
#mu.mean &lt;- apply( mu , 2 , mean )
lines( weight.seq , mu.mean )
# draw 89% region for line
shade( mu.PI , weight.seq )
# draw PI region for simulated heights:
#the area within which the model expects to find 89% of actual heights in the population, at each weight.
shade( height.PI , weight.seq )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.61-1.png" width="672" /></p>
<p>89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded in- terval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.</p>
</div>
<div id="this-is-what-the-rethinkingsim-function-does" class="section level3">
<h3><strong>this is what the rethinking::sim function does</strong></h3>
<pre class="r"><code>#extract samples automatically extracts 1000 from posterior dist
post &lt;- extract.samples(m4.3)
weight.seq &lt;- 25:70
sim.function &lt;- function(weight)
    rnorm(
        n=nrow(post) ,
        mean=post$a + post$b*( weight - xbar ) ,
        sd=post$sigma )
sim.height &lt;- sapply( weight.seq , sim.function )
##compute the mean of each column (dimension “2”) of the matrix mu.
height.interval &lt;- apply(sim.height, 2, quantile, c( 0.05 , 0.94 ))
#example of compatibility interval for 1st col (first weight)
#quantile(sim.height[,1],c( 0.05 , 0.94 ) )
# plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line
lines( weight.seq , mu.mean )
# draw HPDI region for line
shade( mu.PI , weight.seq )
# draw PI region for simulated heights
shade( height.interval, weight.seq )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/4.63-1.png" width="672" /></p>
<p><strong>Rethinking: Two kinds of uncertainty.</strong> In the procedure above, we encountered both uncertainty in parameter values and uncertainty in a sampling process. These are distinct concepts, even though they are processed much the same way and end up blended together in the posterior predictive simu- lation. The posterior distribution is a ranking of the relative plausibilities of every possible combina- tion of parameter values. The distribution of simulated outcomes, like height, is instead a distribution that includes sampling variation from some process that generates Gaussian random variables. This sampling variation is still a model assumption. It’s no more or less objective than the posterior distribution. Both kinds of uncertainty matter, at least sometimes. But it’s important to keep them straight, because they depend upon different model assumptions.</p>
</div>
</div>
<div id="homework-2" class="section level1">
<h1>HOMEWORK 2</h1>
<div id="section" class="section level2">
<h2>1.</h2>
<p><strong>The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% compatibility intervals for each of these individuals, using model-based predictions.</strong></p>
<pre class="r"><code>ind &lt;- 1:5
weight &lt;- c(45, 40, 65, 31, 53)
expected_height &lt;- NA
interval &lt;- NA
kung &lt;- bind_cols(ind, weight, expected_height, interval)</code></pre>
<pre><code>## New names:
## * NA -&gt; ...1
## * NA -&gt; ...2
## * NA -&gt; ...3
## * NA -&gt; ...4</code></pre>
<pre class="r"><code>colnames(kung) &lt;- c(&quot;individual&quot;, &quot;weight&quot;, &quot;expected_height&quot;, &quot;89%interval&quot;)
kable(kung)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">individual</th>
<th align="right">weight</th>
<th align="left">expected_height</th>
<th align="left">89%interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">45</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">40</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">65</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">31</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">53</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
</tbody>
</table>
</div>
<div id="rethinking" class="section level2">
<h2>1. rethinking</h2>
<pre class="r"><code>m4.3 &lt;- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu &lt;- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
), data=d2 )
precis(m4.3)</code></pre>
<pre><code>##              mean         sd        5.5%       94.5%
## a     154.6013672 0.27030763 154.1693634 155.0333710
## b       0.9032809 0.04192363   0.8362788   0.9702829
## sigma   5.0718803 0.19115473   4.7663781   5.3773825</code></pre>
<p>Now we need posterior predictions for each case in the table. Easiest way to do this is to use sim. We need sim, not just link, because we are trying to predict an individual’s height. So the relevant compatibility interval includes the Gaussian variance from sigma. If you provided only the compatibility interval for μ, that’s okay. But be sure you understand the difference.</p>
<p><strong>Solution using rethinking functions</strong></p>
<pre class="r"><code>dat &lt;-  data.frame(weight= c(45, 40, 65, 31, 53))
h_sim &lt;- sim( m4.3 , data=dat )
Eh &lt;- apply(h_sim,2,mean)
h_ci &lt;- apply(h_sim,2,PI,prob=0.89)
dat$Eh &lt;- Eh 
dat$L89 &lt;- h_ci[1,] 
dat$U89 &lt;- h_ci[2,] 
round(dat,1)</code></pre>
<pre><code>##   weight    Eh   L89   U89
## 1     45 154.6 146.6 163.0
## 2     40 149.9 141.8 157.9
## 3     65 172.7 164.5 181.1
## 4     31 141.9 133.6 150.2
## 5     53 162.1 154.2 169.8</code></pre>
<p><strong>Solution using base r functions</strong></p>
<pre class="r"><code>weight= c(45, 40, 65, 31, 53)
sim.hw2.fun &lt;- function(weight, model) {
  post.hw2 = extract.samples(model)
  rnorm(
    n= nrow(post),
    mean= post$a + post$b*(weight - xbar),
    sd= post$sigma)
}
sim.hw2 &lt;- sapply(weight, sim.hw2.fun , m4.3)
hw2.mean &lt;- apply(sim.hw2,2, mean)
hw2.ci &lt;- apply(sim.hw2,2, quantile, c(0.05, 0.95))
hw2.1 &lt;- data.frame(weight= c(45, 40, 65, 31, 53)) %&gt;% 
  mutate(expected_height= hw2.mean, 
         LCI= hw2.ci[1], 
         UCI = hw2.ci[2])
print(hw2.1)</code></pre>
<pre><code>##   weight expected_height      LCI      UCI
## 1     45        154.5601 146.0763 162.9388
## 2     40        150.1655 146.0763 162.9388
## 3     65        172.7602 146.0763 162.9388
## 4     31        141.9387 146.0763 162.9388
## 5     53        161.8957 146.0763 162.9388</code></pre>
</div>
<div id="inla" class="section level2">
<h2>1. inla</h2>
<pre class="r"><code>library(brinla)
library(&quot;inlabru&quot;)</code></pre>
<p>The default mean and precision for fixed effects are:</p>
<pre class="r"><code>inla.set.control.fixed.default()[c(&#39;mean&#39;,&#39;prec&#39;)]</code></pre>
<pre><code>## $mean
## [1] 0
## 
## $prec
## [1] 0.001</code></pre>
<p>We see that the default prior on beta is normal with mean zero and precision 0.001. The precision is the inverse of the variance. We convert this to SD:</p>
<pre class="r"><code>sqrt(1/0.001)</code></pre>
<pre><code>## [1] 31.62278</code></pre>
<p>We wish to predict the response at a new set of inputs. We add a case for the new inputs (weight= c(45, 40, 65, 31, 53)) and set the response to missing (height=NA):</p>
<pre class="r"><code>data(Howell1)
d &lt;- Howell1
d2 &lt;- d[ d$age &gt;= 18 , ]
dat &lt;-  data.frame(weight= c(45, 40, 65, 31, 53))
xbar &lt;- mean(d2$weight) 
new_w &lt;- bind_cols(weight, NA)</code></pre>
<pre><code>## New names:
## * NA -&gt; ...1
## * NA -&gt; ...2</code></pre>
<pre class="r"><code>colnames(new_w) &lt;- c(&quot;weight&quot;, &quot;height&quot;)
# add the weight values of interest to the dataframe
d1.i &lt;- d2  %&gt;% 
  select(c(&quot;weight&quot;, &quot;height&quot;)) %&gt;%  
  rbind(new_w) %&gt;% 
  mutate(w= weight-xbar) %&gt;% 
  select( c(&quot;height&quot;, &quot;w&quot;))
#indices of the weights with missing values of height 
d1.i.na &lt;- which(is.na(d1.i$height))
#We need to set the control.predictor to compute the posterior means of the linear predictors
m1.i&lt;- inla(height ~ w, data= d1.i,
            control.fixed = list(
        mean= 0, 
        prec= 1, 
        mean.intercept= 178, 
        prec.intercept= 1/(20^2)),# sd = 20 --&gt; precision =1/variance --&gt; 1/(sd^2)
        control.compute = list(config= TRUE),
        control.predictor=list(compute=TRUE)
)
# posterior means and SDs from the INLA fit
m1.i$summary.fixed[,1:2]</code></pre>
<pre><code>##                    mean         sd
## (Intercept) 154.6013921 0.27107172
## w             0.9034319 0.04200942</code></pre>
<pre class="r"><code># the summary statistics of the fitted values for the weight values of interest can be shown using index of the values with the missing height
m1.i$summary.fitted.values[d1.i.na, ]</code></pre>
<pre><code>##                          mean        sd 0.025quant 0.5quant 0.975quant     mode
## fitted.Predictor.353 154.6100 0.2710729   154.0782 154.6100   155.1417 154.6100
## fitted.Predictor.354 150.0928 0.3426798   149.4206 150.0928   150.7651 150.0927
## fitted.Predictor.355 172.6786 0.8831834   170.9456 172.6787   174.4108 172.6789
## fitted.Predictor.356 141.9619 0.6472126   140.6924 141.9619   143.2316 141.9617
## fitted.Predictor.357 161.8374 0.4320724   160.9897 161.8375   162.6849 161.8375</code></pre>
</div>
<div id="section-1" class="section level2">
<h2>2.</h2>
<p>Model the relationship between height(cm) and the natural logarithm of weight (log-kg): log(weight). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Use any model type from Chapter 4 that you think useful: an ordinary linear regression, a polynomial or a spline. Plot the posterior predictions against the raw data.</p>
<div id="rethinking-1" class="section level3">
<h3>2.rethinking</h3>
<pre class="r"><code>library(rethinking)
data(Howell1)
d &lt;- Howell1
d$log_weight &lt;- log(d$weight) 
xbar &lt;- mean(d$log_weight) 
m2 &lt;- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu &lt;- a + b*( log_weight - xbar ) , 
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
), data=d )
precis(m2)</code></pre>
<pre><code>##             mean        sd       5.5%      94.5%
## a     138.268410 0.2201360 137.916591 138.620230
## b      47.071138 0.3826317  46.459618  47.682657
## sigma   5.134716 0.1556694   4.885926   5.383506</code></pre>
<pre class="r"><code>plot( d$weight , d$height , col=col.alpha(rangi2,0.7) ) 
x_seq &lt;- log(1:60)
mu &lt;- sim( m2 , data=list(log_weight=x_seq) )
mu_mean &lt;- apply(mu,2,mean)
mu_ci &lt;- apply(mu,2,PI,0.99) 
lines( exp(x_seq) , mu_mean ) 
shade( mu_ci , exp(x_seq) )</code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/hw2%202%20re-1.png" width="672" /></p>
<p>You could certainly do better—the trend is under-predicting in the mid ages. But just taking the log of weight does most of the work. Why? It’ll help to think of a human body as a cylinder. Roughly. The weight of a cylinder is proportional to its volume. And the volume of a cylinder is: V = πr2h where r is the radius and h is the height. As the cylinder, uh human, gets taller, the radius gets bigger. So we can just say the radius is some fraction α of the height: Substituting that in: r = αh V = πα2h3 = kh3 where k = πα2 is just some proportionality constant.</p>
</div>
<div id="inla-1" class="section level3">
<h3>2.INLA</h3>
<pre class="r"><code>library(rethinking)
library(brinla)
library(INLA)
library(tidyverse)

data(Howell1)
d &lt;- Howell1
log_wplot &lt;- log(1:60)
new_logw &lt;- bind_cols(log_wplot, NA)</code></pre>
<pre><code>## New names:
## * NA -&gt; ...1
## * NA -&gt; ...2</code></pre>
<pre class="r"><code>colnames(new_logw) &lt;- c(&quot;log_weight&quot;, &quot;height&quot;)

# add the weight values of interest to the dataframe
d2.i &lt;- d %&gt;% 
  mutate(log_weight= log(weight)) %&gt;% 
  select(c(&quot;log_weight&quot;, &quot;height&quot;)) %&gt;%  
  rbind(new_logw) %&gt;% 
  mutate(xbar= mean(log_weight),
         log_w= log_weight - xbar)

m2.i&lt;- inla(height ~ f(log_w, model = &quot;clinear&quot;, range=c(0, Inf),
                         hyper=list(theta=list(prior = &quot;normal&quot;, param=c(0, 1)))), data= d2.i,
            control.fixed = list(
        mean.intercept= 178, 
        prec.intercept= 1/(20^2)),# sd = 20 --&gt; precision =1/variance --&gt; 1/(sd^2)
        control.compute = list(config= TRUE),
        control.predictor=list(compute=TRUE), 
        control.inla = list(h = 0.0001), 
        verbose = TRUE
)

summary(m2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = height ~ f(log_w, model = \&quot;clinear\&quot;, range = c(0, &quot;, &quot; Inf), hyper = 
##    list(theta = list(prior = \&quot;normal\&quot;, param = c(0, &quot;, &quot; 1)))), data = d2.i, verbose = 
##    TRUE, control.compute = list(config = TRUE), &quot;, &quot; control.predictor = list(compute = 
##    TRUE), control.inla = list(h = 1e-04), &quot;, &quot; control.fixed = list(mean.intercept = 178, 
##    prec.intercept = 1/(20^2)))&quot; ) 
## Time used:
##     Pre = 1.59, Running = 8.92, Post = 1.75, Total = 12.3 
## Fixed effects:
##                mean    sd 0.025quant 0.5quant 0.975quant    mode kld
## (Intercept) 136.873 0.221    136.439  136.873    137.306 136.873   0
## 
## Random effects:
##   Name     Model
##     log_w Constrained linear
## 
## Model hyperparameters:
##                                           mean    sd 0.025quant 0.5quant 0.975quant   mode
## Precision for the Gaussian observations  0.019 0.001      0.017    0.019      0.021  0.018
## Beta for log_w                          38.943 0.569     38.119   38.847     40.277 38.480
## 
## Expected number of effective parameters(stdev): 1.00(0.00)
## Number of equivalent replicates : 543.99 
## 
## Marginal log-Likelihood:  -1876.01 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.i)</code></pre>
<pre><code>##                                       mean        sd    q0.025      q0.5   q0.975     mode
## SD for the Gaussian observations  7.308269 0.2097979  6.829608  7.340101  7.61790  7.48945
## Beta for log_w                   38.943357 0.5687572 38.129274 38.844439 40.26721 38.43846</code></pre>
<pre class="r"><code>#indices of the weights with missing values of height 
d2.i.na &lt;- which(is.na(d2.i$height))

# the summary statistics of the fitted values for the weight values of interest can be shown using index of the values with the missing height m2.i$summary.fitted.values[d2.i.na, ]
m2.i.postmean &lt;- bind_cols( new_logw[,1], m2.i$summary.linear.predictor[d2.i.na,]) %&gt;% 
  select(c(&quot;log_weight&quot;, &quot;mean&quot;, &quot;sd&quot;,  &quot;0.5quant&quot;, &quot;0.975quant&quot;))
names(m2.i.postmean) &lt;- c(&quot;log_weight&quot;, &quot;mean&quot;, &quot;sd&quot;,  &quot;LCI&quot;, &quot;UCI&quot;)
m2.i.plot &lt;- ggplot()+
  geom_point(data= d, aes(weight, height))+
  geom_line(data= m2.i.postmean, aes(exp(log_weight), mean))+
  geom_ribbon(data= m2.i.postmean, aes(exp(log_weight), ymin= LCI, ymax= UCI))
m2.i.plot </code></pre>
<p><img src="rethinkingINLA_HW2_files/figure-html/hw2.2%20inla%20clinear-1.png" width="672" /></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
