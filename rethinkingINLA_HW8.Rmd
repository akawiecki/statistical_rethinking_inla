---
title: "Statistical Rethinking 2nd edition Homework 8 in INLA"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r libraries, message= FALSE}
library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)
```


# 1. 

**Revisit the Reed frog survival data, data(reedfrogs),and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.**

```{r 8.1 dt}
library(rethinking) 
data(reedfrogs)
d <- reedfrogs
```

## 1.1 varying intercepts model

Si ∼ Binomial(Ni, pi)
logit(pi) = αtank[i]                            
αj ∼ Normal($\bar{\alpha}$, σ)                             [adaptive prior]    
$\bar{\alpha}$ ∼ Normal(0, 1.5)                            [prior for average tank]
σ ∼ Exponential(1)                              [prior for standard deviation of tanks]

### 1.1 rethinking

```{r 8.1 dt re}
dat <- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred=="no" , 0L , 1L ), 
size_ = ifelse( d$size=="small" , 1L , 2L )
)
```


```{r 8.1.1 re}
m1.1 <- ulam( alist(
S ~ binomial( n , p ),
logit(p) <- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1)
```

### 1.1 INLA 

following example: https://people.bath.ac.uk/jjf23/brinla/reeds.html

**Here I'm missing custom priors**
I'll use a half cauchy prior for the $\sigma$ to constrain it to >0 numbers, which is what the exponential does as well. 

```{r 8.1.1 inla}

library(brinla)
library(INLA)

d1.i <- d %>% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred=="no", 1, 0), 0),
         pred.yes= na_if(if_else(pred=="pred", 1, 0), 0),
         size.small= na_if(if_else(size=="small", 1, 0), 0),
         size.big= na_if(if_else(size=="big", 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = "expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);"

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i <- inla(surv ~ 1 + f(tank, model="iid", hyper = hcprior), data= d1.i, family = "binomial", 
              Ntrials = density, 
              control.family = list(control.link=list(model="logit")),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)

m1.1.i$summary.fixed

bri.hyperpar.summary(m1.1.i)


```
** it looks like the intercept mean and sd correspond to the $\bar{\alpha}$ mean and sd, and the SD for tank corresponds to the $\sigma$. this makes sense, because the $\bar{\alpha}$ is the average baseline survival for all the tadpoles, which is what the intercept is.** **BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION**. 

## 1.2 varying intercepts + predation

Si ∼ Binomial(Ni, pi)
logit(pi) = αtank[i]  + $\beta$[pred]
$\beta$∼ Normal(-0.5,1)
αj ∼ Normal($\bar{\alpha}$, σ)                             [adaptive prior]    
$\bar{\alpha}$ ∼ Normal(0, 1.5)                            [prior for average tank]
σ ∼ Exponential(1)                              [prior for standard deviation of tanks]

### 1.2 rethinking

```{r 8.1.2 re}
# pred
m1.2 <- ulam(
alist(
S ~ binomial( n , p ),
logit(p) <- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) 

precis(m1.2)
```


### 1.2 inla
```{r 8.1.2 inla}

library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = "expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);"

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i <- inla(surv ~ 1 + pred + f(tank, model="iid", hyper = hcprior), data= d1.i, family = "binomial", 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model="logit")),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)

m1.2.i$summary.fixed

bri.hyperpar.summary(m1.2.i)


```


## 1.3 varying intercepts + size

Si ∼ Binomial(Ni, pi)
logit(pi) = αtank[i]  + $\beta$size    
$\beta$∼ Normal(0 , 0.5 )
αj ∼ Normal($\bar{\alpha}$, σ)                             [adaptive prior]    
$\bar{\alpha}$ ∼ Normal(0, 1.5)                            [prior for average tank]
σ ∼ Exponential(1)                              [prior for standard deviation of tanks]

### 1.3 rethinking

```{r 8.1.3 re}
library(rethinking) 
data(reedfrogs)
d <- reedfrogs

# size
m1.3 <- ulam( alist(
S ~ binomial( n , p ),
logit(p) <- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )


precis(m1.3,  depth=2)
```


### 1.3 inla
```{r 8.1.3 inla}

library(brinla)
library(INLA)

d1.i <- d %>% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred=="no", 1, 0), 0),
         pred.yes= na_if(if_else(pred=="pred", 1, 0), 0),
         size.small= na_if(if_else(size=="small", 1, 0), 0),
         size.big= na_if(if_else(size=="big", 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = "expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);"

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i <- inla(surv ~ 1 + size.small+ size.big + f(tank, model="iid", hyper = hcprior), data= d1.i, family = "binomial", 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 0.5, 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model="logit")),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)

m1.3.i$summary.fixed

bri.hyperpar.summary(m1.3.i)


```

** these estimates are super off, probably something is wrong**

## 1.4 varying intercepts + predation + size

Si ∼ Binomial(Ni, pi)
logit(pi) = αtank[i]  + $\beta$size + $\gamma$size  
$\gamma$ ∼ Normal(0 , 0.5)
$\beta$ ∼ Normal(-0.5,1)
αj ∼ Normal($\bar{\alpha}$, σ)                             [adaptive prior]    
$\bar{\alpha}$ ∼ Normal(0, 1.5)                            [prior for average tank]
σ ∼ Exponential(1)                              [prior for standard deviation of tanks]

### 1.4 rethinking

```{r 8.1.4 re}
# pred + size 
m1.4 <- ulam(
alist(
S ~ binomial( n , p ),
logit(p) <- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.4, depth=2)
```


### 1.4 inla
```{r 8.1.4 inla}

library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = "expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);"

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i <- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model="iid", hyper = hcprior), data= d1.i, family = "binomial", 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model="logit")),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)

m1.4.i$summary.fixed

bri.hyperpar.summary(m1.4.i)


```

## 1.5 varying intercepts + predation + size + prediction*size

Si ∼ Binomial(Ni, pi)
logit(pi) = αtank[i]  + $\beta$size + $\gamma$size + 
$\gamma$ ∼ Normal(0 , 0.5)
$\beta$ ∼ Normal(-0.5,1)
αj ∼ Normal($\bar{\alpha}$, σ)                             [adaptive prior]    
$\bar{\alpha}$ ∼ Normal(0, 1.5)                            [prior for average tank]
σ ∼ Exponential(1)                              [prior for standard deviation of tanks]

### 1.5 rethinking

```{r 8.1.5 re}

# pred + size + interaction 
m1.5 <- ulam(
alist(
S ~ binomial( n , p),
logit(p) <- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)
```


I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.

### 1.5 inla
```{r 8.1.5 inla}

library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = "expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);"

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i <- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model="iid", hyper = hcprior), data= d1.i, family = "binomial", 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model="logit")),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )

m1.5.i$summary.fixed

bri.hyperpar.summary(m1.5.i)


```

## compare using WAIC

#### compare rethinking 

```{r 8 compare re}
rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )
```

```{r compare inla}

inla.models.8.1 <- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic <- function (x){
  x[["waic"]][["waic"]]
}

waic.8.1 <- bind_cols(model = c("m1.1.i"," m1.2.i","m1.3.i", "m1.4.i", "m1.5.i" ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1
```



## 2. 

**In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:**

**(1) district: ID number of administrative district each woman resided in**
**(2) use.contraception: An indicator (0/1) of whether the woman was using contraception**


The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:

```{r 8.2}
sort(unique(d$district))
```

District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:

```{r 8.2 cint}
d$district_id <- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))
```

Now there are 60 values, contiguous integers 1 to 60.


Now, focus on predicting use.contraception, clustered by district_id. Fit both (1) a traditional fixed-effects model that uses an index variable for district and (2) a multilevel model with varying intercepts for district. Plot the predicted pro- portions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?


## 3. 

**Return to the Trolley data, data(Trolley), from Chapter 12. Define and fit a varying intercepts model for these data. By this I mean to add an intercept parameter for the individual to the linear model. Cluster the varying intercepts on individual participants, as indicated by the unique values in the id variable. Include action, intention, and contact as before. Compare the varying intercepts model and a model that ignores individuals, using both WAIC/LOO and posterior predictions. What is the impact of individual variation in these data?**

### Trolley data context from Chapter 12

**Dennis**
Standing by the railroad tracks, Dennis sees an empty, out-of-control boxcar about to hit five people. Next to Dennis is a lever that can be pulled, sending the boxcar down a side track and away from the five people. But pulling the lever will also lower the railing on a footbridge spanning the side track, causing one person to fall off the footbridge and onto the side track, where he will be hit by the boxcar. If Dennis pulls the lever the boxcar will switch tracks and not hit the five people, and the one person to fall and be hit by the boxcar. If Dennis does not pull the lever the boxcar will continue down the tracks and hit five people, and the one person will remain safe above the side track.

**Evan** 
Standing by the railroad tracks, Evan sees an empty, out-of-control boxcar about to hit five people. Next to Evan is a lever that can be pulled, lowering the railing on a footbridge that spans the main track, and causing one person to fall off the footbridge and onto the main track, where he will be hit by the boxcar. The boxcar will slow down because of the one person, therefore preventing the five from being hit. If Evan pulls the lever the one person will fall and be hit by the boxcar, and therefore the boxcar will slow down and not hit the five people. If Evan does not pull the lever the boxcar will continue down the tracks and hit the five people, and the one person will remain safe above the main track.

Most people judge that, if Even pulls the lever, it is morally worse (less permissible) than when Dennis pulls the lever. 

There are 12 columns and 9930 rows, comprising data for 331 unique individuals. The outcome we’ll be interested in is response, which is an integer from 1 to 7 indicating how morally permissible the participant found the action to be taken (or not) in the story. Since this type of rating is categorical and ordered, it’s exactly the right type of problem for our ordered model.


```{r 8.3 data}
library(rethinking) 
data(Trolley)
d <- Trolley
simplehist( d$response , xlim=c(1,7) , xlab="response" )

```




