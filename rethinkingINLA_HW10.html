<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 9 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
<li>
  <a href="rethinkingINLA_HW9.html">Homework 9</a>
</li>
<li>
  <a href="rethinkingINLA_HW10.html">Homework 10</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 9 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="section" class="section level1">
<h1>1.</h1>
<p>Consider the relationship between brain volume (brain) and bodymass (body) in the data (Primates301). These values are presented as single values for each species. However, there is always a range of sizes in a species, and some of these measurements are taken from very small samples. So these values are measured with some unknown error.</p>
<p>We don’t have the raw measurements to work with—that would be best. But we can imagine what might happen if we had them. Suppose error is proportional to the measurement. This makes sense, because larger animals have larger variation. As a consequence, the uncertainty is not uniform across the values and this could mean trouble.</p>
<p>Let’s make up some standard errors for these measurements, to see what might happen. Load the data and scale the the measurements so the maximum is 1 in both cases:</p>
<pre class="r"><code>library(rethinking)
data(Primates301)
d &lt;- Primates301
cc &lt;- complete.cases( d$brain , d$body )
B &lt;- d$brain[cc]
M &lt;- d$body[cc]
B &lt;- B / max(B) 
M &lt;- M / max(M)</code></pre>
<p>Now I’ll make up some standard errors for B and M, assuming error is 10% of the measurement.</p>
<pre class="r"><code>Bse &lt;- B*0.1 
Mse &lt;- M*0.1</code></pre>
<p>Let’s model these variables with this relationship: Bi ∼ Log-Normal(μi, σ) μi =α+βlogMi</p>
<p>This says that brain volume is a log-normal variable, and the mean on the log scale is given by μ. What this model implies is that the expected value of B is: E(Bi|Mi) = exp(α)Mi^β</p>
<p>So this is a standard allometric scaling relationship—incredibly common in biology.</p>
<p>Ignoring measurement error, the corresponding ulam model is:</p>
<pre class="r"><code>dat_list &lt;- list( B = B,
M=M)
m1.1 &lt;- ulam( 
  alist(  
B ~ dlnorm( mu , sigma ), 
mu &lt;- a + b*log(M),
a ~ normal(0,1),
b ~ normal(0,1),
sigma ~ exponential(1) 
) , data=dat_list )</code></pre>
<pre><code>## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG   -I&quot;/Users/annakawiecki/Library/R/4.0/library/Rcpp/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/unsupported&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/BH/include&quot; -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/src/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppParallel/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/rstan/include&quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:88:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL &#39;2bcae94530709687a15da757cbb5d4b7&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.119494 seconds (Warm-up)
## Chain 1:                0.099695 seconds (Sampling)
## Chain 1:                0.219189 seconds (Total)
## Chain 1:</code></pre>
<pre class="r"><code>precis( m1.1 )</code></pre>
<pre><code>##            mean         sd      5.5%     94.5%    n_eff    Rhat4
## a     0.4322723 0.05669226 0.3483946 0.5315580 138.2680 1.003606
## b     0.7844220 0.01371671 0.7645884 0.8086419 139.0159 1.003419
## sigma 0.2926110 0.01517623 0.2706390 0.3195089 155.9351 1.004581</code></pre>
<p>Your job is to add the measurement errors to this model. Use the divorce/marriage example in the chapter as a guide. It might help to initialize the unobserved true values of B and M using the observed values, by adding a list like this to ulam:</p>
<pre class="r"><code> start=list( M_true=dat_list$M , B_true=dat_list$B )</code></pre>
<p>Compare the inference of the measurement error model to those of m1.1 above. Has anything changed? Why or why not?</p>
<div id="model-with-measurement-error" class="section level2">
<h2>1.1 model with measurement error</h2>
<p>To build the measurement error model, all we really need to do is add the observation process. This means that the observed values arise from their own distribution, each having a true value as the mean. We use these unknown true values in the regression.</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<p>The top chunk is the model for the B values. The first line is the measurement process. Then the next two lines are the same regression as before, but with B_true replacing the observed B values. Likewise M_true replaces the observed M in the linear model. The second chunk is the measurement model for M. The prior for M_true covers the entire range of the normalize variable—it ranges from 0 to 1 now, recall, because we scaled it that way to start by dividing by the maximum observed value. The last chunk holds the same priors as before.</p>
<p>Note the control list at the bottom. If you run this model without that, it will work, but be inefficient and warn about exceeding maximum “treedepth.” This is not a concern for the validity of the chains, just how well they run. Treedepth is a control parameter for NUTS algorithm. The Stan manual contains more detail, if you want it.</p>
<pre class="r"><code>d.complete &lt;- d[which(cc ==TRUE),]

N_spp &lt;- nrow(d.complete)
  
m1.2 &lt;- ulam( alist(
# B model
B ~ normal( B_true , Bse ), 
vector[N_spp]:B_true ~ dlnorm( mu , sigma ),
mu &lt;- a + b*log( M_true[i] ),
# M model
M ~ normal( M_true , Mse ), 
vector[N_spp]:M_true ~ normal( 0.5 , 1 ),
# priors
a ~ normal(0,1),
b ~ normal(0,1),
sigma ~ exponential(1) ),
data=dat_list ,
start=list( M_true=dat_list$M , B_true=dat_list$B ) , 
chains=4 , cores=4 , 
control=list(max_treedepth=15) )

precis( m1.2 )</code></pre>
<p>Those 364 hidden parameters are the estimated true values. We can look at those later on. For now, notice that the posterior distributions of a and b are nearly identical to m1.1. Adding measurement error hasn’t changed a thing!</p>
<p>Plotting the regression against the observed values:</p>
<pre class="r"><code>plot( B ~ M , xlab=&quot;body mass&quot; , ylab=&quot;brain volume&quot; , col=rangi2 , pch=16 )
post &lt;- extract.samples(m1.2)
for ( i in 1:50 ) curve( exp(post$a[i])*x^(post$b[i]) , add=TRUE , col=grau(0.2) )</code></pre>
<p>The two points in the upper right are gorillas. Most primates are small, and obviously gorillas have something special going on. Now let’s plot the estimated values on this:</p>
<pre class="r"><code>B_est &lt;- apply( post$B_true , 2 , mean ) 
M_est &lt;- apply( post$M_true , 2 , mean )
plot( B ~ M , xlab=&quot;body mass&quot; , ylab=&quot;brain volume&quot; , col=rangi2 , pch=16 ) points( M_est , B_est , pch=1 , lwd=1.5 )
x_seq &lt;- seq( from=0 , to=1 , length.out=100 )
EB &lt;- sapply( x_seq , function(x) mean( exp(post$a)*x^(post$b) ) ) lines( x_seq , EB )</code></pre>
<p>The open points are the posterior mean estimates. Notice that they have moved towards the regression line, as you’d expect. But even the outlier gorillas haven’t moved much. The assumed error just isn’t big enough to get them any closer. If you increase the amount of error, you can get all of the species to fall right on the regression line. Try for example 30% error. The model will mix poorly, but take a look at the inferred true values. The truth of this example is that there are just so many small primates that they dominate the relationship. And their measurement errors are also smaller (in abso- lute terms). So adding plausible amounts of measurement error here doesn’t make a big difference. We still don’t have a good explanation for gorillas.</p>
<p>Before moving on, I’ll also plot the estimated species values with 50% compati- bility ellipses.</p>
<pre class="r"><code>library(ellipse)
plot( B_est ~ M_est , xlab=&quot;body mass&quot; , ylab=&quot;brain volume&quot; , lwd=1.5 ,
col=grau() , xlim=c(0,1.2) , ylim=c(0,1.2) ) 

for ( i in 1:length(B_est) ) {
SIGMA &lt;- cov( cbind( post$M_true[,i] , post$B_true[,i] ) )
el &lt;- ellipse( SIGMA , centre=c(M_est[i],B_est[i]) , level=0.5 ) lines( el , col=grau(0.3) )
}</code></pre>
</div>
</div>
<div id="inla" class="section level2">
<h2>1.1 INLA</h2>
<p>good resource for this: <a href="http://www.biometrische-gesellschaft.de/fileadmin/AG_Daten/BayesMethodik/workshops_etc/2016-12_Mainz/Muff2016-slides.pdf" class="uri">http://www.biometrische-gesellschaft.de/fileadmin/AG_Daten/BayesMethodik/workshops_etc/2016-12_Mainz/Muff2016-slides.pdf</a> <a href="http://www.r-inla.org/models/tools#TOC-Copying-a-model" class="uri">http://www.r-inla.org/models/tools#TOC-Copying-a-model</a></p>
<p>But i can’t wrap my head around it right now.</p>
<pre class="r"><code>d1.i &lt;- d %&gt;% 
  filter(!is.na(brain) &amp;!is.na(body) ) %&gt;% 
  mutate(B= brain/max(brain), 
         M= body/max(body), 
         Bse= B*0.1, 
         Mse= M*0.1)

n.spp &lt;- nrow(d1.i) # 182 

M_true= rnorm(n.spp, d1.i$M, d1.i$Mse)</code></pre>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p>Now consider missing values—this data set is lousy with them. You can ignore measurement error in this problem. Let’s get a quick idea of the missing values by counting them in each variable:</p>
<pre class="r"><code>library(rethinking) 
data(Primates301) 
d &lt;- Primates301 
colSums( is.na(d) )</code></pre>
<pre><code>##                name               genus             species          subspecies 
##                   0                   0                   0                 267 
##              spp_id            genus_id     social_learning     research_effort 
##                   0                   0                  98                 115 
##               brain                body          group_size           gestation 
##                 117                  63                 114                 161 
##             weaning           longevity        sex_maturity maternal_investment 
##                 185                 181                 194                 197</code></pre>
<p>We’ll continue to focus on just brain and body, to stave off insanity. Consider only those species with measured body masses:</p>
<pre class="r"><code>cc &lt;- complete.cases( d$body ) 
M &lt;- d$body[cc]
M &lt;- M / max(M)
B &lt;- d$brain[cc]
B &lt;- B / max( B , na.rm=TRUE )</code></pre>
<p>You should end up with 238 species and 56 missing brain values among them.</p>
<p><strong>First, consider whether there is a pattern to the missing values. Does it look like missing values are associated with particular values of body mass? Draw a DAG that represents how missingness works in this case. Which type (MCAR, MAR, MNAR) is this?</strong></p>
<p><strong>Second, impute missing values for brain size.</strong></p>
<p>It might help to initialize the 56 imputed variables to a valid value:</p>
<pre class="r"><code>start=list( B_impute=rep(0.5,56) )</code></pre>
<p>This just helps the chain get started.</p>
<p><strong>Compare the inferences to an analysis that drops all the missing values</strong>. Has anything changed? Why or why not? Hint: Consider the density of data in the ranges where there are missing values. You might want to plot the imputed brain sizes together with the observed values.</p>
<div id="pattern-of-the-missing-values" class="section level2">
<h2>2.1 pattern of the missing values</h2>
<p>First,let’s see where the missing values are,to get some idea about the missingness mechanism. If missing brain sizes are associated with certain ranges of body sizes, then it isn’t plausibly MCAR (dog eats any homework). Let’s plot body size against missingness:</p>
<pre class="r"><code>Bna &lt;- is.na(d$brain[cc])
plot( Bna ~ M , ylab=&quot;B is NA&quot; )</code></pre>
<p><img src="rethinkingINLA_HW10_files/figure-html/2.1%20plot-1.png" width="672" /> Looks like the missing brain values are almost all for small bodied species. This implies at least a MAR (dog eats students’ homework) mechanism. Let’s try a DAG to express it:</p>
<pre class="r"><code>library(dagitty)

hw10.2.1.dag &lt;- dagitty(&#39;dag{
M -&gt; R_B -&gt; &quot;B*&quot; &lt;- B
M -&gt; B }&#39;)

plot(hw10.2.1.dag)</code></pre>
<p><img src="rethinkingINLA_HW10_files/figure-html/2.1%20dag-1.png" width="672" /> M here is body mass, B (unobserved) is brain size, R_B is the missingness mechanism, and B* is the observed brain sizes (with missing values). The arrow from M to R_B indicates that body size influences missingness. In this case, it would imply that small body size makes a missing brain value more likely.</p>
</div>
<div id="impute-missing-values-for-brain-size" class="section level2">
<h2>2.2 impute missing values for brain size</h2>
<p>Now let’s do some imputation. Remember that the model for imputation is really no different than an ordinary model. It just needs a prior for any variable with missing values. In this case, the missing values are in the outcome, so the likelihood is the prior we need. So the model doesn’t change at all.</p>
<div id="rethinking-1" class="section level3">
<h3>2.2 rethinking</h3>
<p>In ulam:</p>
<pre class="r"><code>dat_list &lt;- list(
B = B,
M=M)


m2.2 &lt;- ulam( alist(
B ~ dlnorm( mu , sigma ), 
mu &lt;- a + b*log(M),
a ~ normal(0,1),
b ~ normal(0,1),
sigma ~ exponential(1) ),
data=dat_list , chains=4 , cores=4 , start=list( B_impute = rep(0.5,56) ) )</code></pre>
<pre><code>## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG   -I&quot;/Users/annakawiecki/Library/R/4.0/library/Rcpp/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/unsupported&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/BH/include&quot; -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/src/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppParallel/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/rstan/include&quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:88:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1</code></pre>
<p>ulam figures out how to do the imputation. But an equivalent model that is more explicit would be:</p>
<pre class="r"><code>m2.2b &lt;- ulam( alist(
B_merge ~ dlnorm( mu , sigma ),
mu &lt;- a + b*log(M),
B_merge &lt;- merge_missing( B , B_impute ), 
a ~ normal(0,1),
b ~ normal(0,1),
sigma ~ exponential(1) ),
data=dat_list , chains=4 , cores=4 , start=list( B_impute = rep(0.5,56) ) )</code></pre>
<pre><code>## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG   -I&quot;/Users/annakawiecki/Library/R/4.0/library/Rcpp/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/unsupported&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/BH/include&quot; -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/src/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppParallel/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/rstan/include&quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:88:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1</code></pre>
<pre class="r"><code> precis( m2.2b )</code></pre>
<pre><code>##            mean         sd      5.5%     94.5%     n_eff     Rhat4
## a     0.4308374 0.05825864 0.3355201 0.5221980  868.3310 1.0001886
## b     0.7840493 0.01385899 0.7620001 0.8053178  863.9023 0.9999186
## sigma 0.2935054 0.01603319 0.2686648 0.3206893 1032.9590 1.0004339</code></pre>
<p>It’s a little more obvious now what ulam is doing. It constructs the merged vector of observed and imputed values, B_merge, and then uses that merged vector as the outcome. The outcome distribution at the top of the model is the prior for each B_impute parameter. That prior is adaptive—it has parameters inside it. Hence, shrinkage happens.</p>
</div>
<div id="inla-1" class="section level3">
<h3>2.2 INLA</h3>
<pre class="r"><code>d2.i &lt;- d %&gt;% 
  #only cases with complete body masses
  filter(!is.na(body)) %&gt;% 
  mutate(M = body / max(body), 
         B= brain / max(brain, na.rm=TRUE), 
         logM= log(M))

m2.2.i &lt;- inla(B ~ logM, family =&quot;lognormal&quot;, data=d2.i, 
               control.fixed = list(
        mean= 0, 
        prec= 1,
        mean.intercept= 0, 
        prec.intercept= 1), 
        control.predictor=list(compute=TRUE)
               )
summary(m2.2.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = B ~ logM, family = \&quot;lognormal\&quot;, data = d2.i, 
##    control.predictor = list(compute = TRUE), &quot;, &quot; control.fixed = 
##    list(mean = 0, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 
##    1))&quot;) 
## Time used:
##     Pre = 1.57, Running = 0.248, Post = 0.2, Total = 2.01 
## Fixed effects:
##              mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 0.427 0.058      0.313    0.427      0.541 0.427   0
## logM        0.783 0.014      0.756    0.783      0.811 0.783   0
## 
## Model hyperparameters:
##                                           mean   sd 0.025quant 0.5quant
## Precision for the lognormal observations 11.91 1.25       9.60    11.87
##                                          0.975quant  mode
## Precision for the lognormal observations      14.48 11.78
## 
## Expected number of effective parameters(stdev): 2.01(0.002)
## Number of equivalent replicates : 90.56 
## 
## Marginal log-Likelihood:  420.27 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="compare-to-the-analysis-with-complete-cases" class="section level2">
<h2>2.3 compare to the analysis with complete cases</h2>
<div id="rethinking-2" class="section level3">
<h3>2.3 rethinking</h3>
<pre class="r"><code>cc2 &lt;- complete.cases( B )
dat_list2 &lt;- list( B = B[cc2],
M = M[cc2] )
m2.3 &lt;- ulam( alist(
B ~ dlnorm( mu , sigma ), 
mu &lt;- a + b*log(M),
a ~ normal(0,1),
b ~ normal(0,1),
sigma ~ exponential(1) ),
data=dat_list2 , chains=4 , cores=4 ) </code></pre>
<pre><code>## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG   -I&quot;/Users/annakawiecki/Library/R/4.0/library/Rcpp/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/unsupported&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/BH/include&quot; -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/src/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/RcppParallel/include/&quot;  -I&quot;/Users/annakawiecki/Library/R/4.0/library/rstan/include&quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:88:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Users/annakawiecki/Library/R/4.0/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Dense:1:
## /Users/annakawiecki/Library/R/4.0/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1</code></pre>
<pre class="r"><code>precis( m2.3 )</code></pre>
<pre><code>##            mean         sd      5.5%     94.5%    n_eff   Rhat4
## a     0.4276048 0.06075464 0.3308738 0.5243846 730.6154 1.00159
## b     0.7834105 0.01489573 0.7602058 0.8069504 713.5963 1.00203
## sigma 0.2938875 0.01537826 0.2713802 0.3198219 989.7054 1.00186</code></pre>
<p>Really no difference from before.</p>
</div>
<div id="inla-2" class="section level3">
<h3>2.3 INLA</h3>
<pre class="r"><code>d3.i &lt;- d2.i %&gt;% 
  filter(!is.na(brain))

m2.3.i &lt;- inla(B ~ logM, family =&quot;lognormal&quot;, data=d3.i, 
               control.fixed = list(
        mean= 0, 
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1), 
        control.compute = list(config= TRUE),
        control.predictor=list(compute=TRUE)
               )
summary(m2.3.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = B ~ logM, family = \&quot;lognormal\&quot;, data = d3.i, 
##    control.compute = list(config = TRUE), &quot;, &quot; control.predictor = 
##    list(compute = TRUE), control.fixed = list(mean = 0, &quot;, &quot; prec = 1, 
##    mean.intercept = 0, prec.intercept = 1))&quot;) 
## Time used:
##     Pre = 1.37, Running = 0.196, Post = 0.251, Total = 1.82 
## Fixed effects:
##              mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 0.427 0.058      0.313    0.427      0.541 0.427   0
## logM        0.783 0.014      0.756    0.783      0.811 0.783   0
## 
## Model hyperparameters:
##                                           mean   sd 0.025quant 0.5quant
## Precision for the lognormal observations 11.91 1.25       9.60    11.87
##                                          0.975quant  mode
## Precision for the lognormal observations      14.48 11.78
## 
## Expected number of effective parameters(stdev): 2.01(0.002)
## Number of equivalent replicates : 90.56 
## 
## Marginal log-Likelihood:  420.27 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="plot-the-imputed-brain-sizes-together-with-the-observed-values." class="section level2">
<h2>2.4 plot the imputed brain sizes together with the observed values.</h2>
<div id="rethinking-3" class="section level3">
<h3>2.4 rethinking</h3>
<pre class="r"><code>library(rethinking)
post &lt;- rethinking::extract.samples(m2.2)
Bi &lt;- apply( post$B_impute , 2 , mean ) 
miss_idx &lt;- which( is.na(B) )
plot( M[-miss_idx] , B[-miss_idx] , col=rangi2 , pch=16 , xlab=&quot;body mass M&quot; , ylab=&quot;brain size B&quot; )
points( M[miss_idx] , Bi )
Bi_ci &lt;- apply( post$B_impute , 2 , PI , 0.5 )
for ( i in 1:length(Bi) ) lines( rep(M[miss_idx][i],2) , Bi_ci[,i] )</code></pre>
<p><img src="rethinkingINLA_HW10_files/figure-html/10.2.4%20re-1.png" width="672" /></p>
<p>Black open points are the imputed values, with 50% compatibility intervals. Imputation hasn’t done much, apparently because all but one of the missing values are in a very dense region of the body size range. So almost no information was lost—the missing info is redundant.</p>
</div>
<div id="inla-3" class="section level3">
<h3>2.4 INLA</h3>
<pre class="r"><code>#indices of the weights with missing values of brain 
d2.i.na &lt;- which(is.na(d2.i$B))

#cases with response data
d2.i.cc &lt;- d2.i %&gt;% 
  filter(!is.na(brain))

#cases with no response data
d2.i.nobrain &lt;- d2.i %&gt;% 
  filter(is.na(brain))

#imputed values of the response variable in the response scale exp(brain)
Bi.i &lt;- exp(m2.2.i$summary.fitted.values[d2.i.na,])

#imputed values df

d2.i.imp &lt;- bind_cols(d2.i.nobrain, Bi.i) %&gt;% 
  rename(&quot;LCI&quot;= &quot;0.025quant&quot;, &quot;UCI&quot;= &quot;0.975quant&quot;)


m2.2.i.plot &lt;- ggplot() +
  geom_point(data= d2.i.cc, aes(M,B), color= &quot;blue&quot;, alpha= 0.8)+
  geom_pointrange(data= d2.i.imp , aes(x= M,  y= mean, ymin= LCI, ymax= UCI))+
  theme_bw()

m2.2.i.plot </code></pre>
<p><img src="rethinkingINLA_HW10_files/figure-html/10.2.4%20plot%20inla-1.png" width="672" /></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
