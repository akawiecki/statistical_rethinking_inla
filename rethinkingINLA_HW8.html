<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 8 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
<li>
  <a href="rethinkingINLA_HW9.html">Homework 9</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 8 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="section" class="section level1">
<h1>1.</h1>
<p><strong>Revisit the Reed frog survival data, data(reedfrogs),and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.</strong></p>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs</code></pre>
<div id="varying-intercepts-model" class="section level2">
<h2>1.1 varying intercepts model</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i]</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<pre class="r"><code>dat &lt;- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), 
size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L )
)</code></pre>
<pre class="r"><code>m1.1 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1, depth= 2)</code></pre>
<pre><code>##               mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]   2.122094336 0.8560740  0.79739774  3.591521145 4239.770 0.9985890
## a[2]   3.047584417 1.0945332  1.52458170  4.951698469 3101.428 0.9985558
## a[3]   1.006718146 0.6792434 -0.03457335  2.153672704 4246.308 0.9999445
## a[4]   3.043717701 1.1189856  1.47379821  4.997578429 2522.921 0.9986972
## a[5]   2.139822123 0.8595585  0.85599422  3.627666262 2668.098 0.9990286
## a[6]   2.127989116 0.8515739  0.86605397  3.603175876 3536.930 0.9988684
## a[7]   3.059898637 1.1301976  1.48826191  5.012265780 2551.846 0.9992597
## a[8]   2.114374369 0.9049509  0.81442497  3.626404902 2916.684 0.9981775
## a[9]  -0.166540797 0.6122275 -1.14092459  0.807039129 3441.764 0.9990430
## a[10]  2.129705624 0.8691455  0.87230047  3.563344761 3225.193 0.9991200
## a[11]  1.001925467 0.6876866 -0.05581692  2.122220045 3339.423 0.9990243
## a[12]  0.578887320 0.6402776 -0.39574129  1.600128723 3637.986 0.9994051
## a[13]  1.016350725 0.6599329  0.04084091  2.117698141 3471.130 0.9998483
## a[14]  0.207253288 0.6528626 -0.82112946  1.275577711 4657.247 0.9982654
## a[15]  2.136427895 0.8796675  0.84657895  3.685499140 2511.941 0.9998032
## a[16]  2.146568589 0.8820717  0.83159395  3.597883804 2476.514 1.0007148
## a[17]  2.917652598 0.8205173  1.71762234  4.328673472 2629.847 0.9999092
## a[18]  2.416444004 0.6787641  1.39486599  3.547345407 3158.025 0.9997981
## a[19]  2.008630941 0.5919686  1.12396823  3.017745362 4575.238 0.9988618
## a[20]  3.694967564 1.0440805  2.26192200  5.540679118 1991.632 1.0010636
## a[21]  2.373756092 0.6425212  1.41672997  3.468873645 3607.650 0.9981346
## a[22]  2.393545670 0.6461636  1.44307338  3.464981923 3836.318 0.9991014
## a[23]  2.389758415 0.6623637  1.42819185  3.490126303 2727.355 1.0000961
## a[24]  1.712496526 0.5431996  0.90866190  2.615548462 3653.487 0.9995671
## a[25] -1.000174517 0.4546017 -1.75212578 -0.313283120 3895.862 1.0001154
## a[26]  0.152437413 0.3841889 -0.47109432  0.763228736 3814.614 0.9995417
## a[27] -1.434044616 0.4797482 -2.20444024 -0.704053726 3272.235 0.9995084
## a[28] -0.476076442 0.3964132 -1.11448548  0.143697079 3463.230 0.9988954
## a[29]  0.158301714 0.4152911 -0.48505206  0.845924052 4074.497 0.9987576
## a[30]  1.440597779 0.4952943  0.69570043  2.265324236 3922.137 0.9989351
## a[31] -0.641527237 0.4074258 -1.32653300  0.004393233 4060.475 0.9987195
## a[32] -0.305602715 0.4095574 -0.95441164  0.355591952 4046.132 0.9990526
## a[33]  3.197465895 0.7721744  2.08312678  4.581340531 2766.142 0.9992293
## a[34]  2.708890560 0.6332194  1.76588469  3.799281392 3632.034 1.0001351
## a[35]  2.724012583 0.6489630  1.75849681  3.818484012 4146.911 0.9984247
## a[36]  2.082555564 0.5278203  1.30051722  2.969256942 2791.813 0.9996944
## a[37]  2.052837520 0.4980797  1.32998223  2.886157567 3896.093 0.9989072
## a[38]  3.917464833 1.0153203  2.49709316  5.668513100 2135.927 0.9985448
## a[39]  2.707373227 0.6405603  1.79612908  3.769561704 2746.909 0.9986481
## a[40]  2.362407519 0.5906088  1.48158424  3.379832304 3768.673 0.9990909
## a[41] -1.818262772 0.4610151 -2.57851604 -1.141759047 3067.030 0.9991454
## a[42] -0.571719009 0.3514610 -1.14158702 -0.011663141 3813.971 0.9989500
## a[43] -0.459073034 0.3568186 -1.04533966  0.086201077 6331.225 0.9990445
## a[44] -0.339857539 0.3509883 -0.91028453  0.205562382 3172.953 0.9982855
## a[45]  0.571125267 0.3379287  0.04521070  1.125133535 5316.379 0.9997198
## a[46] -0.574774843 0.3605398 -1.16434283 -0.023272058 3608.246 0.9990389
## a[47]  2.058473261 0.4971540  1.33499129  2.873059538 2966.425 0.9984289
## a[48] -0.000668115 0.3570048 -0.57690802  0.565201303 4169.655 0.9995800
## a_bar  1.347710510 0.2604745  0.93526600  1.776872039 2445.655 0.9998769
## sigma  1.625242360 0.2130146  1.31693333  2.000212046 1643.697 1.0017978</code></pre>
</div>
<div id="inla" class="section level3">
<h3>1.1 INLA</h3>
<p>following example: <a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>Here I’m missing custom priors</strong> I’ll use a half cauchy prior for the <span class="math inline">\(\sigma\)</span> to constrain it to &gt;0 numbers, which is what the exponential does as well.</p>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i &lt;- inla(surv ~ 1 + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = 
##    d1.i, Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor 
##    = list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)))&quot; ) 
## Time used:
##     Pre = 1.35, Running = 0.174, Post = 0.2, Total = 1.73 
## Fixed effects:
##             mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 1.38 0.256       0.89    1.375      1.901 1.364   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.415 0.109      0.237    0.404      0.661 0.381
## 
## Expected number of effective parameters(stdev): 40.36(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.00
## Deviance Information Criterion (DIC, saturated) ....: 89.62
## Effective number of parameters .....................: 39.50
## 
## Watanabe-Akaike information criterion (WAIC) ...: 205.61
## Effective number of parameters .................: 22.72
## 
## Marginal log-Likelihood:  -140.19 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.1.i$summary.fixed</code></pre>
<pre><code>##                 mean        sd 0.025quant 0.5quant 0.975quant     mode          kld
## (Intercept) 1.380249 0.2562768  0.8904279 1.374944    1.90069 1.364469 1.902443e-05</code></pre>
<pre class="r"><code>m1.1.i$summary.hyperpar</code></pre>
<pre><code>##                         mean        sd 0.025quant  0.5quant 0.975quant      mode
## Precision for tank 0.4152469 0.1088217  0.2366839 0.4035007  0.6608823 0.3807222</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.1.i)</code></pre>
<pre><code>##                mean        sd   q0.025     q0.5   q0.975     mode
## SD for tank 1.59197 0.2096781 1.231084 1.573964 2.053666 1.539229</code></pre>
<p>it looks like the intercept mean and sd correspond to the <span class="math inline">\(\bar{\alpha}\)</span> mean and sd, and the SD for tank corresponds to the <span class="math inline">\(\sigma\)</span>. this makes sense, because the <span class="math inline">\(\bar{\alpha}\)</span> is the average baseline survival for all the tadpoles, which is what the intercept is. <strong>BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION</strong>.</p>
</div>
</div>
<div id="varying-intercepts-predation" class="section level2">
<h2>1.2 varying intercepts + predation</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>[pred]</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(-0.5,1)</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-1" class="section level3">
<h3>1.2 rethinking</h3>
<pre class="r"><code># pred
m1.2 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis(m1.2)</code></pre>
<pre><code>##             mean        sd       5.5%     94.5%    n_eff    Rhat4
## bp    -2.4155700 0.3037482 -2.9014200 -1.928584 221.6604 1.021967
## a_bar  2.5244074 0.2353512  2.1443352  2.907853 284.9385 1.015421
## sigma  0.8263955 0.1373925  0.6259346  1.070136 718.8868 1.004308</code></pre>
</div>
<div id="inla-1" class="section level3">
<h3>1.2 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i &lt;- inla(surv ~ 1 + pred + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, 
##    data = d1.i, Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), 
##    control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = -0.5, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 
##    1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.66, Running = 0.201, Post = 0.213, Total = 2.07 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.523 0.225      2.088    2.520      2.974  2.515   0
## predpred    -2.435 0.288     -2.997   -2.437     -1.863 -2.440   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 1.80 0.677       0.85     1.68       3.46 1.48
## 
## Expected number of effective parameters(stdev): 28.98(3.36)
## Number of equivalent replicates : 1.66 
## 
## Deviance Information Criterion (DIC) ...............: 204.42
## Deviance Information Criterion (DIC, saturated) ....: 79.99
## Effective number of parameters .....................: 29.04
## 
## Watanabe-Akaike information criterion (WAIC) ...: 201.45
## Effective number of parameters .................: 19.59
## 
## Marginal log-Likelihood:  -122.10 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.2.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant  0.5quant 0.975quant      mode          kld
## (Intercept)  2.523164 0.2252490   2.087742  2.520414   2.974171  2.514827 2.391528e-07
## predpred    -2.435197 0.2876891  -2.996768 -2.437046  -1.863173 -2.440294 8.877976e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.2.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7815832 0.1385647 0.5384615 0.7711308 1.082806 0.7517625</code></pre>
</div>
</div>
<div id="varying-intercepts-size" class="section level2">
<h2>1.3 varying intercepts + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(0 , 0.5 ) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-2" class="section level3">
<h3>1.3 rethinking</h3>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs

# size
m1.3 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis(m1.3,  depth=2)</code></pre>
<pre><code>##               mean        sd        5.5%       94.5%     n_eff     Rhat4
## a[1]   2.228204096 0.9072115  0.87890176  3.76372195  684.8553 1.0022428
## a[2]   3.142097669 1.1524347  1.42010806  5.13153403  886.4844 1.0015426
## a[3]   1.099889946 0.7833356 -0.09774579  2.37653430  680.7999 1.0060614
## a[4]   3.107543124 1.1201736  1.48122117  4.95655767  965.2885 1.0034297
## a[5]   2.010065175 0.9499504  0.59855946  3.59809249  828.1373 1.0006310
## a[6]   2.022157209 1.0101240  0.52474313  3.64287229  987.9136 1.0014488
## a[7]   2.930086820 1.1658649  1.15575344  4.86638912 1255.4897 1.0012038
## a[8]   2.015555000 0.9801948  0.53896605  3.65218057 1090.5226 1.0053279
## a[9]  -0.072993581 0.7102793 -1.26948047  1.05718053  567.9682 1.0056649
## a[10]  2.213185363 0.9104204  0.86309058  3.76984186 1063.2448 1.0055048
## a[11]  1.081407598 0.7631171 -0.12431774  2.28475014  690.7472 1.0036649
## a[12]  0.688311175 0.7568129 -0.50146751  1.92668423  664.4097 1.0066625
## a[13]  0.847852737 0.7649580 -0.34230544  2.14376785  730.9480 1.0001873
## a[14]  0.062534657 0.7229845 -1.10090148  1.22442165  716.9673 1.0038040
## a[15]  2.056075381 0.9864784  0.58195434  3.68615407  984.1975 1.0023537
## a[16]  1.983487965 0.9723267  0.49177551  3.59840299  820.4625 1.0032745
## a[17]  3.020780612 0.8695315  1.71059895  4.43363964  494.3981 1.0065794
## a[18]  2.480865824 0.7696389  1.33377159  3.78285556  539.3708 1.0109427
## a[19]  2.125276451 0.7051004  1.03434454  3.29136344  630.8308 1.0056026
## a[20]  3.761674203 1.1007068  2.23272183  5.61199634  842.5161 1.0031191
## a[21]  2.265645561 0.8014820  1.01965186  3.57907083  715.4628 1.0008868
## a[22]  2.249584151 0.7968138  1.03941355  3.54030245  707.2893 0.9997967
## a[23]  2.246581426 0.7543876  1.07721838  3.50232116  645.1942 0.9997033
## a[24]  1.555797871 0.6762985  0.50848627  2.62961134  591.2536 1.0022586
## a[25] -0.882773562 0.5907456 -1.84305286  0.05154815  372.1505 1.0109167
## a[26]  0.276140417 0.5497563 -0.59961214  1.13340022  356.0330 1.0139931
## a[27] -1.329065247 0.6247900 -2.35526216 -0.34188902  508.3271 1.0105573
## a[28] -0.351688252 0.5652286 -1.26477939  0.51830946  354.1732 1.0091834
## a[29]  0.004146597 0.5527179 -0.89644251  0.88450221  385.4118 1.0034631
## a[30]  1.304729016 0.6358749  0.31529302  2.31143167  486.0458 1.0021223
## a[31] -0.814504670 0.5894831 -1.78213598  0.10765327  450.2980 1.0015527
## a[32] -0.471312784 0.5580826 -1.38430152  0.40287926  377.0450 1.0023633
## a[33]  3.296931234 0.8854160  1.98179244  4.86080786  567.7694 1.0078537
## a[34]  2.816492568 0.7442034  1.67020551  4.05964321  405.6955 1.0074316
## a[35]  2.797924081 0.7389306  1.68581793  4.04270328  603.0254 1.0056860
## a[36]  2.178119370 0.6278355  1.17359069  3.18594416  345.9103 1.0106327
## a[37]  1.908896296 0.6458570  0.89637169  2.91544093  655.9536 1.0042991
## a[38]  3.750097206 1.0737736  2.15619732  5.62617650  897.3089 1.0035309
## a[39]  2.575596370 0.7472063  1.46181236  3.81045602  625.2403 1.0014422
## a[40]  2.199958954 0.7013100  1.14764860  3.32098067  558.3504 1.0008056
## a[41] -1.698176211 0.6197478 -2.69864258 -0.76778210  363.2247 1.0136702
## a[42] -0.455571456 0.5350916 -1.29290690  0.40221360  315.9296 1.0121426
## a[43] -0.339528536 0.5191374 -1.16770876  0.47725345  388.7693 1.0101836
## a[44] -0.225204629 0.5221060 -1.09552368  0.58711428  341.8640 1.0130716
## a[45]  0.415597556 0.5235437 -0.46472109  1.21932466  368.0713 1.0037284
## a[46] -0.725590477 0.5314770 -1.59068106  0.09847777  379.3963 1.0032582
## a[47]  1.917114428 0.6602681  0.85321862  3.01478068  547.3849 1.0017784
## a[48] -0.163019773 0.5383815 -1.02339681  0.70295811  352.2260 1.0033053
## s[1]   0.173146675 0.4044908 -0.45782690  0.83117203  234.5352 1.0057593
## s[2]  -0.120734315 0.3986600 -0.74208149  0.54050799  181.4139 1.0204542
## a_bar  1.325795730 0.4201634  0.65140204  1.97758051  228.2897 1.0114657
## sigma  1.615428805 0.2122639  1.30176590  1.99011158  701.4716 1.0042759</code></pre>
</div>
<div id="inla-2" class="section level3">
<h3>1.3 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i &lt;- inla(surv ~ 1 + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 1/(0.5^2), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + f(tank, model = \&quot;iid\&quot;, &quot;, &quot; hyper = hcprior), family 
##    = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, &quot;, &quot; control.compute = list(config = T, dic = TRUE, waic = 
##    TRUE), &quot;, &quot; control.predictor = list(link = 1, compute = T), control.family = list(control.link = list(model 
##    = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1/(0.5^2), mean.intercept = 0, &quot;, &quot; prec.intercept 
##    = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.58, Running = 0.206, Post = 0.23, Total = 2.01 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.272 0.418      0.455    1.271      2.095  1.268   0
## size.small   0.222 0.400     -0.564    0.223      1.007  0.223   0
## size.big    -0.081 0.400     -0.866   -0.082      0.705 -0.082   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.421 0.111      0.239    0.408      0.672 0.385
## 
## Expected number of effective parameters(stdev): 40.42(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.43
## Deviance Information Criterion (DIC, saturated) ....: 90.04
## Effective number of parameters .....................: 39.58
## 
## Watanabe-Akaike information criterion (WAIC) ...: 206.21
## Effective number of parameters .................: 22.90
## 
## Marginal log-Likelihood:  -142.21 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.3.i$summary.fixed</code></pre>
<pre><code>##                    mean        sd 0.025quant    0.5quant 0.975quant       mode          kld
## (Intercept)  1.27186665 0.4177740  0.4549238  1.27059867   2.095210  1.2681232 2.406671e-06
## size.small   0.22245891 0.4001376 -0.5640040  0.22269633   1.006898  0.2232035 1.123068e-07
## size.big    -0.08147702 0.4002655 -0.8664804 -0.08182627   0.704788 -0.0824901 6.786554e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.3.i)</code></pre>
<pre><code>##                 mean        sd   q0.025    q0.5   q0.975     mode
## SD for tank 1.582503 0.2100774 1.220948 1.56446 2.045071 1.529635</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size" class="section level2">
<h2>1.4 varying intercepts + predation + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size + <span class="math inline">\(\gamma\)</span>size</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5)</p>
<p><span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-3" class="section level3">
<h3>1.4 rethinking</h3>
<pre class="r"><code># pred + size 
m1.4 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.4, depth=2)</code></pre>
<pre><code>##             mean        sd         5.5%      94.5%    n_eff    Rhat4
## a[1]   2.4773576 0.7118944  1.367990070  3.6184862 392.2086 1.006185
## a[2]   2.9104403 0.7556197  1.718771301  4.1484790 455.9487 1.010231
## a[3]   1.7350892 0.6776926  0.660245169  2.8144973 402.9866 1.006657
## a[4]   2.9051319 0.7915368  1.643408657  4.2000272 559.6016 1.005011
## a[5]   2.3360233 0.7375920  1.186935190  3.4986308 458.5977 1.009083
## a[6]   2.3381612 0.7469074  1.194563229  3.5963957 520.1568 1.006445
## a[7]   2.7612132 0.8028629  1.536399707  4.1026211 576.8220 1.006035
## a[8]   2.3386824 0.7722624  1.180941874  3.6491710 408.9483 1.008062
## a[9]   2.2804718 0.6646669  1.185625258  3.2801894 300.3312 1.010211
## a[10]  3.5354394 0.6951396  2.425999591  4.6674867 329.2401 1.013791
## a[11]  3.0108564 0.6597999  1.953442593  4.0658444 283.5728 1.010210
## a[12]  2.7599826 0.6556730  1.674176271  3.8000064 267.5906 1.011683
## a[13]  2.7791598 0.6513100  1.726137749  3.8513487 316.7647 1.011062
## a[14]  2.2859811 0.6678834  1.204597814  3.3605511 352.1985 1.013567
## a[15]  3.3359334 0.7085717  2.214438397  4.4940745 413.5866 1.008162
## a[16]  3.3287027 0.6871804  2.279177196  4.4726934 374.0329 1.010463
## a[17]  2.8870014 0.6844680  1.839658411  4.0551383 426.1276 1.008353
## a[18]  2.5792649 0.6334725  1.578416684  3.6257757 480.9741 1.006839
## a[19]  2.3236995 0.6111185  1.379756643  3.3108008 326.4845 1.010270
## a[20]  3.2340547 0.7327028  2.126028146  4.4669799 579.7268 1.007485
## a[21]  2.3643292 0.6692470  1.356275393  3.4642498 440.2251 1.006852
## a[22]  2.3610241 0.6812402  1.279434040  3.4286782 436.4541 1.009957
## a[23]  2.3763149 0.6739769  1.306998057  3.4794128 333.8516 1.009641
## a[24]  1.8269221 0.5839164  0.920544262  2.7798308 394.8392 1.009493
## a[25]  1.6804778 0.5970785  0.686811152  2.5984435 209.7517 1.014844
## a[26]  2.6058918 0.5705794  1.642240593  3.4661315 194.8726 1.016466
## a[27]  1.3517388 0.6101524  0.334514525  2.2747982 242.6965 1.011577
## a[28]  2.0888546 0.5736677  1.139381079  2.9891587 211.5687 1.017048
## a[29]  2.2775986 0.5543081  1.370360386  3.1435719 222.2833 1.015693
## a[30]  3.2413946 0.5858934  2.353837713  4.2041140 235.9429 1.015416
## a[31]  1.6274874 0.5690070  0.724645956  2.5231770 185.1807 1.018461
## a[32]  1.8919443 0.5606666  0.969492617  2.7701181 196.7047 1.019814
## a[33]  3.0346147 0.6202656  2.069943937  4.0382597 435.1611 1.006789
## a[34]  2.7979849 0.6213813  1.812647258  3.8067312 377.0388 1.010488
## a[35]  2.7965294 0.6088806  1.857155431  3.7659046 314.2422 1.010174
## a[36]  2.3271777 0.5851699  1.394853844  3.2668432 285.1343 1.009888
## a[37]  2.0503679 0.5962453  1.128998738  3.0175871 383.9945 1.009286
## a[38]  3.1929158 0.7393115  2.053941873  4.4040057 584.5334 1.006889
## a[39]  2.5305543 0.6499396  1.533425727  3.5880867 471.4731 1.008855
## a[40]  2.2703498 0.6184004  1.298951352  3.2626591 410.8660 1.009464
## a[41]  1.0320018 0.6288340  0.007673707  1.9925003 221.8626 1.012520
## a[42]  2.0074664 0.5570133  1.093112349  2.8774407 166.0926 1.020381
## a[43]  2.1254181 0.5478662  1.230514671  2.9983699 186.5526 1.018245
## a[44]  2.2156699 0.5620100  1.321943808  3.1156588 184.6493 1.020264
## a[45]  2.6469805 0.5357682  1.812572703  3.4899766 195.5498 1.016547
## a[46]  1.6600002 0.5455192  0.794524286  2.5274597 191.0866 1.019328
## a[47]  3.7329584 0.5848088  2.782335837  4.6561273 279.3518 1.016281
## a[48]  2.1483456 0.5285664  1.285874297  2.9670875 175.1196 1.023810
## bp    -2.4534377 0.2951387 -2.931989339 -1.9732683 491.2607 1.012066
## s[1]   0.3062539 0.3656089 -0.281413124  0.8823463 190.5277 1.019449
## s[2]  -0.1254138 0.3626208 -0.709368076  0.4442904 155.2765 1.017304
## a_bar  2.4527617 0.4041370  1.811184325  3.1126009 118.6189 1.030773
## sigma  0.7784418 0.1478500  0.564614840  1.0335570 649.0898 1.009339</code></pre>
</div>
<div id="inla-3" class="section level3">
<h3>1.4 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i &lt;- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + size.small + size.big + f(tank, &quot;, &quot; model = \&quot;iid\&quot;, hyper = hcprior), 
##    family = \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = density, control.compute = list(config = T, dic = TRUE, 
##    &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link 
##    = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big 
##    = 0), prec = list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 
##    1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.57, Running = 0.216, Post = 0.242, Total = 2.02 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.161 0.666      0.854    2.161      3.469  2.160   0
## predpred    -2.628 0.290     -3.204   -2.626     -2.062 -2.622   0
## size.small   0.729 0.656     -0.559    0.729      2.016  0.729   0
## size.big     0.231 0.656     -1.056    0.231      1.517  0.231   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.13 0.907      0.941     1.95       4.40 1.66
## 
## Expected number of effective parameters(stdev): 27.66(3.70)
## Number of equivalent replicates : 1.74 
## 
## Deviance Information Criterion (DIC) ...............: 204.82
## Deviance Information Criterion (DIC, saturated) ....: 80.44
## Effective number of parameters .....................: 27.99
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.79
## Effective number of parameters .................: 19.63
## 
## Marginal log-Likelihood:  -123.31 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.4.i$summary.fixed</code></pre>
<pre><code>##                   mean        sd 0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)  2.1613253 0.6663035  0.8539924  2.1610031   3.469238  2.1604153 1.094653e-07
## predpred    -2.6276156 0.2895818 -3.2042666 -2.6256993  -2.062158 -2.6218552 3.292691e-06
## size.small   0.7293355 0.6560320 -0.5587394  0.7293248   2.016310  0.7293579 4.990355e-07
## size.big     0.2308486 0.6557372 -1.0562819  0.2307203   1.517498  0.2305195 1.043209e-08</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.4.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7260825 0.1402651 0.4772367 0.7163201 1.028863 0.6986102</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size-predationsize" class="section level2">
<h2>1.5 varying intercepts + predation + size + predation*size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p><strong>this formula’s wrong</strong></p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>predation + <span class="math inline">\(\gamma\)</span>size + <span class="math inline">\(\eta\)</span>size*predation</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5) <span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]<br />
<span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank] σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-4" class="section level3">
<h3>1.5 rethinking</h3>
<pre class="r"><code># pred + size + interaction 
m1.5 &lt;- ulam(
alist(
S ~ binomial( n , p),
logit(p) &lt;- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)</code></pre>
<pre><code>##              mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1]  -0.06704646 0.8397083 -1.3688441  1.32281167 2529.7405 0.9996240
## z[2]   0.48708267 0.8782092 -0.8935538  1.94452432 2443.0070 1.0012903
## z[3]  -0.98423710 0.7794975 -2.1975606  0.29696178 2120.3692 0.9992738
## z[4]   0.47641043 0.9163080 -0.9539869  1.96508164 2000.3744 1.0007798
## z[5]  -0.05301853 0.8115255 -1.2810306  1.25718309 2071.1460 0.9983185
## z[6]  -0.00628242 0.8218996 -1.2789889  1.34789278 2141.5863 0.9988246
## z[7]   0.53800982 0.8484319 -0.7749083  1.88648699 2290.5382 0.9990028
## z[8]  -0.05664953 0.8220788 -1.3198066  1.27641925 2230.8172 1.0014182
## z[9]  -0.08874109 0.6960165 -1.2212308  1.03486913 1771.0432 1.0012390
## z[10]  1.51422140 0.7077847  0.3996556  2.68080751 2056.4390 1.0013598
## z[11]  0.85901815 0.6644529 -0.2052000  1.91659666 1477.5768 0.9989939
## z[12]  0.56858439 0.6844490 -0.5417008  1.66917207 1739.6498 1.0059217
## z[13]  0.21531397 0.6877450 -0.8818822  1.36347641 1680.3635 1.0003384
## z[14] -0.42446478 0.7017327 -1.5396690  0.68323708 2042.1255 1.0008560
## z[15]  0.91957403 0.7443646 -0.2078963  2.11172075 2080.6914 1.0014218
## z[16]  0.95002282 0.7496064 -0.2284956  2.16433037 2557.3661 1.0009494
## z[17]  0.44754677 0.7751777 -0.7698784  1.68444777 1943.5003 1.0018064
## z[18]  0.07410780 0.6980638 -1.0211844  1.16455799 1660.4682 1.0005007
## z[19] -0.28312260 0.7489060 -1.4271262  0.94644157 2200.2164 1.0000236
## z[20]  0.88667220 0.8147231 -0.4019458  2.21386393 2349.6069 0.9996297
## z[21]  0.07735694 0.7087871 -1.0440155  1.21027519 2109.3724 1.0006676
## z[22]  0.08095993 0.7685377 -1.1446600  1.34316622 2530.3012 0.9993300
## z[23]  0.09618903 0.7363141 -1.0487194  1.27134302 2491.4893 0.9986556
## z[24] -0.58129403 0.6981120 -1.6542773  0.53900480 1518.3165 1.0006253
## z[25] -0.86669642 0.6106775 -1.8852913  0.07486317 1653.5684 0.9993503
## z[26]  0.39361564 0.5494958 -0.4664351  1.29501330 1212.6368 1.0048746
## z[27] -1.27711975 0.6060510 -2.2447653 -0.32413091 1529.0034 1.0030094
## z[28] -0.29151648 0.5569584 -1.1781210  0.60049761 1736.6853 1.0063436
## z[29] -0.52320584 0.5597604 -1.4157429  0.36549192 1305.1127 1.0012499
## z[30]  0.77842167 0.6196448 -0.1931243  1.78442775 1491.7515 1.0012533
## z[31] -1.37366835 0.5701811 -2.2807725 -0.47132193 1464.1623 1.0006355
## z[32] -1.03014426 0.5620612 -1.9373080 -0.13387412 1690.7296 0.9999896
## z[33]  0.68010094 0.7792378 -0.5197073  1.99145504 2004.9103 0.9992111
## z[34]  0.31381550 0.7195154 -0.7975645  1.47769124 1902.4812 0.9995945
## z[35]  0.32171918 0.7294849 -0.7867109  1.58120169 2120.5362 0.9989319
## z[36] -0.26430284 0.6750788 -1.3307700  0.84494123 2259.1144 1.0001623
## z[37] -0.27117665 0.6568708 -1.2702693  0.78306267 2520.6363 0.9989276
## z[38]  1.12123694 0.7847346 -0.1245975  2.39846578 2218.1474 1.0009130
## z[39]  0.34734267 0.7026787 -0.7618808  1.52623975 1795.6299 1.0011692
## z[40]  0.03721593 0.6777645 -1.0061311  1.14912872 1815.8494 0.9992218
## z[41] -1.70939964 0.5779295 -2.6358143 -0.82468289 1557.0063 1.0020291
## z[42] -0.39598013 0.5320773 -1.2413357  0.46325169 1344.1141 1.0039465
## z[43] -0.25241463 0.5179207 -1.0640405  0.57273930 1424.1829 1.0047491
## z[44] -0.12355022 0.5226468 -0.9298230  0.73190478 1375.7761 1.0042686
## z[45] -0.05913908 0.5521930 -0.9376778  0.82407659 1411.8458 1.0011624
## z[46] -1.39418948 0.5197289 -2.2430590 -0.58440180  910.1198 1.0015473
## z[47]  1.43127680 0.5948824  0.5157967  2.39188079 1319.7846 1.0016989
## z[48] -0.70558899 0.5249476 -1.5326030  0.13194638 1251.2751 1.0033712
## bp[1] -1.83717326 0.3726464 -2.4135283 -1.23524582  739.4412 1.0011312
## bp[2] -2.76590198 0.3610285 -3.3643237 -2.21292172  749.6066 1.0093368
## s[1]   0.11792258 0.3892887 -0.5008188  0.72633699 1185.8345 1.0022072
## s[2]   0.15869224 0.3750855 -0.4342934  0.74151893 1182.4791 1.0000980
## a_bar  2.30452733 0.3879266  1.7101387  2.92758881 1000.5474 0.9999170
## sigma  0.74648358 0.1384270  0.5348761  0.98181524  689.8179 1.0031636</code></pre>
<p>I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.</p>
</div>
<div id="inla-4" class="section level3">
<h3>1.5 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i &lt;- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + pred * size.small + &quot;, &quot; pred * size.big + f(tank, 
##    model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, 
##    control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 1, &quot;, &quot; 
##    compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean 
##    = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec = list(pred = 1, size.small = 1, &quot;, &quot; size.big 
##    = 1), mean.intercept = 0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.77, Running = 0.192, Post = 0.249, Total = 2.21 
## Fixed effects:
##                       mean     sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)          2.156  0.665      0.851    2.156      3.462  2.155   0
## size.small           0.406  0.675     -0.919    0.405      1.730  0.405   0
## size.big             0.551  0.676     -0.775    0.551      1.877  0.551   0
## predpred            -1.754 18.258    -37.601   -1.754     34.064 -1.754   0
## size.small:predpred -0.341 18.260    -36.191   -0.342     35.479 -0.341   0
## size.big:predpred   -1.411 18.260    -37.261   -1.412     34.409 -1.411   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.37 1.07       1.01     2.15       5.07 1.80
## 
## Expected number of effective parameters(stdev): 27.12(3.80)
## Number of equivalent replicates : 1.77 
## 
## Deviance Information Criterion (DIC) ...............: 203.91
## Deviance Information Criterion (DIC, saturated) ....: 79.54
## Effective number of parameters .....................: 27.31
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.42
## Effective number of parameters .................: 19.54
## 
## Marginal log-Likelihood:  -126.05 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.5.i$summary.fixed</code></pre>
<pre><code>##                           mean         sd  0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)          2.1559548  0.6651605   0.8508812  2.1556250   3.461695  2.1550209 5.782096e-07
## size.small           0.4056285  0.6747850  -0.9188799  0.4054782   1.729780  0.4052337 3.563633e-07
## size.big             0.5511408  0.6756415  -0.7748341  0.5509153   1.877178  0.5505204 5.496338e-07
## predpred            -1.7535485 18.2583848 -37.6008935 -1.7540628  34.063846 -1.7535481 6.603266e-10
## size.small:predpred -0.3409941 18.2597045 -36.1909263 -0.3415085  35.478979 -0.3409938 9.191619e-10
## size.big:predpred   -1.4114467 18.2597582 -37.2614792 -1.4119606  34.408614 -1.4114449 1.461611e-09</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.5.i)</code></pre>
<pre><code>##                  mean        sd    q0.025     q0.5    q0.975      mode
## SD for tank 0.6920361 0.1392399 0.4444564 0.682502 0.9922881 0.6654361</code></pre>
</div>
</div>
<div id="compare-using-waic" class="section level2">
<h2>compare using WAIC</h2>
<div id="compare-rethinking" class="section level4">
<h4>compare rethinking</h4>
<pre class="r"><code>rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )</code></pre>
<pre><code>##          WAIC       SE     dWAIC      dSE    pWAIC     weight
## m1.5 199.2385 8.918353 0.0000000       NA 18.84936 0.31351271
## m1.4 199.5352 8.760780 0.2966803 2.297714 19.07494 0.27029117
## m1.2 199.6842 9.072559 0.4456940 3.233409 19.54801 0.25088456
## m1.1 201.6415 7.432213 2.4030646 6.013272 21.61952 0.09428364
## m1.3 202.2080 7.357505 2.9695336 5.875800 21.83867 0.07102793</code></pre>
<p>These models are really very similar in expected out-of-sample accuracy. The tank variation is huge. But take a look at the posterior distributions for predation and size. You’ll see that predation does seem to matter, as you’d expect. Size matters a lot less. So while predation doesn’t explain much of the total variation, there is plenty of evidence that it is a real effect. Remember: We don’t select a model using WAIC (or LOO). A predictor can make little difference in total accuracy but still be a real causal effect.</p>
</div>
<div id="compare-inla" class="section level4">
<h4>compare inla</h4>
<pre class="r"><code>inla.models.8.1 &lt;- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic &lt;- function (x){
  x[[&quot;waic&quot;]][[&quot;waic&quot;]]
}

waic.8.1 &lt;- bind_cols(model = c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1</code></pre>
<pre><code>## # A tibble: 5 x 2
##   model   waic
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 m1.1.i  206.
## 2 m1.2.i  201.
## 3 m1.3.i  206.
## 4 m1.4.i  203.
## 5 m1.5.i  202.</code></pre>
<p>Let’s look at all the sigma posterior distributions: The two models that omit predation, m1.1 and m1.3, have larger values of sigma. This is because predation explains some of the variation among tanks. So when you add it to the model, the variation in the tank intercepts gets smaller.</p>
<pre class="r"><code>sigma.8.1 &lt;- bind_cols( model= c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), do.call(rbind.data.frame, lapply(inla.models.8.1 ,bri.hyperpar.summary)))


sigma.8.1</code></pre>
<pre><code>##               model      mean        sd    q0.025      q0.5    q0.975      mode
## SD for tank  m1.1.i 1.5919697 0.2096781 1.2310838 1.5739639 2.0536663 1.5392289
## SD for tank1 m1.2.i 0.7815832 0.1385647 0.5384615 0.7711308 1.0828062 0.7517625
## SD for tank2 m1.3.i 1.5825033 0.2100774 1.2209479 1.5644595 2.0450713 1.5296348
## SD for tank3 m1.4.i 0.7260825 0.1402651 0.4772367 0.7163201 1.0288635 0.6986102
## SD for tank4 m1.5.i 0.6920361 0.1392399 0.4444564 0.6825020 0.9922881 0.6654361</code></pre>
<pre class="r"><code>sigma.8.1.plot &lt;-  ggplot(data= sigma.8.1, aes(y=model, x=mean, label=model)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=q0.025, xmax=q0.975), height=.3) +
    coord_fixed(ratio=.3) +
    theme_bw()

sigma.8.1.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/sigma.8.1%20plot-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p><strong>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</strong></p>
<p><strong>(1) district: ID number of administrative district each woman resided in</strong></p>
<p><strong>(2) use.contraception: An indicator (0/1) of whether the woman was using contraception</strong></p>
<p><strong>Focus on predicting use.contraception, clustered by district_id. Fit both:</strong></p>
<p><strong>1) a traditional fixed-effects model that uses an index variable for district</strong></p>
<p><strong>2) a multilevel model with varying intercepts for district.</strong></p>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?**</p>
<pre class="r"><code>library(rethinking)
data(bangladesh)
d &lt;- bangladesh</code></pre>
<p>The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:</p>
<pre class="r"><code>sort(unique(d$district))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39
## [40] 40 41 42 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61</code></pre>
<p>District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:</p>
<pre class="r"><code>d$district_id &lt;- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39
## [40] 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</code></pre>
<p>Now there are 60 values, contiguous integers 1 to 60.</p>
<div id="traditional-fixed-effects-model-that-uses-an-index-variable-for-district" class="section level2">
<h2>2.1 traditional fixed-effects model that uses an index variable for district</h2>
<div id="rethinking-5" class="section level3">
<h3>2.1 rethinking</h3>
<pre class="r"><code>dat_list &lt;- list(
C = d$use.contraception, 
did = d$district_id
)

m2.1 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( 0 , 1.5 )
) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.1, depth = 2)</code></pre>
<pre><code>##               mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]  -1.051277732 0.2187737 -1.41293009 -0.704052865 4363.128 0.9986573
## a[2]  -0.584044746 0.4461138 -1.31664462  0.118391364 3795.906 0.9993041
## a[3]   1.227255245 1.0985063 -0.46435897  2.985098222 4823.395 1.0008378
## a[4]  -0.001373011 0.3585269 -0.57540059  0.580353565 4028.495 0.9987710
## a[5]  -0.564253069 0.3373545 -1.09308899 -0.028105997 4822.825 0.9992251
## a[6]  -0.872101004 0.2667140 -1.30120354 -0.468669701 3837.889 0.9989515
## a[7]  -0.907227615 0.5342922 -1.76736681 -0.069728854 6602.060 0.9992741
## a[8]  -0.485418042 0.3444449 -1.02653239  0.049867129 4283.168 0.9987745
## a[9]  -0.786967048 0.4308087 -1.48135990 -0.132328049 4212.589 0.9992414
## a[10] -1.964181503 0.7308395 -3.19676694 -0.849999805 3780.496 0.9991827
## a[11] -2.968332524 0.8646785 -4.44203878 -1.685319028 5041.336 0.9983276
## a[12] -0.625274945 0.3902330 -1.25833927 -0.006918214 5206.578 0.9986580
## a[13] -0.327347354 0.3864354 -0.91779233  0.305580625 3844.449 0.9990966
## a[14]  0.513044315 0.1839021  0.22645027  0.804175036 3052.916 0.9995362
## a[15] -0.529468545 0.4212083 -1.21757286  0.125430797 4419.708 0.9995344
## a[16]  0.197452099 0.4265995 -0.47852012  0.891897426 4804.057 0.9989197
## a[17] -0.854897938 0.4319400 -1.57561739 -0.176614920 4743.889 0.9981850
## a[18] -0.658029756 0.2968625 -1.13642520 -0.202068331 5265.729 0.9985850
## a[19] -0.457266858 0.4019419 -1.11829296  0.174502971 4403.122 0.9987039
## a[20] -0.393226885 0.5049933 -1.21693323  0.389486190 3947.316 0.9986752
## a[21] -0.422103075 0.4759102 -1.17522758  0.333198234 4679.205 0.9988270
## a[22] -1.282068319 0.5288986 -2.13301885 -0.480059457 3472.259 0.9990884
## a[23] -0.939352682 0.5567181 -1.84223255 -0.066905596 5318.806 0.9986128
## a[24] -2.048972980 0.7542342 -3.40432006 -0.971794294 3554.509 1.0001472
## a[25] -0.207840211 0.2382014 -0.59418014  0.171341136 4766.682 0.9983747
## a[26] -0.449249366 0.5254694 -1.30679726  0.383228972 4198.404 0.9998363
## a[27] -1.447319345 0.3644633 -2.02989841 -0.883767509 3765.247 0.9993580
## a[28] -1.097394106 0.3130466 -1.60323645 -0.602308518 4677.473 0.9985301
## a[29] -0.901516774 0.3679806 -1.49717550 -0.330491769 3682.227 0.9989540
## a[30] -0.034116660 0.2550510 -0.43806150  0.375515754 5238.645 0.9989561
## a[31] -0.173276465 0.3519360 -0.74115376  0.383553153 4440.369 0.9994091
## a[32] -1.268102372 0.4933978 -2.09477843 -0.496851890 5261.138 0.9986388
## a[33] -0.288000153 0.5388612 -1.17240813  0.557992287 4130.483 0.9988865
## a[34]  0.631348828 0.3577408  0.07008661  1.207672511 5281.801 0.9988665
## a[35] -0.008279885 0.3019531 -0.47895560  0.474829283 3900.972 0.9990804
## a[36] -0.578919962 0.4902906 -1.37753772  0.203814946 5204.025 0.9993452
## a[37]  0.143166494 0.5354847 -0.70239261  1.002023920 5033.765 0.9982148
## a[38] -0.835412651 0.5470439 -1.70773183 -0.022625910 4507.700 0.9986683
## a[39] -0.011553018 0.4001779 -0.64376210  0.610173594 3933.119 0.9984120
## a[40] -0.140497164 0.3158319 -0.65523763  0.363881124 5041.158 0.9982212
## a[41] -0.002136198 0.3801659 -0.60939107  0.608584846 4594.936 0.9986193
## a[42]  0.172541409 0.5519140 -0.70921714  1.064737549 4288.485 0.9988578
## a[43]  0.129776699 0.3017947 -0.34379226  0.601494164 4548.748 0.9985249
## a[44] -1.190889368 0.4446941 -1.95753298 -0.527852629 4988.183 0.9987687
## a[45] -0.675784668 0.3351468 -1.22981431 -0.146843884 4007.388 0.9988252
## a[46]  0.087854792 0.2182215 -0.26357305  0.431982715 4342.511 1.0002665
## a[47] -0.119232643 0.5026847 -0.91641083  0.676675927 3877.193 0.9991823
## a[48]  0.093133002 0.3070919 -0.40055290  0.585622735 4746.164 0.9984415
## a[49] -1.720384840 1.0580351 -3.51111705 -0.126773304 4269.556 0.9995560
## a[50] -0.098209750 0.4525759 -0.81405429  0.629770243 6536.338 0.9987904
## a[51] -0.167883442 0.3256408 -0.67698004  0.360915770 4623.692 0.9992896
## a[52] -0.229373425 0.2640379 -0.64934747  0.198644738 5305.127 0.9985589
## a[53] -0.326688210 0.4762032 -1.09939292  0.420095775 3037.545 1.0000100
## a[54] -1.237406748 0.8371034 -2.66758654  0.033885633 3814.694 0.9988978
## a[55]  0.316266312 0.3182194 -0.19130924  0.822253008 4111.931 0.9988708
## a[56] -1.401732140 0.4745231 -2.20574384 -0.694348019 4120.777 0.9992284
## a[57] -0.178999741 0.3629464 -0.74461849  0.389085972 4565.087 0.9984169
## a[58] -1.732160771 0.7948909 -3.10303947 -0.568892808 4546.866 0.9992326
## a[59] -1.220904204 0.4226371 -1.92157919 -0.592729508 3728.636 0.9982040
## a[60] -1.252234818 0.3774130 -1.84790144 -0.664098488 4026.163 0.9993475</code></pre>
</div>
<div id="inla-5" class="section level3">
<h3>2.1 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d2.i &lt;- d %&gt;% 
  mutate(did= paste(&quot;d&quot;, as.integer(d$district_id), sep= &quot;.&quot;), 
         d.value= 1
         ) %&gt;% 
  spread(did, d.value)

#use this to quickly make a list of the index vbles to include in the model 
did_formula &lt;- paste(&quot;d&quot;, 1:60, sep=&quot;.&quot;, collapse = &quot;+&quot;)


m2.1.i &lt;- inla(use.contraception ~ d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9+d.10+d.11+d.12+d.13+d.14+d.15+d.16+d.17+d.18+d.19+d.20+d.21+d.22+d.23+d.24+d.25+d.26+d.27+d.28+d.29+d.30+d.31+d.32+d.33+d.34+d.35+d.36+d.37+d.38+d.39+d.40+d.41+d.42+d.43+d.44+d.45+d.46+d.47+d.48+d.49+d.50+d.51+d.52+d.53+d.54+d.55+d.56+d.57+d.58+d.59+d.60, data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean=  0 ,
        prec= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ d.1 + d.2 + d.3 + d.4 + d.5 + &quot;, &quot; d.6 + d.7 + d.8 + d.9 + d.10 + d.11 
##    + d.12 + d.13 + d.14 + &quot;, &quot; d.15 + d.16 + d.17 + d.18 + d.19 + d.20 + d.21 + d.22 + d.23 + &quot;, &quot; d.24 + d.25 
##    + d.26 + d.27 + d.28 + d.29 + d.30 + d.31 + d.32 + &quot;, &quot; d.33 + d.34 + d.35 + d.36 + d.37 + d.38 + d.39 + 
##    d.40 + d.41 + &quot;, &quot; d.42 + d.43 + d.44 + d.45 + d.46 + d.47 + d.48 + d.49 + d.50 + &quot;, &quot; d.51 + d.52 + d.53 + 
##    d.54 + d.55 + d.56 + d.57 + d.58 + d.59 + &quot;, &quot; d.60, family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, 
##    control.compute = list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; 
##    control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 
##    1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 4.96, Running = 0.531, Post = 0.87, Total = 6.36 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.634 0.204     -1.035   -0.634     -0.234 -0.634   0
## d.1         -0.427 0.290     -1.001   -0.425      0.136 -0.421   0
## d.2          0.009 0.484     -0.967    0.017      0.936  0.034   0
## d.3          1.421 1.074     -0.583    1.382      3.640  1.306   0
## d.4          0.599 0.404     -0.194    0.600      1.391  0.600   0
## d.5          0.048 0.380     -0.709    0.052      0.782  0.060   0
## d.6         -0.247 0.333     -0.911   -0.244      0.397 -0.237   0
## d.7         -0.294 0.525     -1.367   -0.279      0.695 -0.250   0
## d.8          0.128 0.384     -0.636    0.131      0.871  0.138   0
## d.9         -0.183 0.471     -1.136   -0.173      0.713 -0.153   0
## d.10        -1.349 0.750     -2.950   -1.302     -0.002 -1.208   0
## d.11        -2.343 0.845     -4.166   -2.281     -0.851 -2.155   0
## d.12        -0.012 0.424     -0.861   -0.006      0.803  0.006   0
## d.13         0.274 0.443     -0.606    0.278      1.132  0.286   0
## d.14         1.138 0.276      0.599    1.137      1.681  1.136   0
## d.15         0.064 0.465     -0.869    0.071      0.957  0.085   0
## d.16         0.767 0.469     -0.148    0.766      1.692  0.762   0
## d.17        -0.239 0.467     -1.187   -0.229      0.649 -0.208   0
## d.18        -0.030 0.360     -0.747   -0.027      0.666 -0.019   0
## d.19         0.150 0.434     -0.716    0.155      0.987  0.165   0
## d.20         0.200 0.530     -0.863    0.208      1.220  0.223   0
## d.21         0.161 0.497     -0.834    0.168      1.117  0.182   0
## d.22        -0.673 0.542     -1.796   -0.652      0.332 -0.611   0
## d.23        -0.337 0.566     -1.501   -0.318      0.724 -0.282   0
## d.24        -1.413 0.743     -3.001   -1.367     -0.079 -1.272   0
## d.25         0.413 0.314     -0.205    0.413      1.027  0.415   0
## d.26         0.139 0.563     -0.994    0.149      1.216  0.169   0
## d.27        -0.826 0.418     -1.679   -0.814     -0.037 -0.791   0
## d.28        -0.476 0.377     -1.235   -0.470      0.244 -0.456   0
## d.29        -0.291 0.424     -1.148   -0.283      0.517 -0.266   0
## d.30         0.585 0.321     -0.046    0.585      1.214  0.585   0
## d.31         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.32        -0.641 0.502     -1.676   -0.624      0.296 -0.590   0
## d.33         0.304 0.541     -0.775    0.310      1.348  0.322   0
## d.34         1.222 0.394      0.461    1.217      2.008  1.208   0
## d.35         0.612 0.345     -0.066    0.612      1.289  0.612   0
## d.36         0.020 0.514     -1.018    0.030      1.002  0.049   0
## d.37         0.693 0.551     -0.384    0.692      1.777  0.690   0
## d.38        -0.252 0.574     -1.428   -0.235      0.825 -0.200   0
## d.39         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.40         0.467 0.364     -0.251    0.468      1.178  0.470   0
## d.41         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.42         0.703 0.587     -0.445    0.701      1.859  0.698   0
## d.43         0.739 0.353      0.048    0.739      1.433  0.737   0
## d.44        -0.575 0.474     -1.546   -0.561      0.316 -0.532   0
## d.45        -0.061 0.384     -0.828   -0.056      0.680 -0.047   0
## d.46         0.713 0.293      0.139    0.713      1.288  0.712   0
## d.47         0.447 0.523     -0.590    0.450      1.464  0.456   0
## d.48         0.700 0.360     -0.006    0.700      1.408  0.699   0
## d.49        -1.230 1.047     -3.465   -1.166      0.651 -1.035   0
## d.50         0.483 0.478     -0.461    0.485      1.415  0.489   0
## d.51         0.449 0.377     -0.294    0.451      1.185  0.453   0
## d.52         0.391 0.323     -0.244    0.392      1.022  0.394   0
## d.53         0.286 0.482     -0.675    0.290      1.219  0.300   0
## d.54        -0.669 0.837     -2.437   -0.625      0.856 -0.537   0
## d.55         0.913 0.355      0.221    0.912      1.613  0.909   0
## d.56        -0.777 0.495     -1.799   -0.759      0.144 -0.724   0
## d.57         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.58        -1.120 0.776     -2.772   -1.074      0.278 -0.980   0
## d.59        -0.601 0.448     -1.514   -0.588      0.243 -0.563   0
## d.60        -0.635 0.409     -1.465   -0.625      0.140 -0.606   0
## 
## Expected number of effective parameters(stdev): 54.05(0.00)
## Number of equivalent replicates : 35.78 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2521.28
## Effective number of parameters .................: 52.54
## 
## Marginal log-Likelihood:  -1291.66 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="varying-intercepts-model-1" class="section level2">
<h2>2.2 varying intercepts model</h2>
<div id="rethinking-6" class="section level3">
<h3>2.2 rethinking</h3>
<pre class="r"><code>m2.2 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( a_bar , sigma ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
) ,data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.2, depth= 2)</code></pre>
<pre><code>##               mean         sd       5.5%        94.5%     n_eff     Rhat4
## a[1]  -0.996107330 0.19592475 -1.3094348 -0.692077967 2484.5747 0.9996577
## a[2]  -0.594154682 0.35557674 -1.1541187 -0.047021007 3122.4380 1.0007856
## a[3]  -0.229552734 0.51351340 -1.0193568  0.580140753 3739.0165 0.9995512
## a[4]  -0.190671534 0.29028164 -0.6358995  0.265881731 3134.7466 0.9987445
## a[5]  -0.570016730 0.29015733 -1.0412882 -0.106361911 3194.3514 0.9984883
## a[6]  -0.814954706 0.25014491 -1.2051537 -0.407617696 3190.8892 0.9992445
## a[7]  -0.762882922 0.36645661 -1.3653928 -0.198205754 2896.7886 0.9996983
## a[8]  -0.521513071 0.28712566 -0.9851115 -0.054027997 3454.5797 0.9985428
## a[9]  -0.701817659 0.35033228 -1.2863120 -0.184507965 3062.9538 0.9983213
## a[10] -1.135682281 0.42941279 -1.8717301 -0.475639277 1805.9334 1.0000421
## a[11] -1.556046662 0.43181983 -2.2930849 -0.898404263 1537.7892 0.9993339
## a[12] -0.603280981 0.31431504 -1.0933481 -0.110488422 3228.6105 0.9982891
## a[13] -0.424875098 0.33082477 -0.9666445  0.101239500 3159.0923 0.9989676
## a[14]  0.389257671 0.17942861  0.1065947  0.672658557 2674.3181 0.9991330
## a[15] -0.549871976 0.34808155 -1.1007385 -0.012109765 4211.3454 0.9989464
## a[16] -0.121179191 0.34765939 -0.6714898  0.435804614 2395.4089 0.9990229
## a[17] -0.746889904 0.34722267 -1.3102086 -0.236209706 2989.4806 0.9989135
## a[18] -0.641441963 0.25617976 -1.0526644 -0.245725083 3173.3710 0.9985442
## a[19] -0.502468582 0.30890821 -1.0013226 -0.021326419 3487.4279 0.9988903
## a[20] -0.475904723 0.38559686 -1.0946878  0.123673462 3083.0132 0.9996950
## a[21] -0.495759796 0.37092040 -1.1137579  0.091680001 3333.8886 0.9988145
## a[22] -0.966454348 0.38224511 -1.5853838 -0.375724965 2163.9276 0.9992278
## a[23] -0.762162832 0.39087943 -1.4090828 -0.135948960 2823.5852 0.9995401
## a[24] -1.179964446 0.42482332 -1.8863117 -0.525144548 1706.9077 0.9997857
## a[25] -0.273381695 0.22016327 -0.6328193  0.075379351 3913.7134 0.9986965
## a[26] -0.512912966 0.36975531 -1.1077315  0.075019255 2880.7288 1.0007457
## a[27] -1.178263391 0.28732756 -1.6485937 -0.720886383 3320.1023 0.9985670
## a[28] -0.966939217 0.28199303 -1.4212360 -0.531529057 2845.1033 0.9990477
## a[29] -0.807954377 0.31511616 -1.3228508 -0.312584747 2511.1714 0.9994142
## a[30] -0.138711543 0.23220648 -0.5182603  0.228343630 3003.2278 1.0005298
## a[31] -0.298325859 0.29680213 -0.7604095  0.163153714 3563.0332 0.9996742
## a[32] -0.968821290 0.35777742 -1.5560908 -0.401166427 2424.9760 0.9989993
## a[33] -0.417858740 0.38727830 -1.0265645  0.201783146 3108.7891 0.9986570
## a[34]  0.281387228 0.29746142 -0.1801240  0.753079547 2864.5419 1.0005196
## a[35] -0.130902578 0.24910156 -0.5304665  0.277416738 3326.0389 1.0002942
## a[36] -0.575708557 0.37149991 -1.1905825  0.001852868 3355.2696 0.9997123
## a[37] -0.228457934 0.38792950 -0.8192529  0.407673227 3492.7659 0.9987853
## a[38] -0.719139475 0.39934849 -1.3680209 -0.092439065 3231.8100 0.9989255
## a[39] -0.206424995 0.31955443 -0.7087797  0.303270073 3546.1409 0.9986340
## a[40] -0.258005689 0.25935868 -0.6764363  0.161961230 2880.8890 0.9983749
## a[41] -0.203988963 0.30789341 -0.6857248  0.286382464 3513.7163 0.9990316
## a[42] -0.237021038 0.38853548 -0.8556580  0.394483924 2826.0544 1.0000608
## a[43] -0.035218309 0.27002809 -0.4672409  0.403330745 3418.6322 0.9992207
## a[44] -0.958570540 0.35363674 -1.5312672 -0.428990851 3147.0110 0.9997007
## a[45] -0.647266998 0.28622394 -1.1118641 -0.199464654 3287.8934 0.9990575
## a[46] -0.004107236 0.19760181 -0.3172463  0.314483254 3407.4087 0.9989248
## a[47] -0.343712564 0.36783930 -0.9275199  0.240031744 3086.3240 0.9990967
## a[48] -0.077964830 0.26876530 -0.5163625  0.348245610 3568.8607 0.9992973
## a[49] -0.871456855 0.46307081 -1.6292788 -0.163968958 2970.0630 0.9995418
## a[50] -0.299924507 0.35636098 -0.8711771  0.307174363 3528.8391 0.9986870
## a[51] -0.282822970 0.28529336 -0.7473213  0.185565869 3332.8080 0.9988862
## a[52] -0.292728214 0.23214733 -0.6588154  0.063225954 2192.0239 1.0004922
## a[53] -0.419863307 0.36717683 -1.0181909  0.144170757 3048.4481 0.9987871
## a[54] -0.785669656 0.47609806 -1.5799705 -0.057788548 2539.7148 0.9989268
## a[55]  0.093679060 0.26709618 -0.3347534  0.512358734 2904.5608 0.9999463
## a[56] -1.070400676 0.35631137 -1.6474733 -0.510128499 2678.2090 0.9998367
## a[57] -0.290736827 0.29381447 -0.7470297  0.181113811 2826.5603 0.9991671
## a[58] -1.011053484 0.44658761 -1.7580390 -0.312595177 2639.7600 0.9992693
## a[59] -0.992159951 0.32887941 -1.5436996 -0.492489154 3181.0281 0.9989443
## a[60] -1.055585445 0.30415075 -1.5467792 -0.593302336 3317.4026 0.9998179
## a_bar -0.537572265 0.08672117 -0.6782064 -0.399153100 1822.9567 1.0015843
## sigma  0.521290785 0.08559616  0.4003559  0.667227776  794.4166 1.0002832</code></pre>
</div>
<div id="inla-6" class="section level3">
<h3>2.2 inla</h3>
<pre class="r"><code>m2.2.i &lt;- inla(use.contraception ~ f(district_id, model=&quot;iid&quot;), data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean.intercept=  0 ,
        prec.intercept= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ f(district_id, model = \&quot;iid\&quot;), &quot;, &quot; family = \&quot;binomial\&quot;, data = 
##    d2.i, Ntrials = 1, control.compute = list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, 
##    compute = T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = 
##    list(mean.intercept = 0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.47, Running = 1.24, Post = 0.239, Total = 2.95 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.532 0.084     -0.701   -0.531     -0.371 -0.528   0
## 
## Random effects:
##   Name     Model
##     district_id IID model
## 
## Model hyperparameters:
##                           mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for district_id 4.72 1.64       2.38     4.43       8.72 3.95
## 
## Expected number of effective parameters(stdev): 33.99(4.22)
## Number of equivalent replicates : 56.89 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2514.73
## Effective number of parameters .................: 33.32
## 
## Marginal log-Likelihood:  -1278.85 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean        sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479907 0.0789684 0.3386476 0.4748586 0.6490173 0.4657216</code></pre>
<p>Side note: this is how you calculate the sd from the hyperprior (<span class="math inline">\(\sigma\)</span>)</p>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean        sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479907 0.0789684 0.3386476 0.4748586 0.6490173 0.4657216</code></pre>
<pre class="r"><code># hyperparameter of the precision
m2.2.i.prec &lt;- m2.2.i$internal.marginals.hyperpar

#transform precision to sd using inla.tmarginal
#m2.2.i.prec[[1]] is used to access the actual values inside the list
m2.2.i.sd &lt;- inla.tmarginal(function(x) sqrt(exp(-x)), m2.2.i.prec[[1]])
#plot the post of the sd per district (sigma)
plot(m2.2.i.sd)</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.2%20inla%20hyper%20sd-1.png" width="672" /></p>
<pre class="r"><code>#summary stats for the sd 
m2.2.i.sd.sum &lt;- inla.zmarginal(m2.2.i.sd)</code></pre>
<pre><code>## Mean            0.479907 
## Stdev           0.0789684 
## Quantile  0.025 0.338648 
## Quantile  0.25  0.424442 
## Quantile  0.5   0.474859 
## Quantile  0.75  0.529762 
## Quantile  0.975 0.649017</code></pre>
<pre class="r"><code># this coincides perfectly with the result from bri.hyperpar.summary</code></pre>
</div>
</div>
<div id="plot-of-posterior-mean-probabilities-in-each-district" class="section level2">
<h2>2.3 plot of posterior mean probabilities in each district</h2>
<p>Now let’s extract the samples, compute posterior mean probabilities in each district, and plot it all:</p>
<div id="plot-rethinking" class="section level3">
<h3>plot rethinking</h3>
<pre class="r"><code>post1 &lt;- extract.samples( m2.1 ) 
post2 &lt;- extract.samples( m2.2 )
p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) 
p2 &lt;- apply( inv_logit(post2$a) , 2 , mean )
nd &lt;- max(dat_list$did)
plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; )
points( 1:nd , p1 , pch=16 , col=rangi2 ) 
points( 1:nd , p2 )
abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20rethinking-1.png" width="672" /></p>
</div>
<div id="plot-inla" class="section level3">
<h3>plot inla</h3>
<p><a href="https://people.bath.ac.uk/jjf23/inla/oneway.html" class="uri">https://people.bath.ac.uk/jjf23/inla/oneway.html</a></p>
<p><a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>posterior mean for each district a for the idex fixed effect model m2.1:</strong></p>
<pre class="r"><code># m2.2.i$summary.fixed[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
# we eliminate the first element of this list, the intercept.
m2.1.i.fix&lt;- lapply(m2.1.i$marginals.fixed, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))[-1]</code></pre>
<p><strong>posterior mean for each district a for the varying intercept model m2.2:</strong></p>
<pre class="r"><code># m2.2.i$summary.random[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
m2.2.i.rand&lt;- lapply(m2.2.i$marginals.random$district_id, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))</code></pre>
<pre class="r"><code># sapply(m2.2.i.rand, function(x) x[1]) extracts the first element (the mean) from the summary of the posterior of each district
m2.i.mean &lt;- bind_cols(district= 1:60,mean.m2.1= unlist(sapply(m2.1.i.fix, function(x) x[1])), mean.m2.2=unlist(sapply(m2.2.i.rand, function(x) x[1])))

m2.2.i.abar &lt;- inla.zmarginal( inla.tmarginal (inverse_logit, m2.2.i$marginals.fixed[[&quot;(Intercept)&quot;]] ))</code></pre>
<pre><code>## Mean            0.37015 
## Stdev           0.0193713 
## Quantile  0.025 0.331665 
## Quantile  0.25  0.357166 
## Quantile  0.5   0.370221 
## Quantile  0.75  0.383176 
## Quantile  0.975 0.407995</code></pre>
<pre class="r"><code>m2.i.district.plot &lt;- ggplot() +
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.1), color= &quot;blue&quot;, alpha= 0.5)+
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.2), color= &quot;black&quot;, alpha= 0.5, shape= 1)+
  geom_hline(yintercept=m2.2.i.abar[[1]], linetype=&#39;longdash&#39;) +
  ylim(0,1)+
  labs(y = &quot;prob use contraception&quot;)+
  theme_bw()
  

m2.i.district.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20inla-1.png" width="672" /></p>
<p>The blue points are the fixed estimations. The open points are the varying effects. As you’d expect, they are shrunk towards the mean (the dashed line). Some are shrunk more than others. The third district from the left shrunk a lot.</p>
<p>Let’s look at the sample size in each district:</p>
<pre class="r"><code> table(d$district_id)</code></pre>
<pre><code>## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30 
## 117  20   2  30  39  65  18  37  23  13  21  29  24 118  22  20  24  47  26  15  18  20  15  14  67  13  44  49  32  61 
##  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  33  24  14  35  48  17  13  14  26  41  26  11  45  27  39  86  15  42   4  19  37  61  19   6  45  27  33  10  32  42</code></pre>
<p>District 3 has only 2 women sampled. So it shrinks a lot. There are couple of other districts, like 49 and 54, that also have very few women sampled. But their fixed estimates aren’t as extreme, so they don’t shrink as much as district 3 does. All of this is explained by partial pooling, of course.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
