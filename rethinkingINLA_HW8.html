<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 8 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 8 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="section" class="section level1">
<h1>1.</h1>
<p><strong>Revisit the Reed frog survival data, data(reedfrogs),and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.</strong></p>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs</code></pre>
<div id="varying-intercepts-model" class="section level2">
<h2>1.1 varying intercepts model</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i]</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<pre class="r"><code>dat &lt;- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), 
size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L )
)</code></pre>
<pre class="r"><code>m1.1 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1, depth= 2)</code></pre>
<pre><code>##               mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]   2.166437593 0.9049938  0.87331203  3.670159278 3118.971 0.9986802
## a[2]   3.060779687 1.0976194  1.44734819  4.991934648 3266.505 1.0000620
## a[3]   1.001287512 0.6922426 -0.07231858  2.144960710 3629.332 0.9987845
## a[4]   3.076193235 1.0861624  1.51442923  4.972109806 2481.627 1.0006627
## a[5]   2.127393963 0.8760137  0.87874728  3.664519797 4255.179 0.9996352
## a[6]   2.137409038 0.8633391  0.86329270  3.630269678 2804.030 0.9998589
## a[7]   3.087770237 1.1097585  1.47301102  4.999146510 3297.103 0.9995901
## a[8]   2.151461786 0.8895070  0.81283546  3.643943497 2907.610 0.9988213
## a[9]  -0.169991553 0.6103279 -1.14959499  0.774129311 4132.404 0.9985203
## a[10]  2.146699177 0.8894311  0.85624026  3.696370189 3123.172 0.9998528
## a[11]  0.991050837 0.6711267 -0.02920615  2.092977618 4444.171 0.9990141
## a[12]  0.568511738 0.6085754 -0.38365313  1.565266487 4360.875 0.9994346
## a[13]  1.001827639 0.6653577 -0.01152840  2.073914053 4158.098 0.9996360
## a[14]  0.193335459 0.6312511 -0.83853108  1.168931942 4099.841 0.9990884
## a[15]  2.171223759 0.8998391  0.82576097  3.676038607 3121.328 0.9982859
## a[16]  2.156201415 0.8831249  0.86338161  3.619873802 3090.347 0.9987193
## a[17]  2.920618024 0.7975539  1.74915718  4.318592175 3368.644 0.9991270
## a[18]  2.394386674 0.6459933  1.45490947  3.494785840 3750.969 0.9984919
## a[19]  2.013225063 0.5676467  1.16086088  2.920897990 3834.654 1.0000448
## a[20]  3.672220007 1.0039354  2.24830668  5.465186909 2274.050 0.9990479
## a[21]  2.398232601 0.6835937  1.39980070  3.584458351 2397.505 0.9996292
## a[22]  2.399687212 0.6539262  1.41941448  3.546820940 3689.147 0.9985698
## a[23]  2.402281746 0.6355416  1.46226766  3.491905646 3432.788 0.9994404
## a[24]  1.706101896 0.5072151  0.92635922  2.576352008 4337.514 0.9989751
## a[25] -0.993422228 0.4330722 -1.69042498 -0.331002600 3514.145 0.9995689
## a[26]  0.157406730 0.4021504 -0.46930536  0.794892544 4925.288 0.9988614
## a[27] -1.429623934 0.4910004 -2.22870008 -0.707460439 3482.255 0.9999426
## a[28] -0.477456793 0.3988202 -1.14066725  0.153425883 3570.600 0.9996632
## a[29]  0.163379519 0.4059653 -0.49392553  0.819974379 5504.263 0.9990032
## a[30]  1.461261656 0.5057849  0.68945046  2.332008163 3137.665 1.0003664
## a[31] -0.641119636 0.4189980 -1.31014334  0.009745963 2888.448 0.9998263
## a[32] -0.318343158 0.4004404 -0.97813585  0.325857961 3943.796 0.9985476
## a[33]  3.183711872 0.7687218  2.04888571  4.536520550 2669.556 0.9989114
## a[34]  2.712548096 0.6633234  1.72537826  3.831996006 3981.387 0.9986188
## a[35]  2.708273318 0.6369014  1.78399332  3.780379319 3530.575 0.9990135
## a[36]  2.049435804 0.4989253  1.28692259  2.883263282 3836.672 0.9989311
## a[37]  2.059981628 0.5120934  1.28809931  2.906414491 3783.254 0.9987298
## a[38]  3.903492408 1.0028891  2.46158083  5.617530921 2452.340 0.9984133
## a[39]  2.725380194 0.6712240  1.75377955  3.869846026 3555.565 0.9989544
## a[40]  2.351577571 0.5671053  1.51896990  3.287515821 3094.832 0.9983933
## a[41] -1.806710793 0.4794759 -2.58960672 -1.084556135 4752.083 0.9990433
## a[42] -0.568674334 0.3598801 -1.15386426  0.001559291 3625.787 0.9994131
## a[43] -0.448549336 0.3507221 -1.00729538  0.099242407 4481.959 0.9983974
## a[44] -0.339996683 0.3450300 -0.89173054  0.205312129 4557.361 0.9987608
## a[45]  0.580846030 0.3492325  0.05250947  1.157440891 3934.519 0.9988752
## a[46] -0.567866422 0.3311448 -1.08853630 -0.033264490 3862.929 0.9987312
## a[47]  2.061887049 0.5029216  1.30454235  2.893470854 3679.841 0.9987820
## a[48]  0.004764631 0.3395802 -0.53279937  0.543108753 4017.581 1.0008677
## a_bar  1.352730394 0.2585198  0.94227777  1.777056788 4201.115 0.9990395
## sigma  1.619132759 0.2112360  1.31415379  1.992188334 1469.676 0.9997039</code></pre>
</div>
<div id="inla" class="section level3">
<h3>1.1 INLA</h3>
<p>following example: <a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>Here I’m missing custom priors</strong> I’ll use a half cauchy prior for the <span class="math inline">\(\sigma\)</span> to constrain it to &gt;0 numbers, which is what the exponential does as well.</p>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i &lt;- inla(surv ~ 1 + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), 
##    &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, 
##    control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), 
##    control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = 
##    list(control.link = list(model = \&quot;logit\&quot;)))&quot; ) 
## Time used:
##     Pre = 1.61, Running = 0.277, Post = 0.203, Total = 2.09 
## Fixed effects:
##             mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 1.38 0.256       0.89    1.375      1.901 1.364   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.415 0.109      0.237    0.404      0.661 0.381
## 
## Expected number of effective parameters(stdev): 40.36(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.00
## Deviance Information Criterion (DIC, saturated) ....: 89.62
## Effective number of parameters .....................: 39.50
## 
## Watanabe-Akaike information criterion (WAIC) ...: 205.61
## Effective number of parameters .................: 22.72
## 
## Marginal log-Likelihood:  -140.19 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.1.i$summary.fixed</code></pre>
<pre><code>##                 mean        sd 0.025quant 0.5quant 0.975quant     mode
## (Intercept) 1.380249 0.2562768  0.8904279 1.374944    1.90069 1.364469
##                      kld
## (Intercept) 1.902443e-05</code></pre>
<pre class="r"><code>m1.1.i$summary.hyperpar</code></pre>
<pre><code>##                         mean        sd 0.025quant  0.5quant 0.975quant
## Precision for tank 0.4152469 0.1088217  0.2366839 0.4035007  0.6608823
##                         mode
## Precision for tank 0.3807222</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.1.i)</code></pre>
<pre><code>##                mean        sd   q0.025     q0.5   q0.975     mode
## SD for tank 1.59197 0.2096781 1.231084 1.573964 2.053666 1.539229</code></pre>
<p>it looks like the intercept mean and sd correspond to the <span class="math inline">\(\bar{\alpha}\)</span> mean and sd, and the SD for tank corresponds to the <span class="math inline">\(\sigma\)</span>. this makes sense, because the <span class="math inline">\(\bar{\alpha}\)</span> is the average baseline survival for all the tadpoles, which is what the intercept is. <strong>BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION</strong>.</p>
</div>
</div>
<div id="varying-intercepts-predation" class="section level2">
<h2>1.2 varying intercepts + predation</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>[pred]</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(-0.5,1)</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-1" class="section level3">
<h3>1.2 rethinking</h3>
<pre class="r"><code># pred
m1.2 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.2)</code></pre>
<pre><code>##            mean        sd       5.5%     94.5%    n_eff    Rhat4
## bp    -2.405875 0.3105921 -2.8831414 -1.894289 211.3146 1.011290
## a_bar  2.510497 0.2401391  2.1151589  2.890566 223.0128 1.011007
## sigma  0.833051 0.1474558  0.6134063  1.080726 626.3119 1.007475</code></pre>
</div>
<div id="inla-1" class="section level3">
<h3>1.2 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i &lt;- inla(surv ~ 1 + pred + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + f(tank, model = \&quot;iid\&quot;, hyper = 
##    hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, 
##    control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), 
##    control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = 
##    list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = 
##    list(mean = -0.5, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 
##    1.5))&quot;) 
## Time used:
##     Pre = 1.46, Running = 0.228, Post = 0.185, Total = 1.88 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.393 0.217      1.967    2.393      2.822  2.391   0
## predpred    -2.310 0.285     -2.859   -2.314     -1.736 -2.321   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 1.80 0.681      0.843     1.68       3.46 1.47
## 
## Expected number of effective parameters(stdev): 29.26(3.39)
## Number of equivalent replicates : 1.64 
## 
## Deviance Information Criterion (DIC) ...............: 205.43
## Deviance Information Criterion (DIC, saturated) ....: 78.17
## Effective number of parameters .....................: 29.34
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.53
## Effective number of parameters .................: 19.86
## 
## Marginal log-Likelihood:  -124.71 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.2.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant  0.5quant 0.975quant      mode
## (Intercept)  2.393389 0.2173783   1.966560  2.392846   2.822296  2.391477
## predpred    -2.309696 0.2849520  -2.858789 -2.313815  -1.736031 -2.321433
##                      kld
## (Intercept) 1.097535e-07
## predpred    2.538474e-06</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.2.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7826381 0.1398744 0.5378891 0.7718996 1.087421 0.7524708</code></pre>
</div>
</div>
<div id="varying-intercepts-size" class="section level2">
<h2>1.3 varying intercepts + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(0 , 0.5 ) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-2" class="section level3">
<h3>1.3 rethinking</h3>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs

# size
m1.3 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.3,  depth=2)</code></pre>
<pre><code>##               mean        sd       5.5%       94.5%     n_eff    Rhat4
## a[1]   2.183201750 0.9066915  0.8265096  3.69872054 1036.6662 1.001573
## a[2]   3.111795223 1.1423275  1.4172814  4.98864500 1085.9271 1.000830
## a[3]   1.089040202 0.7908155 -0.1525619  2.36667515  752.6545 1.005404
## a[4]   3.072906150 1.1232536  1.4040616  5.05095597 1111.1250 1.002016
## a[5]   1.989137931 0.9623195  0.5635960  3.59933748  803.1867 1.005046
## a[6]   1.978903093 0.9725602  0.5557594  3.58100065  730.9011 1.003847
## a[7]   2.942840834 1.2096757  1.1262439  4.97949658  768.2465 1.002815
## a[8]   1.981730047 0.9572023  0.5217542  3.55493278  782.6476 1.004020
## a[9]  -0.109418541 0.7115124 -1.2051113  1.02590730  421.6113 1.007245
## a[10]  2.168989798 0.9357719  0.7185817  3.68054860  789.7717 1.001876
## a[11]  1.066900878 0.7763311 -0.1401951  2.33721685  630.3231 1.001446
## a[12]  0.642063163 0.7173615 -0.4865053  1.78836834  692.2918 1.005261
## a[13]  0.831159489 0.7738391 -0.4099222  2.11766179  674.9181 1.000094
## a[14] -0.001096451 0.7397557 -1.1854080  1.18349115  631.2090 1.003737
## a[15]  1.992042095 1.0024748  0.5072724  3.71429020  635.2905 1.004380
## a[16]  1.983041140 0.9612572  0.5102851  3.64280706  832.9212 1.002505
## a[17]  2.948702750 0.8552000  1.6186460  4.37932135  947.9951 1.003481
## a[18]  2.455111294 0.7451941  1.3239640  3.68395083  787.1260 1.002647
## a[19]  2.085052844 0.6673397  1.0571185  3.16757821  563.3195 1.007111
## a[20]  3.673754115 1.0255014  2.1532876  5.42181316  685.5653 1.003182
## a[21]  2.215794693 0.7711558  1.0525414  3.54129526  699.7224 1.004616
## a[22]  2.203065580 0.7722583  1.0266943  3.51211616  625.9504 1.001876
## a[23]  2.197800118 0.7462260  1.0928694  3.43486726  568.8434 1.003534
## a[24]  1.519923445 0.6568160  0.5035511  2.62375966  545.8265 1.006106
## a[25] -0.916751698 0.5708211 -1.8334087 -0.01427132  417.9422 1.006464
## a[26]  0.242926089 0.5208182 -0.5791625  1.05544869  320.8035 1.010067
## a[27] -1.344320024 0.6105883 -2.3033997 -0.38392172  414.7363 1.008918
## a[28] -0.397721285 0.5434892 -1.2474372  0.47344175  403.7376 1.011325
## a[29] -0.046527559 0.5546035 -0.9180489  0.84038272  335.6880 1.007860
## a[30]  1.242808365 0.6155445  0.2541104  2.19135903  411.2551 1.006989
## a[31] -0.835592480 0.5686206 -1.7680839  0.06392141  390.8517 1.005128
## a[32] -0.508627617 0.5629230 -1.4031323  0.41246840  355.0381 1.006071
## a[33]  3.266341863 0.8517038  1.9999749  4.74459845  893.3766 1.003130
## a[34]  2.799676735 0.7454522  1.6580448  4.09063023  779.5767 1.004395
## a[35]  2.774616519 0.7423147  1.6318735  3.98945570  591.6179 1.002160
## a[36]  2.131614674 0.6307850  1.1483666  3.12680838  424.5846 1.008564
## a[37]  1.860778287 0.6330580  0.9061042  2.90164579  463.9257 1.004020
## a[38]  3.760795386 1.0854157  2.2350994  5.65395350  656.5033 1.004770
## a[39]  2.518675898 0.7668500  1.3981106  3.78739294  569.6790 1.004969
## a[40]  2.165864874 0.6795076  1.1488203  3.28828648  520.0225 1.009470
## a[41] -1.721588989 0.6001693 -2.6837209 -0.77116258  458.9846 1.006414
## a[42] -0.479961438 0.5105964 -1.2941345  0.34672728  358.2266 1.010924
## a[43] -0.352259734 0.5174244 -1.1671007  0.48645966  329.8359 1.012516
## a[44] -0.247163984 0.5043204 -1.0562984  0.54489404  319.5141 1.015023
## a[45]  0.371944591 0.5129127 -0.4388541  1.17666954  352.7820 1.007194
## a[46] -0.773618005 0.5248633 -1.6147969  0.08388299  331.8900 1.007230
## a[47]  1.862679388 0.6351135  0.8620731  2.90610042  477.2675 1.002989
## a[48] -0.202586533 0.5177723 -1.0319020  0.64062741  332.7807 1.007186
## s[1]   0.210841851 0.3921024 -0.4097423  0.83580623  208.8583 1.012131
## s[2]  -0.091781405 0.3813955 -0.6858463  0.50563198  196.6321 1.021303
## a_bar  1.292412091 0.3997159  0.6647162  1.93821473  203.9376 1.014972
## sigma  1.611455056 0.2085479  1.2986629  1.97580505  924.9564 1.002358</code></pre>
</div>
<div id="inla-2" class="section level3">
<h3>1.3 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i &lt;- inla(surv ~ 1 + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 1/(0.5^2), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + f(tank, model = 
##    \&quot;iid\&quot;, &quot;, &quot; hyper = hcprior), family = \&quot;binomial\&quot;, data = d1.i, 
##    Ntrials = density, &quot;, &quot; control.compute = list(config = T, dic = TRUE, 
##    waic = TRUE), &quot;, &quot; control.predictor = list(link = 1, compute = T), 
##    control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; 
##    control.fixed = list(mean = 0, prec = 1/(0.5^2), mean.intercept = 0, &quot;, 
##    &quot; prec.intercept = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 2.08, Running = 0.188, Post = 0.213, Total = 2.48 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.272 0.418      0.455    1.271      2.095  1.268   0
## size.small   0.222 0.400     -0.564    0.223      1.007  0.223   0
## size.big    -0.081 0.400     -0.866   -0.082      0.705 -0.082   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.421 0.111      0.239    0.408      0.672 0.385
## 
## Expected number of effective parameters(stdev): 40.42(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.43
## Deviance Information Criterion (DIC, saturated) ....: 90.04
## Effective number of parameters .....................: 39.58
## 
## Watanabe-Akaike information criterion (WAIC) ...: 206.21
## Effective number of parameters .................: 22.90
## 
## Marginal log-Likelihood:  -142.21 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.3.i$summary.fixed</code></pre>
<pre><code>##                    mean        sd 0.025quant    0.5quant 0.975quant       mode
## (Intercept)  1.27186665 0.4177740  0.4549238  1.27059867   2.095210  1.2681232
## size.small   0.22245891 0.4001376 -0.5640040  0.22269633   1.006898  0.2232035
## size.big    -0.08147702 0.4002655 -0.8664804 -0.08182627   0.704788 -0.0824901
##                      kld
## (Intercept) 2.406671e-06
## size.small  1.123068e-07
## size.big    6.786554e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.3.i)</code></pre>
<pre><code>##                 mean        sd   q0.025    q0.5   q0.975     mode
## SD for tank 1.582503 0.2100774 1.220948 1.56446 2.045071 1.529635</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size" class="section level2">
<h2>1.4 varying intercepts + predation + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size + <span class="math inline">\(\gamma\)</span>size</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5)</p>
<p><span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-3" class="section level3">
<h3>1.4 rethinking</h3>
<pre class="r"><code># pred + size 
m1.4 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.4, depth=2)</code></pre>
<pre><code>##              mean        sd         5.5%      94.5%    n_eff    Rhat4
## a[1]   2.44733159 0.7608947  1.257043154  3.6767382 371.4833 1.009319
## a[2]   2.84731377 0.7962248  1.628859464  4.1734615 405.8963 1.009269
## a[3]   1.72099423 0.6805665  0.655749670  2.8348872 319.8094 1.010251
## a[4]   2.84935837 0.7612884  1.719306145  4.0685061 377.2943 1.004206
## a[5]   2.28697065 0.7469723  1.173873067  3.5066056 388.0640 1.012345
## a[6]   2.30843264 0.7574053  1.120747580  3.5338126 335.5995 1.011304
## a[7]   2.75143739 0.7970154  1.520531814  4.1103189 391.6067 1.010763
## a[8]   2.30297483 0.7361587  1.141030387  3.4926868 362.6472 1.014719
## a[9]   2.25889628 0.6457867  1.232953706  3.3029271 201.0410 1.019111
## a[10]  3.51640218 0.7006947  2.450981062  4.6405476 228.0751 1.020811
## a[11]  2.97632195 0.6474334  1.945657303  4.0105092 211.7545 1.020171
## a[12]  2.72395427 0.6757150  1.670142402  3.7863257 224.1531 1.020943
## a[13]  2.72686644 0.6665481  1.681528101  3.7543086 191.3774 1.022829
## a[14]  2.24339429 0.6716290  1.189901494  3.2720424 234.8426 1.017643
## a[15]  3.27941695 0.7160822  2.161377804  4.4552707 242.4735 1.018093
## a[16]  3.27749571 0.6998238  2.237695318  4.4449306 246.7714 1.020222
## a[17]  2.83452607 0.6731221  1.773653672  3.9667390 315.4965 1.016246
## a[18]  2.53911869 0.6390618  1.540017668  3.5699852 325.6941 1.012949
## a[19]  2.27111465 0.6306692  1.306870216  3.2744234 284.1770 1.019224
## a[20]  3.18216189 0.7147995  2.068870819  4.3658171 374.6545 1.008260
## a[21]  2.35059832 0.6609331  1.353365949  3.4436077 331.7987 1.018082
## a[22]  2.32887872 0.6634586  1.309635836  3.4350227 263.9295 1.017677
## a[23]  2.34753683 0.6658589  1.339331552  3.4538918 297.5341 1.014355
## a[24]  1.79624403 0.6038163  0.843594160  2.7963262 255.1853 1.016410
## a[25]  1.64016797 0.5882826  0.673990689  2.5324451 169.7165 1.023127
## a[26]  2.58374562 0.5710032  1.698969730  3.5033433 158.8585 1.025610
## a[27]  1.33960276 0.5991673  0.371549927  2.2620389 192.5197 1.018769
## a[28]  2.06243653 0.5923518  1.099042413  2.9675962 163.3527 1.025546
## a[29]  2.23730033 0.5664296  1.345129352  3.1434487 159.3675 1.027545
## a[30]  3.20909606 0.5918580  2.296285753  4.1370331 190.7992 1.027759
## a[31]  1.60633162 0.5707506  0.709277382  2.4969963 172.9107 1.024017
## a[32]  1.87066193 0.5654404  0.941978444  2.7408128 155.7238 1.026634
## a[33]  3.01042152 0.6724700  1.963347372  4.1248543 316.2843 1.016114
## a[34]  2.73605234 0.6221653  1.757543564  3.7521408 313.4038 1.011136
## a[35]  2.73612193 0.6170238  1.757888782  3.7429435 274.9590 1.010291
## a[36]  2.27462252 0.5821564  1.345676804  3.2148501 183.4580 1.025202
## a[37]  2.02095447 0.5952304  1.074473081  2.9476758 278.2744 1.019216
## a[38]  3.15194558 0.7258374  2.048991107  4.3633206 304.4088 1.011286
## a[39]  2.50882676 0.6350308  1.556533958  3.5551541 331.0955 1.013799
## a[40]  2.24518532 0.5952169  1.331332513  3.2169913 263.7477 1.020761
## a[41]  1.02169474 0.6141986 -0.009818696  1.9462183 158.4342 1.022073
## a[42]  1.98224351 0.5566999  1.112688886  2.8677792 166.6322 1.025998
## a[43]  2.08031451 0.5518083  1.191551615  2.9198582 141.7425 1.030679
## a[44]  2.18535314 0.5451697  1.285041903  3.0541229 160.0009 1.025655
## a[45]  2.61006853 0.5480871  1.733787144  3.4827790 154.4182 1.024914
## a[46]  1.63691612 0.5382373  0.782682131  2.4766757 144.2901 1.034058
## a[47]  3.67826950 0.5903196  2.760999200  4.6280830 185.9146 1.023455
## a[48]  2.11559156 0.5500257  1.252379827  2.9532070 140.6441 1.032159
## bp    -2.45534183 0.2840006 -2.890511456 -1.9942590 509.9200 1.004866
## s[1]   0.33847890 0.3685793 -0.253636314  0.9100528 134.5237 1.037386
## s[2]  -0.09396187 0.3648941 -0.656380734  0.4711403 115.4636 1.037424
## a_bar  2.41624508 0.4081934  1.779261257  3.0576713 101.6121 1.046385
## sigma  0.77221953 0.1490046  0.556731289  1.0192030 592.3804 1.004377</code></pre>
</div>
<div id="inla-3" class="section level3">
<h3>1.4 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i &lt;- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + size.small + size.big + f(tank, &quot;, 
##    &quot; model = \&quot;iid\&quot;, hyper = hcprior), family = \&quot;binomial\&quot;, data = 
##    d1.i, &quot;, &quot; Ntrials = density, control.compute = list(config = T, dic = 
##    TRUE, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = 
##    T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), 
##    &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; 
##    size.big = 0), prec = list(pred = 1, size.small = 1, &quot;, &quot; size.big = 
##    1), mean.intercept = 0, prec.intercept = 1.5))&quot; ) 
## Time used:
##     Pre = 1.8, Running = 0.197, Post = 0.211, Total = 2.21 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.471 0.549      0.393    1.471      2.549  1.471   0
## predpred    -2.561 0.285     -3.125   -2.561     -2.000 -2.560   0
## size.small   1.352 0.561      0.251    1.351      2.452  1.351   0
## size.big     0.855 0.560     -0.244    0.854      1.953  0.854   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.15 0.917      0.951     1.97       4.45 1.68
## 
## Expected number of effective parameters(stdev): 27.70(3.71)
## Number of equivalent replicates : 1.73 
## 
## Deviance Information Criterion (DIC) ...............: 205.18
## Deviance Information Criterion (DIC, saturated) ....: 80.80
## Effective number of parameters .....................: 28.06
## 
## Watanabe-Akaike information criterion (WAIC) ...: 203.18
## Effective number of parameters .................: 19.69
## 
## Marginal log-Likelihood:  -124.56 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.4.i$summary.fixed</code></pre>
<pre><code>##                   mean        sd 0.025quant   0.5quant 0.975quant       mode
## (Intercept)  1.4711483 0.5493772  0.3928099  1.4710314   2.549145  1.4708434
## predpred    -2.5613067 0.2853384 -3.1252940 -2.5608941  -2.000232 -2.5598428
## size.small   1.3515418 0.5607661  0.2506563  1.3514723   2.451813  1.3513794
## size.big     0.8546349 0.5598008 -0.2437793  0.8543749   1.953462  0.8539036
##                      kld
## (Intercept) 1.338582e-07
## predpred    4.159984e-06
## size.small  8.502856e-07
## size.big    2.894649e-08</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.4.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7223338 0.1395241 0.4747482 0.7126375 1.023499 0.6951084</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size-predationsize" class="section level2">
<h2>1.5 varying intercepts + predation + size + predation*size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p><strong>this formula’s wrong</strong></p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>predation + <span class="math inline">\(\gamma\)</span>size + <span class="math inline">\(\eta\)</span>size*predation</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5) <span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]<br />
<span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank] σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-4" class="section level3">
<h3>1.5 rethinking</h3>
<pre class="r"><code># pred + size + interaction 
m1.5 &lt;- ulam(
alist(
S ~ binomial( n , p),
logit(p) &lt;- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)</code></pre>
<pre><code>##              mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1]  -0.04241295 0.8273539 -1.3449925  1.31076348 2763.5443 0.9994998
## z[2]   0.50916333 0.8810758 -0.9019723  1.91385744 3279.4028 0.9989577
## z[3]  -0.95927067 0.7831736 -2.1819246  0.33989673 2746.2905 0.9996184
## z[4]   0.48222192 0.8750085 -0.8960637  1.91462310 3768.2513 0.9997933
## z[5]  -0.03787364 0.8409988 -1.3706002  1.34570033 3937.4825 0.9990001
## z[6]  -0.03799295 0.8158171 -1.3313590  1.27759921 2655.9522 0.9994702
## z[7]   0.51751781 0.8499014 -0.7928190  1.90596417 2893.6272 1.0000428
## z[8]  -0.01196065 0.7957140 -1.2849404  1.27688755 2486.8890 1.0025253
## z[9]  -0.10164706 0.6925030 -1.2078716  0.97701158 1918.0877 0.9996709
## z[10]  1.51694578 0.7309636  0.3786467  2.71290800 2080.9269 0.9994815
## z[11]  0.88632901 0.7041012 -0.2179038  1.99947656 1652.5316 1.0017756
## z[12]  0.51019174 0.7036353 -0.6038235  1.61098576 2194.4224 1.0005032
## z[13]  0.21746490 0.7187864 -0.9181217  1.38593617 2649.3894 0.9984314
## z[14] -0.43913126 0.6914340 -1.5342648  0.69706528 2392.0096 0.9983399
## z[15]  0.96026432 0.7530998 -0.2116081  2.19679769 2036.9541 0.9992734
## z[16]  0.94465461 0.7567283 -0.2091999  2.16064017 2646.6457 0.9985427
## z[17]  0.47414989 0.7638334 -0.7142637  1.72251125 2685.6104 0.9992049
## z[18]  0.05747504 0.7309133 -1.0614908  1.25667297 2223.5420 1.0002726
## z[19] -0.28576193 0.7344616 -1.4685009  0.86141044 2363.9436 0.9995635
## z[20]  0.91029255 0.8201587 -0.3468132  2.23361835 3354.3352 0.9992587
## z[21]  0.10197169 0.7348698 -1.0298484  1.31636706 2432.7165 0.9989487
## z[22]  0.08448720 0.7269257 -1.0506432  1.24361171 2086.7733 0.9997989
## z[23]  0.08373087 0.7460419 -1.0997721  1.28117988 2018.9210 1.0015623
## z[24] -0.57803804 0.6846313 -1.6693409  0.54385824 2710.1919 0.9989396
## z[25] -0.85358008 0.5958825 -1.8174994  0.08306956 1685.5541 0.9995067
## z[26]  0.39677163 0.5518648 -0.4629682  1.29363538 1189.7676 0.9990261
## z[27] -1.28322568 0.6159570 -2.2549280 -0.29105247 2241.5740 0.9994303
## z[28] -0.31065454 0.5523253 -1.1837450  0.54063542 1666.3280 0.9992372
## z[29] -0.51054722 0.5629109 -1.4067330  0.40232450 1936.8856 0.9985383
## z[30]  0.79689571 0.6367165 -0.1571823  1.87054751 1953.7516 0.9989306
## z[31] -1.35455733 0.5640974 -2.2757443 -0.45295707 1708.2108 1.0018627
## z[32] -1.01966991 0.5606369 -1.9193013 -0.11916507 1733.9297 0.9997250
## z[33]  0.67882638 0.7378333 -0.4737864  1.84644820 2678.3566 0.9993159
## z[34]  0.30585899 0.7159169 -0.8053994  1.47633868 2358.8798 0.9990315
## z[35]  0.31741171 0.6887310 -0.7446409  1.43335066 2201.3322 1.0013529
## z[36] -0.29401683 0.6503527 -1.3494616  0.73942579 2552.7318 1.0004939
## z[37] -0.25645428 0.6593112 -1.3211868  0.79089115 1604.3091 0.9992848
## z[38]  1.10387248 0.7906487 -0.1354743  2.38724967 2648.2524 0.9994430
## z[39]  0.36814075 0.7198096 -0.7320115  1.54804831 1910.3291 1.0010523
## z[40]  0.03801285 0.6897448 -1.0734262  1.16809563 2335.4455 0.9989886
## z[41] -1.69771785 0.5895733 -2.6453578 -0.80685107 1726.9287 0.9998572
## z[42] -0.39500613 0.5347995 -1.2385716  0.48558430 1144.9029 1.0039926
## z[43] -0.25242867 0.5093939 -1.0703151  0.57402223 1352.2788 1.0028996
## z[44] -0.13534259 0.4963177 -0.9085957  0.65181819 1482.3300 0.9997620
## z[45] -0.03936456 0.5206774 -0.8581023  0.79044928 1557.9100 1.0002930
## z[46] -1.36723665 0.5231200 -2.2168268 -0.54428027 1472.0474 1.0003421
## z[47]  1.41806035 0.6057516  0.4438276  2.40113015 1556.7209 1.0001748
## z[48] -0.70368250 0.5085891 -1.5254355  0.11465681 1536.2605 1.0001593
## bp[1] -1.84683662 0.3602505 -2.3866038 -1.26766061  960.3354 0.9994452
## bp[2] -2.74944135 0.3752829 -3.3210283 -2.13635939  731.9064 1.0010613
## s[1]   0.10954988 0.3931581 -0.5309625  0.74871383 1193.0038 0.9994544
## s[2]   0.13898970 0.3922129 -0.4862589  0.77983064 1407.3029 0.9988464
## a_bar  2.31900979 0.3973902  1.6671881  2.94899375  960.7497 0.9989921
## sigma  0.75454396 0.1543128  0.5352006  1.01653978  836.4658 0.9992590</code></pre>
<p>I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.</p>
</div>
<div id="inla-4" class="section level3">
<h3>1.5 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i &lt;- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + pred * size.small 
##    + &quot;, &quot; pred * size.big + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, 
##    &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, 
##    control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), 
##    control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = 
##    list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = 
##    list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec 
##    = list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 
##    0, prec.intercept = 1.5))&quot; ) 
## Time used:
##     Pre = 2.84, Running = 0.301, Post = 0.315, Total = 3.46 
## Fixed effects:
##                       mean     sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)          1.469  0.549      0.392    1.469      2.546  1.469   0
## size.small           1.030  0.581     -0.110    1.029      2.171  1.029   0
## size.big             1.171  0.583      0.028    1.171      2.317  1.170   0
## predpred            -1.711 18.258    -37.558   -1.712     34.106 -1.711   0
## size.small:predpred -0.322 18.260    -36.172   -0.323     35.498 -0.322   0
## size.big:predpred   -1.388 18.260    -37.238   -1.389     34.432 -1.388   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.40 1.08       1.02     2.17       5.14 1.82
## 
## Expected number of effective parameters(stdev): 27.14(3.83)
## Number of equivalent replicates : 1.77 
## 
## Deviance Information Criterion (DIC) ...............: 204.28
## Deviance Information Criterion (DIC, saturated) ....: 44.90
## Effective number of parameters .....................: 27.35
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.82
## Effective number of parameters .................: 19.60
## 
## Marginal log-Likelihood:  -127.30 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.5.i$summary.fixed</code></pre>
<pre><code>##                           mean         sd  0.025quant   0.5quant 0.975quant
## (Intercept)          1.4691156  0.5487191   0.3920854  1.4689927   2.545859
## size.small           1.0299129  0.5809366  -0.1095596  1.0294821   2.170743
## size.big             1.1712589  0.5830451   0.0280044  1.1707051   2.316552
## predpred            -1.7112074 18.2583559 -37.5584985 -1.7117217  34.106139
## size.small:predpred -0.3220429 18.2596700 -36.1719087 -0.3225574  35.497868
## size.big:predpred   -1.3880774 18.2597165 -37.2380297 -1.3885914  34.431907
##                          mode          kld
## (Intercept)          1.468792 3.122017e-07
## size.small           1.028668 6.547191e-07
## size.big             1.169646 8.373431e-07
## predpred            -1.711207 4.773591e-10
## size.small:predpred -0.322043 7.925479e-10
## size.big:predpred   -1.388076 1.278905e-09</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.5.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5  q0.975      mode
## SD for tank 0.6878866 0.1385057 0.4415024 0.6784345 0.98649 0.6615539</code></pre>
</div>
</div>
<div id="compare-using-waic" class="section level2">
<h2>compare using WAIC</h2>
<div id="compare-rethinking" class="section level4">
<h4>compare rethinking</h4>
<pre class="r"><code>rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )</code></pre>
<pre><code>##          WAIC       SE     dWAIC      dSE    pWAIC    weight
## m1.2 199.3020 8.764440 0.0000000       NA 19.22893 0.2515492
## m1.4 199.7665 8.736585 0.4645584 1.906993 19.05062 0.1994093
## m1.3 199.8418 7.140701 0.5397752 5.402727 20.71807 0.1920491
## m1.5 199.9619 9.065570 0.6599420 3.029939 19.19786 0.1808499
## m1.1 200.0147 7.205131 0.7126909 5.239820 20.85181 0.1761425</code></pre>
<p>These models are really very similar in expected out-of-sample accuracy. The tank variation is huge. But take a look at the posterior distributions for predation and size. You’ll see that predation does seem to matter, as you’d expect. Size matters a lot less. So while predation doesn’t explain much of the total variation, there is plenty of evidence that it is a real effect. Remember: We don’t select a model using WAIC (or LOO). A predictor can make little difference in total accuracy but still be a real causal effect.</p>
</div>
<div id="compare-inla" class="section level4">
<h4>compare inla</h4>
<pre class="r"><code>inla.models.8.1 &lt;- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic &lt;- function (x){
  x[[&quot;waic&quot;]][[&quot;waic&quot;]]
}

waic.8.1 &lt;- bind_cols(model = c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1</code></pre>
<pre><code>## # A tibble: 5 x 2
##   model   waic
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 m1.1.i  206.
## 2 m1.2.i  203.
## 3 m1.3.i  206.
## 4 m1.4.i  203.
## 5 m1.5.i  203.</code></pre>
<p>Let’s look at all the sigma posterior distributions: The two models that omit predation, m1.1 and m1.3, have larger values of sigma. This is because predation explains some of the variation among tanks. So when you add it to the model, the variation in the tank intercepts gets smaller.</p>
<pre class="r"><code>sigma.8.1 &lt;- bind_cols( model= c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), do.call(rbind.data.frame, lapply(inla.models.8.1 ,bri.hyperpar.summary)))


sigma.8.1</code></pre>
<pre><code>##               model      mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank  m1.1.i 1.5919697 0.2096781 1.2310838 1.5739639 2.053666 1.5392289
## SD for tank1 m1.2.i 0.7826381 0.1398744 0.5378891 0.7718996 1.087421 0.7524708
## SD for tank2 m1.3.i 1.5825033 0.2100774 1.2209479 1.5644595 2.045071 1.5296348
## SD for tank3 m1.4.i 0.7223338 0.1395241 0.4747482 0.7126375 1.023499 0.6951084
## SD for tank4 m1.5.i 0.6878866 0.1385057 0.4415024 0.6784345 0.986490 0.6615539</code></pre>
<pre class="r"><code>sigma.8.1.plot &lt;-  ggplot(data= sigma.8.1, aes(y=model, x=mean, label=model)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=q0.025, xmax=q0.975), height=.3) +
    coord_fixed(ratio=.3) +
    theme_bw()

sigma.8.1.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/sigma.8.1%20plot-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p><strong>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</strong></p>
<p><strong>(1) district: ID number of administrative district each woman resided in</strong></p>
<p><strong>(2) use.contraception: An indicator (0/1) of whether the woman was using contraception</strong></p>
<p><strong>Focus on predicting use.contraception, clustered by district_id. Fit both:</strong></p>
<p><strong>1) a traditional fixed-effects model that uses an index variable for district</strong></p>
<p><strong>2) a multilevel model with varying intercepts for district.</strong></p>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?**</p>
<pre class="r"><code>library(rethinking)
data(bangladesh)
d &lt;- bangladesh</code></pre>
<p>The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:</p>
<pre class="r"><code>sort(unique(d$district))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
## [51] 51 52 53 55 56 57 58 59 60 61</code></pre>
<p>District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:</p>
<pre class="r"><code>d$district_id &lt;- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
## [51] 51 52 53 54 55 56 57 58 59 60</code></pre>
<p>Now there are 60 values, contiguous integers 1 to 60.</p>
<div id="traditional-fixed-effects-model-that-uses-an-index-variable-for-district" class="section level2">
<h2>2.1 traditional fixed-effects model that uses an index variable for district</h2>
<div id="rethinking-5" class="section level3">
<h3>2.1 rethinking</h3>
<pre class="r"><code>dat_list &lt;- list(
C = d$use.contraception, 
did = d$district_id
)

m2.1 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( 0 , 1.5 )
) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.1, depth = 2)</code></pre>
<pre><code>##               mean        sd        5.5%       94.5%    n_eff     Rhat4
## a[1]  -1.056260452 0.2110542 -1.40805220 -0.72077433 4933.239 0.9986415
## a[2]  -0.602463159 0.4718091 -1.33977664  0.11854563 4128.214 0.9987299
## a[3]   1.231170766 1.1400016 -0.51992412  3.12752286 4554.501 0.9983108
## a[4]   0.001914560 0.3743173 -0.59960902  0.60015808 5856.501 0.9987352
## a[5]  -0.565375682 0.3286770 -1.09034403 -0.06134702 4638.202 0.9984401
## a[6]  -0.865922880 0.2631488 -1.29096626 -0.45145941 5501.753 0.9990602
## a[7]  -0.904921590 0.5206410 -1.75476614 -0.11216573 4672.326 0.9987512
## a[8]  -0.489131434 0.3269760 -0.98644608  0.02300524 4889.061 0.9987867
## a[9]  -0.795879553 0.4288621 -1.51109080 -0.14086800 5083.457 0.9988103
## a[10] -1.966522285 0.7337365 -3.20804773 -0.87221097 4264.121 0.9986291
## a[11] -2.957761924 0.8092457 -4.39197006 -1.78030377 3214.961 0.9985208
## a[12] -0.613710312 0.3875750 -1.23888919 -0.02293618 5571.219 0.9981884
## a[13] -0.316427896 0.3948852 -0.94311271  0.30919442 3928.587 0.9989191
## a[14]  0.519802336 0.1917238  0.21472286  0.81894505 4830.310 0.9986532
## a[15] -0.533922550 0.4294610 -1.22364447  0.11233808 3297.486 0.9997048
## a[16]  0.183037005 0.4336097 -0.48932753  0.86883588 5594.803 0.9985480
## a[17] -0.859705454 0.4266257 -1.56140939 -0.18430068 5694.645 0.9987858
## a[18] -0.653685861 0.3073162 -1.15420710 -0.16268236 5627.995 0.9998094
## a[19] -0.452070307 0.3873752 -1.07320433  0.15259111 4544.125 0.9983596
## a[20] -0.392858442 0.5068792 -1.22365405  0.41581511 4575.348 0.9986380
## a[21] -0.438961854 0.4771507 -1.22742349  0.29960535 4710.748 0.9984029
## a[22] -1.292156334 0.5308576 -2.21511069 -0.47191383 5649.799 0.9994352
## a[23] -0.921776086 0.5567310 -1.84312334 -0.04068760 3730.405 0.9997200
## a[24] -2.016732018 0.7215060 -3.18674046 -0.92398439 4796.112 0.9983741
## a[25] -0.203347874 0.2536170 -0.61582220  0.20479192 4967.185 0.9988811
## a[26] -0.440916072 0.5381114 -1.28923879  0.41447321 4392.595 0.9986287
## a[27] -1.453519114 0.3688131 -2.06599988 -0.88962100 4041.058 0.9990243
## a[28] -1.096051362 0.3433342 -1.65517722 -0.56464921 4951.429 0.9986140
## a[29] -0.901172756 0.3845283 -1.51904171 -0.30918064 4887.731 0.9987637
## a[30] -0.035009482 0.2575711 -0.44486049  0.37643276 4473.578 0.9984978
## a[31] -0.185877631 0.3435389 -0.76262363  0.37005716 3693.766 0.9993170
## a[32] -1.256749376 0.4704062 -2.05161457 -0.55695145 4210.698 0.9983727
## a[33] -0.263553973 0.5198644 -1.10458620  0.56025998 4248.260 0.9992566
## a[34]  0.626634998 0.3489661  0.07394586  1.21538455 3463.838 0.9994894
## a[35]  0.001463959 0.2820236 -0.45035175  0.44210991 6261.290 0.9993503
## a[36] -0.564695861 0.4914717 -1.38788993  0.17175505 5529.642 0.9982393
## a[37]  0.141740366 0.5436847 -0.70434222  0.99618807 5344.251 0.9982582
## a[38] -0.848779892 0.5410116 -1.71563493 -0.03078356 4568.903 0.9985739
## a[39]  0.004987229 0.3814705 -0.60371747  0.60349668 4783.481 0.9983577
## a[40] -0.140843757 0.3014551 -0.61722251  0.32648042 4497.813 0.9984710
## a[41] -0.001989600 0.3831527 -0.60100182  0.60624504 5434.800 0.9983634
## a[42]  0.163863592 0.5704841 -0.72755265  1.08177736 6138.988 0.9989548
## a[43]  0.131638645 0.2971864 -0.34391681  0.61814356 5777.994 0.9984256
## a[44] -1.193249080 0.4473983 -1.93431342 -0.48862546 4005.787 0.9994707
## a[45] -0.672420966 0.3272160 -1.21813719 -0.14231605 4760.999 0.9986814
## a[46]  0.089500183 0.2095743 -0.23725964  0.42261827 4414.916 0.9986984
## a[47] -0.117851848 0.5129214 -0.94005354  0.71479692 4530.050 0.9993960
## a[48]  0.088636019 0.3086783 -0.42509210  0.57346886 4490.818 0.9994021
## a[49] -1.709540664 1.0690700 -3.52528073 -0.07743692 4910.758 0.9986828
## a[50] -0.096828784 0.4515283 -0.81365725  0.59794208 4523.105 0.9986574
## a[51] -0.153509576 0.3273878 -0.67998217  0.35777746 4279.938 0.9988977
## a[52] -0.222236063 0.2614212 -0.63243079  0.19199286 4404.626 0.9985168
## a[53] -0.306054742 0.4466965 -1.02561073  0.39741857 4429.054 0.9986054
## a[54] -1.213481159 0.8592116 -2.65147322  0.10317034 4777.065 0.9986097
## a[55]  0.304474881 0.3174281 -0.20165719  0.80471010 4673.083 0.9992591
## a[56] -1.398227899 0.4672585 -2.17085485 -0.67619233 3804.023 0.9997033
## a[57] -0.176323394 0.3612232 -0.75823304  0.39753221 5187.777 0.9988071
## a[58] -1.712792458 0.7700087 -3.08048340 -0.54778786 3593.524 0.9999824
## a[59] -1.221101381 0.4021453 -1.87239979 -0.59434162 4723.849 0.9985873
## a[60] -1.255101334 0.3783780 -1.86692864 -0.67025583 4004.099 0.9997332</code></pre>
</div>
<div id="inla-5" class="section level3">
<h3>2.1 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d2.i &lt;- d %&gt;% 
  mutate(did= paste(&quot;d&quot;, as.integer(d$district_id), sep= &quot;.&quot;), 
         d.value= 1
         ) %&gt;% 
  spread(did, d.value)

#use this to quickly make a list of the index vbles to include in the model 
did_formula &lt;- paste(&quot;d&quot;, 1:60, sep=&quot;.&quot;, collapse = &quot;+&quot;)


m2.1.i &lt;- inla(use.contraception ~ d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9+d.10+d.11+d.12+d.13+d.14+d.15+d.16+d.17+d.18+d.19+d.20+d.21+d.22+d.23+d.24+d.25+d.26+d.27+d.28+d.29+d.30+d.31+d.32+d.33+d.34+d.35+d.36+d.37+d.38+d.39+d.40+d.41+d.42+d.43+d.44+d.45+d.46+d.47+d.48+d.49+d.50+d.51+d.52+d.53+d.54+d.55+d.56+d.57+d.58+d.59+d.60, data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean=  0 ,
        prec= 1.5),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ d.1 + d.2 + d.3 + d.4 + d.5 + &quot;, 
##    &quot; d.6 + d.7 + d.8 + d.9 + d.10 + d.11 + d.12 + d.13 + d.14 + &quot;, &quot; d.15 
##    + d.16 + d.17 + d.18 + d.19 + d.20 + d.21 + d.22 + d.23 + &quot;, &quot; d.24 + 
##    d.25 + d.26 + d.27 + d.28 + d.29 + d.30 + d.31 + d.32 + &quot;, &quot; d.33 + 
##    d.34 + d.35 + d.36 + d.37 + d.38 + d.39 + d.40 + d.41 + &quot;, &quot; d.42 + 
##    d.43 + d.44 + d.45 + d.46 + d.47 + d.48 + d.49 + d.50 + &quot;, &quot; d.51 + 
##    d.52 + d.53 + d.54 + d.55 + d.56 + d.57 + d.58 + d.59 + &quot;, &quot; d.60, 
##    family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, control.compute = 
##    list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, 
##    compute = T), &quot;, &quot; control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1.5))&quot;) 
## Time used:
##     Pre = 5.42, Running = 0.594, Post = 0.793, Total = 6.81 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.582 0.121     -0.818   -0.582     -0.345 -0.581   0
## d.1         -0.455 0.233     -0.920   -0.453     -0.005 -0.447   0
## d.2         -0.030 0.416     -0.865   -0.023      0.769 -0.011   0
## d.3          0.645 0.708     -0.743    0.644      2.036  0.642   0
## d.4          0.485 0.349     -0.201    0.486      1.167  0.487   0
## d.5          0.000 0.326     -0.650    0.004      0.629  0.011   0
## d.6         -0.275 0.279     -0.833   -0.271      0.263 -0.263   0
## d.7         -0.268 0.443     -1.166   -0.259      0.575 -0.240   0
## d.8          0.071 0.330     -0.586    0.075      0.709  0.082   0
## d.9         -0.190 0.403     -1.004   -0.182      0.581 -0.167   0
## d.10        -0.920 0.542     -2.037   -0.902      0.091 -0.865   0
## d.11        -1.527 0.535     -2.639   -1.505     -0.539 -1.461   0
## d.12        -0.051 0.365     -0.783   -0.045      0.652 -0.035   0
## d.13         0.194 0.383     -0.568    0.198      0.935  0.205   0
## d.14         1.046 0.217      0.623    1.045      1.474  1.043   0
## d.15         0.015 0.400     -0.787    0.021      0.786  0.032   0
## d.16         0.601 0.403     -0.190    0.601      1.393  0.600   0
## d.17        -0.238 0.400     -1.045   -0.230      0.525 -0.214   0
## d.18        -0.072 0.306     -0.684   -0.068      0.520 -0.061   0
## d.19         0.088 0.375     -0.660    0.093      0.812  0.101   0
## d.20         0.123 0.452     -0.781    0.129      0.996  0.140   0
## d.21         0.095 0.426     -0.758    0.100      0.917  0.111   0
## d.22        -0.563 0.447     -1.476   -0.551      0.280 -0.526   0
## d.23        -0.290 0.472     -1.246   -0.279      0.606 -0.258   0
## d.24        -0.972 0.536     -2.077   -0.953      0.027 -0.916   0
## d.25         0.341 0.260     -0.173    0.342      0.849  0.344   0
## d.26         0.074 0.475     -0.879    0.080      0.989  0.093   0
## d.27        -0.761 0.352     -1.479   -0.752     -0.096 -0.734   0
## d.28        -0.471 0.320     -1.117   -0.465      0.139 -0.453   0
## d.29        -0.293 0.363     -1.026   -0.286      0.401 -0.272   0
## d.30         0.500 0.268     -0.027    0.500      1.025  0.501   0
## d.31         0.337 0.338     -0.332    0.339      0.996  0.342   0
## d.32        -0.558 0.420     -1.414   -0.547      0.235 -0.524   0
## d.33         0.203 0.460     -0.714    0.208      1.094  0.217   0
## d.34         1.041 0.335      0.393    1.038      1.709  1.032   0
## d.35         0.517 0.293     -0.058    0.518      1.091  0.518   0
## d.36        -0.019 0.439     -0.901   -0.012      0.824  0.001   0
## d.37         0.504 0.467     -0.415    0.505      1.417  0.507   0
## d.38        -0.223 0.478     -1.191   -0.213      0.687 -0.193   0
## d.39         0.473 0.367     -0.251    0.474      1.191  0.476   0
## d.40         0.379 0.311     -0.236    0.380      0.987  0.383   0
## d.41         0.473 0.367     -0.251    0.474      1.191  0.476   0
## d.42         0.495 0.492     -0.474    0.496      1.456  0.498   0
## d.43         0.631 0.300      0.043    0.631      1.220  0.630   0
## d.44        -0.516 0.400     -1.331   -0.506      0.242 -0.486   0
## d.45        -0.097 0.329     -0.756   -0.092      0.537 -0.083   0
## d.46         0.631 0.237      0.166    0.631      1.096  0.630   0
## d.47         0.319 0.447     -0.569    0.322      1.188  0.328   0
## d.48         0.592 0.307     -0.011    0.592      1.195  0.592   0
## d.49        -0.619 0.672     -1.981   -0.605      0.660 -0.576   0
## d.50         0.361 0.412     -0.455    0.364      1.164  0.368   0
## d.51         0.360 0.324     -0.280    0.361      0.991  0.364   0
## d.52         0.319 0.270     -0.213    0.320      0.845  0.322   0
## d.53         0.198 0.416     -0.630    0.202      1.002  0.210   0
## d.54        -0.415 0.612     -1.656   -0.401      0.747 -0.374   0
## d.55         0.789 0.301      0.202    0.788      1.382  0.785   0
## d.56        -0.674 0.412     -1.516   -0.662      0.102 -0.639   0
## d.57         0.337 0.338     -0.332    0.339      0.996  0.342   0
## d.58        -0.742 0.564     -1.899   -0.724      0.318 -0.690   0
## d.59        -0.550 0.379     -1.321   -0.541      0.168 -0.522   0
## d.60        -0.599 0.347     -1.302   -0.591      0.059 -0.575   0
## 
## Expected number of effective parameters(stdev): 46.14(0.00)
## Number of equivalent replicates : 41.92 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2515.50
## Effective number of parameters .................: 44.53
## 
## Marginal log-Likelihood:  -1273.53 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="varying-intercepts-model-1" class="section level2">
<h2>2.2 varying intercepts model</h2>
<div id="rethinking-6" class="section level3">
<h3>2.2 rethinking</h3>
<pre class="r"><code>m2.2 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( a_bar , sigma ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
) ,data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.2, depth= 2)</code></pre>
<pre><code>##               mean         sd       5.5%         94.5%     n_eff     Rhat4
## a[1]  -0.998501331 0.19433228 -1.3092453 -6.835142e-01 2930.6327 0.9989185
## a[2]  -0.592716396 0.35742501 -1.1615162 -3.388958e-02 4161.8037 0.9989333
## a[3]  -0.246180483 0.51263854 -1.0254745  5.789192e-01 2410.3787 0.9993795
## a[4]  -0.190135668 0.31939604 -0.7052630  3.157327e-01 2407.1235 1.0010000
## a[5]  -0.572717228 0.29093739 -1.0413490 -1.188944e-01 4298.4521 0.9988433
## a[6]  -0.826456389 0.23799561 -1.2057893 -4.552401e-01 2670.9013 0.9988173
## a[7]  -0.745803849 0.36595403 -1.3320372 -1.888969e-01 2498.4462 0.9983489
## a[8]  -0.526101427 0.28107307 -0.9794547 -7.823871e-02 2528.9737 0.9990072
## a[9]  -0.722762177 0.34700314 -1.2845070 -1.760861e-01 2959.1177 0.9991659
## a[10] -1.142834212 0.41525360 -1.8403120 -5.078728e-01 2274.4698 0.9987684
## a[11] -1.562327841 0.42870328 -2.2827841 -9.043656e-01 2105.5310 0.9988476
## a[12] -0.620254192 0.31385054 -1.1243393 -1.231812e-01 3574.5332 0.9988305
## a[13] -0.437579641 0.31821700 -0.9520933  6.522307e-02 3311.6347 0.9984519
## a[14]  0.389042848 0.18411902  0.1025289  6.856141e-01 3207.3294 0.9999858
## a[15] -0.557214230 0.32372912 -1.0659400 -5.293942e-02 2363.0620 0.9998131
## a[16] -0.127028301 0.32823416 -0.6291945  4.053187e-01 3809.2953 0.9996631
## a[17] -0.762212833 0.34720803 -1.3426098 -2.388035e-01 3126.8767 0.9994144
## a[18] -0.634749289 0.27023654 -1.0584686 -2.123135e-01 3365.0764 0.9998772
## a[19] -0.501628037 0.31948647 -1.0215857 -8.550326e-05 3357.5649 0.9996637
## a[20] -0.484331406 0.36769227 -1.0881112  1.090888e-01 3910.5230 0.9991972
## a[21] -0.515291654 0.37092004 -1.0994117  8.747443e-02 2576.2626 0.9996107
## a[22] -0.984347042 0.37997366 -1.6070102 -4.007885e-01 3110.6385 0.9988114
## a[23] -0.768275656 0.39474513 -1.4039394 -1.509182e-01 3489.4743 1.0002113
## a[24] -1.197470863 0.45336366 -1.9423343 -5.007452e-01 2067.4348 0.9993647
## a[25] -0.274870641 0.22393383 -0.6388765  7.671975e-02 3200.5975 0.9992106
## a[26] -0.525318221 0.38739464 -1.1595147  4.655494e-02 3559.9831 0.9985343
## a[27] -1.200763665 0.31607897 -1.7134130 -7.045100e-01 2858.2291 0.9985381
## a[28] -0.969655968 0.27443005 -1.4258469 -5.536299e-01 3467.4234 0.9992202
## a[29] -0.809892441 0.30492146 -1.3074100 -3.318453e-01 3679.4322 0.9981575
## a[30] -0.137191734 0.22814027 -0.4988173  2.242952e-01 3884.2691 0.9983574
## a[31] -0.300462998 0.27668106 -0.7450743  1.405943e-01 3002.5669 0.9991642
## a[32] -0.988234711 0.35531115 -1.5725423 -4.317657e-01 3208.6128 0.9989360
## a[33] -0.426480323 0.39892948 -1.0684821  2.276270e-01 3211.1090 0.9988106
## a[34]  0.273697946 0.29530993 -0.1896877  7.557404e-01 2705.3734 0.9990830
## a[35] -0.142492880 0.25428387 -0.5534007  2.632101e-01 3447.7648 0.9989088
## a[36] -0.601027399 0.36716895 -1.1651798 -2.655782e-02 4332.4825 0.9996830
## a[37] -0.220239046 0.38662629 -0.8261459  4.118280e-01 3160.4347 0.9988029
## a[38] -0.722846708 0.39003403 -1.3521235 -1.062704e-01 3603.3323 0.9981177
## a[39] -0.204479979 0.32200360 -0.7033279  3.109867e-01 3308.1353 0.9984265
## a[40] -0.260487622 0.27683629 -0.6949077  1.830596e-01 3419.2126 0.9987265
## a[41] -0.205159797 0.32853591 -0.7207918  3.215794e-01 2670.1565 0.9998639
## a[42] -0.253049546 0.40489038 -0.8792986  4.026254e-01 2495.3123 1.0001402
## a[43] -0.044468253 0.25287380 -0.4545525  3.730515e-01 3368.4027 0.9989089
## a[44] -0.966522966 0.34327781 -1.5208187 -4.458875e-01 2917.2287 1.0005921
## a[45] -0.660865192 0.28684048 -1.1258981 -1.990745e-01 3460.5159 0.9985700
## a[46] -0.004302815 0.19838783 -0.3185380  3.193976e-01 3806.5364 0.9991751
## a[47] -0.345143032 0.36954953 -0.9363349  2.422902e-01 4259.3902 0.9994607
## a[48] -0.074127232 0.25769141 -0.4868754  3.275199e-01 3526.0400 0.9989653
## a[49] -0.871701538 0.48235448 -1.6608618 -1.482810e-01 2892.6208 0.9996907
## a[50] -0.311028528 0.36078989 -0.8953787  2.497186e-01 3099.9880 0.9988219
## a[51] -0.273264997 0.28199684 -0.7213320  1.680206e-01 3082.5865 0.9996543
## a[52] -0.297232079 0.22839767 -0.6610264  6.685356e-02 3195.4464 0.9992723
## a[53] -0.433464860 0.37177330 -0.9997143  1.548975e-01 3643.2720 0.9983181
## a[54] -0.804610759 0.44255969 -1.5359685 -1.279695e-01 2620.4866 0.9999812
## a[55]  0.098143182 0.26541956 -0.3096398  5.184503e-01 2874.6095 0.9985480
## a[56] -1.085887199 0.34681554 -1.6532943 -5.449222e-01 2385.3130 0.9995566
## a[57] -0.292159801 0.28446218 -0.7286822  1.574148e-01 2938.8503 0.9994875
## a[58] -1.022212237 0.44186991 -1.7544064 -3.268529e-01 2584.7525 1.0002716
## a[59] -1.002056740 0.31305546 -1.5134644 -5.246802e-01 2108.9922 0.9989147
## a[60] -1.059404239 0.30735400 -1.5461504 -5.678086e-01 2890.4331 0.9985733
## a_bar -0.546382811 0.08989047 -0.6948087 -4.077156e-01 1605.7151 1.0007831
## sigma  0.520504315 0.08471291  0.3961312  6.610745e-01  798.6929 1.0010212</code></pre>
</div>
<div id="inla-6" class="section level3">
<h3>2.2 inla</h3>
<pre class="r"><code>m2.2.i &lt;- inla(use.contraception ~ f(district_id, model=&quot;iid&quot;), data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean.intercept=  0 ,
        prec.intercept= 1.5),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ f(district_id, model = \&quot;iid\&quot;), 
##    &quot;, &quot; family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, control.compute = 
##    list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, 
##    compute = T), &quot;, &quot; control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean.intercept = 0, 
##    prec.intercept = 1.5))&quot; ) 
## Time used:
##     Pre = 1.49, Running = 1.15, Post = 0.274, Total = 2.92 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.529 0.083     -0.696   -0.527     -0.368 -0.525   0
## 
## Random effects:
##   Name     Model
##     district_id IID model
## 
## Model hyperparameters:
##                           mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for district_id 4.73 1.64       2.38     4.45       8.75 3.96
## 
## Expected number of effective parameters(stdev): 33.96(4.23)
## Number of equivalent replicates : 56.94 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2514.77
## Effective number of parameters .................: 33.28
## 
## Marginal log-Likelihood:  -1278.40 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479265 0.07887791 0.3381567 0.4742283 0.6481667 0.4651158</code></pre>
<p>Side note: this is how you calculate the sd from the hyperprior (<span class="math inline">\(\sigma\)</span>)</p>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479265 0.07887791 0.3381567 0.4742283 0.6481667 0.4651158</code></pre>
<pre class="r"><code># hyperparameter of the precision
m2.2.i.prec &lt;- m2.2.i$internal.marginals.hyperpar

#transform precision to sd using inla.tmarginal
#m2.2.i.prec[[1]] is used to access the actual values inside the list
m2.2.i.sd &lt;- inla.tmarginal(function(x) sqrt(exp(-x)), m2.2.i.prec[[1]])
#plot the post of the sd per district (sigma)
plot(m2.2.i.sd)</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.2%20inla%20hyper%20sd-1.png" width="672" /></p>
<pre class="r"><code>#summary stats for the sd 
m2.2.i.sd.sum &lt;- inla.zmarginal(m2.2.i.sd)</code></pre>
<pre><code>## Mean            0.479265 
## Stdev           0.0788779 
## Quantile  0.025 0.338157 
## Quantile  0.25  0.423862 
## Quantile  0.5   0.474228 
## Quantile  0.75  0.529068 
## Quantile  0.975 0.648167</code></pre>
<pre class="r"><code># this coincides perfectly with the result from bri.hyperpar.summary</code></pre>
</div>
</div>
<div id="plot-of-posterior-mean-probabilities-in-each-district" class="section level2">
<h2>2.3 plot of posterior mean probabilities in each district</h2>
<p>Now let’s extract the samples, compute posterior mean probabilities in each district, and plot it all:</p>
<div id="plot-rethinking" class="section level3">
<h3>plot rethinking</h3>
<pre class="r"><code>post1 &lt;- extract.samples( m2.1 ) 
post2 &lt;- extract.samples( m2.2 )
p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) 
p2 &lt;- apply( inv_logit(post2$a) , 2 , mean )
nd &lt;- max(dat_list$did)
plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; )
points( 1:nd , p1 , pch=16 , col=rangi2 ) 
points( 1:nd , p2 )
abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20rethinking-1.png" width="672" /></p>
</div>
<div id="plot-inla" class="section level3">
<h3>plot inla</h3>
<p><a href="https://people.bath.ac.uk/jjf23/inla/oneway.html" class="uri">https://people.bath.ac.uk/jjf23/inla/oneway.html</a></p>
<p><a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>posterior mean for each district a for the idex fixed effect model m2.1:</strong></p>
<pre class="r"><code># m2.2.i$summary.fixed[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
# we eliminate the first element of this list, the intercept.
m2.1.i.fix&lt;- lapply(m2.1.i$marginals.fixed, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))[-1]</code></pre>
<p><strong>posterior mean for each district a for the varying intercept model m2.2:</strong></p>
<pre class="r"><code># m2.2.i$summary.random[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
m2.2.i.rand&lt;- lapply(m2.2.i$marginals.random$district_id, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))</code></pre>
<pre class="r"><code># sapply(m2.2.i.rand, function(x) x[1]) extracts the first element (the mean) from the summary of the posterior of each district
m2.i.mean &lt;- bind_cols(district= 1:60,mean.m2.1= unlist(sapply(m2.1.i.fix, function(x) x[1])), mean.m2.2=unlist(sapply(m2.2.i.rand, function(x) x[1])))

m2.2.i.abar &lt;- inla.zmarginal( inla.tmarginal (inverse_logit, m2.2.i$marginals.fixed[[&quot;(Intercept)&quot;]] ))</code></pre>
<pre><code>## Mean            0.371073 
## Stdev           0.019274 
## Quantile  0.025 0.332833 
## Quantile  0.25  0.358143 
## Quantile  0.5   0.371126 
## Quantile  0.75  0.384026 
## Quantile  0.975 0.408776</code></pre>
<pre class="r"><code>m2.i.district.plot &lt;- ggplot() +
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.1), color= &quot;blue&quot;, alpha= 0.5)+
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.2), color= &quot;black&quot;, alpha= 0.5, shape= 1)+
  geom_hline(yintercept=m2.2.i.abar[[1]], linetype=&#39;longdash&#39;) +
  ylim(0,1)+
  labs(y = &quot;prob use contraception&quot;)+
  theme_bw()
  

m2.i.district.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20inla-1.png" width="672" /></p>
<p>The blue points are the fixed estimations. The open points are the varying effects. As you’d expect, they are shrunk towards the mean (the dashed line). Some are shrunk more than others. The third district from the left shrunk a lot. Let’s look at the sample size in each district:</p>
<pre class="r"><code> table(d$district_id)</code></pre>
<pre><code>## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
## 117  20   2  30  39  65  18  37  23  13  21  29  24 118  22  20  24  47  26  15 
##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 
##  18  20  15  14  67  13  44  49  32  61  33  24  14  35  48  17  13  14  26  41 
##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  26  11  45  27  39  86  15  42   4  19  37  61  19   6  45  27  33  10  32  42</code></pre>
<p>District 3 has only 2 women sampled. So it shrinks a lot. There are couple of other districts, like 49 and 54, that also have very few women sampled. But their fixed estimates aren’t as extreme, so they don’t shrink as much as district 3 does. All of this is explained by partial pooling, of course.</p>
</div>
</div>
</div>
<div id="section-2" class="section level1">
<h1>3.</h1>
<p>I don’t really care about ordered categorical data so i’m going to skip exercise 3.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
