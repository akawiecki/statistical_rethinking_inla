<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 8 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
<li>
  <a href="rethinkingINLA_HW9.html">Homework 9</a>
</li>
<li>
  <a href="rethinkingINLA_HW10.html">Homework 10</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 8 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="datareedfrogs-add-the-predation-and-size-treatment-variables-to-the-varying-intercepts-model" class="section level1">
<h1>1. data(reedfrogs) add the predation and size treatment variables to the varying intercepts model</h1>
<p><strong>Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.</strong></p>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs</code></pre>
<div id="varying-intercepts-model" class="section level2">
<h2>1.1 varying intercepts model</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i]</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<pre class="r"><code>dat &lt;- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), 
size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L )
)</code></pre>
<pre class="r"><code>m1.1 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1, depth= 2)</code></pre>
<pre><code>##               mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]   2.119567487 0.8848916  0.83659128  3.645672617 3844.783 0.9991020
## a[2]   3.042482752 1.0773284  1.49555738  4.970948912 2652.989 0.9987811
## a[3]   0.999294097 0.6726570 -0.01285850  2.155039143 3914.640 0.9991128
## a[4]   3.051707539 1.0568778  1.51356371  4.853901008 3206.081 1.0000088
## a[5]   2.122628355 0.8653377  0.87115899  3.588698784 2748.998 0.9988220
## a[6]   2.127679004 0.8520236  0.88038752  3.564726231 3938.738 0.9997947
## a[7]   3.028808181 1.0473179  1.50051561  4.835774152 3250.945 0.9987182
## a[8]   2.126438970 0.8799787  0.83928605  3.647600581 2982.811 0.9992453
## a[9]  -0.172750274 0.6352641 -1.18506175  0.823479681 4235.505 1.0004391
## a[10]  2.125901277 0.8935699  0.81270580  3.619008389 3456.305 0.9993935
## a[11]  0.994981910 0.6763741 -0.01855946  2.113784975 3049.083 0.9995543
## a[12]  0.593895799 0.6214454 -0.38614945  1.635032628 3895.143 0.9984980
## a[13]  1.006984330 0.6931374 -0.07233021  2.166628600 3569.373 0.9985275
## a[14]  0.199471435 0.5878051 -0.71683189  1.145730450 4608.997 0.9995793
## a[15]  2.110177567 0.8673686  0.83886645  3.488819474 3728.671 0.9982769
## a[16]  2.134462008 0.8564843  0.91327740  3.609322865 2936.983 0.9986705
## a[17]  2.909921932 0.8139215  1.74686445  4.300943181 3356.415 0.9992976
## a[18]  2.390053061 0.6285290  1.46627686  3.497717911 2826.518 0.9991003
## a[19]  2.020166592 0.5629077  1.17513999  2.961829306 3443.178 1.0001294
## a[20]  3.659053249 1.0272591  2.18871767  5.456153695 2377.250 0.9990054
## a[21]  2.388510273 0.6607146  1.41514095  3.521682341 3128.677 0.9989563
## a[22]  2.395065032 0.6475553  1.42869162  3.451304939 2918.056 1.0001051
## a[23]  2.393583296 0.6573572  1.45888552  3.548910933 3535.759 0.9996133
## a[24]  1.691140734 0.5339994  0.86628831  2.595017500 2855.766 0.9987956
## a[25] -0.997337075 0.4549275 -1.71986144 -0.286696054 4279.699 0.9991001
## a[26]  0.162858100 0.4040649 -0.49016482  0.821639452 3892.655 0.9997321
## a[27] -1.425288012 0.4911761 -2.26203405 -0.656832516 3526.274 0.9988928
## a[28] -0.450548383 0.4004686 -1.10519597  0.186671867 4165.740 0.9987955
## a[29]  0.156297627 0.4025998 -0.47880936  0.791874313 3917.868 1.0008058
## a[30]  1.441842617 0.4791926  0.73613182  2.227886595 3772.677 0.9987016
## a[31] -0.647882909 0.4127819 -1.30035707 -0.002800635 3924.574 0.9990751
## a[32] -0.311398443 0.3865897 -0.92889128  0.284293038 3209.211 0.9997920
## a[33]  3.206691993 0.8033667  2.05422481  4.601489159 2756.241 0.9996188
## a[34]  2.701945477 0.6438784  1.75297156  3.854051563 3750.463 1.0006065
## a[35]  2.702330707 0.6252815  1.78110086  3.787097768 3009.150 0.9989269
## a[36]  2.055279160 0.5208582  1.27266092  2.940231030 4992.146 0.9986847
## a[37]  2.047982289 0.4860151  1.33457938  2.871242636 4344.558 0.9985015
## a[38]  3.886743630 0.9734088  2.50514892  5.538363471 2615.839 1.0003373
## a[39]  2.705372419 0.6273058  1.78915901  3.775551646 3103.756 0.9987359
## a[40]  2.353574453 0.5761582  1.48020668  3.322725872 3889.837 0.9995215
## a[41] -1.808950462 0.4904749 -2.61384896 -1.059876658 2765.496 0.9996807
## a[42] -0.567733046 0.3460649 -1.12343380 -0.021636144 4491.718 0.9987877
## a[43] -0.449035998 0.3448587 -0.99763257  0.096723573 4461.489 0.9982710
## a[44] -0.343073300 0.3384694 -0.87850716  0.189269612 3605.011 0.9989444
## a[45]  0.581377697 0.3419052  0.04324944  1.141682884 3484.756 0.9983844
## a[46] -0.577699350 0.3465018 -1.14331696 -0.037445458 4268.779 0.9993508
## a[47]  2.062054037 0.5208948  1.27437309  2.953038343 4572.409 0.9986130
## a[48]  0.006473232 0.3300812 -0.52493019  0.548414422 4664.614 0.9990782
## a_bar  1.340099949 0.2534662  0.92526546  1.742928672 2598.033 0.9988648
## sigma  1.615359541 0.2146268  1.30473874  1.994961261 1627.711 1.0035417</code></pre>
</div>
<div id="inla" class="section level3">
<h3>1.1 INLA</h3>
<p>following example: <a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>Here I’m missing custom priors</strong> I’ll use a half cauchy prior for the <span class="math inline">\(\sigma\)</span> to constrain it to &gt;0 numbers, which is what the exponential does as well.</p>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i &lt;- inla(surv ~ 1 + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, 
##    Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link 
##    = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)))&quot; ) 
## Time used:
##     Pre = 1.32, Running = 0.182, Post = 0.198, Total = 1.7 
## Fixed effects:
##             mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 1.38 0.256       0.89    1.375      1.901 1.364   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.415 0.109      0.237    0.404      0.661 0.381
## 
## Expected number of effective parameters(stdev): 40.36(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.00
## Deviance Information Criterion (DIC, saturated) ....: 89.62
## Effective number of parameters .....................: 39.50
## 
## Watanabe-Akaike information criterion (WAIC) ...: 205.61
## Effective number of parameters .................: 22.72
## 
## Marginal log-Likelihood:  -140.19 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.1.i$summary.fixed</code></pre>
<pre><code>##                 mean        sd 0.025quant 0.5quant 0.975quant     mode          kld
## (Intercept) 1.380249 0.2562768  0.8904279 1.374944    1.90069 1.364469 1.902443e-05</code></pre>
<pre class="r"><code>m1.1.i$summary.hyperpar</code></pre>
<pre><code>##                         mean        sd 0.025quant  0.5quant 0.975quant      mode
## Precision for tank 0.4152469 0.1088217  0.2366839 0.4035007  0.6608823 0.3807222</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.1.i)</code></pre>
<pre><code>##                mean        sd   q0.025     q0.5   q0.975     mode
## SD for tank 1.59197 0.2096781 1.231084 1.573964 2.053666 1.539229</code></pre>
<p>it looks like the intercept mean and sd correspond to the <span class="math inline">\(\bar{\alpha}\)</span> mean and sd, and the SD for tank corresponds to the <span class="math inline">\(\sigma\)</span>. this makes sense, because the <span class="math inline">\(\bar{\alpha}\)</span> is the average baseline survival for all the tadpoles, which is what the intercept is. <strong>BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION</strong>.</p>
</div>
</div>
<div id="varying-intercepts-predation" class="section level2">
<h2>1.2 varying intercepts + predation</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>[pred]</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(-0.5,1)</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-1" class="section level3">
<h3>1.2 rethinking</h3>
<pre class="r"><code># pred
m1.2 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.2)</code></pre>
<pre><code>##             mean        sd       5.5%     94.5%    n_eff    Rhat4
## bp    -2.4472107 0.3032715 -2.9061977 -1.955052 192.6095 1.018283
## a_bar  2.5462824 0.2418258  2.1601562  2.923047 215.7680 1.013680
## sigma  0.8320242 0.1458875  0.6240988  1.083203 565.2540 1.003836</code></pre>
</div>
<div id="inla-1" class="section level3">
<h3>1.2 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i &lt;- inla(surv ~ 1 + pred + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = 
##    d1.i, Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = 
##    list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; 
##    control.fixed = list(mean = -0.5, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.4, Running = 0.2, Post = 0.207, Total = 1.8 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.523 0.225      2.088    2.520      2.974  2.515   0
## predpred    -2.435 0.288     -2.997   -2.437     -1.863 -2.440   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 1.80 0.677       0.85     1.68       3.46 1.48
## 
## Expected number of effective parameters(stdev): 28.98(3.36)
## Number of equivalent replicates : 1.66 
## 
## Deviance Information Criterion (DIC) ...............: 204.42
## Deviance Information Criterion (DIC, saturated) ....: 79.99
## Effective number of parameters .....................: 29.04
## 
## Watanabe-Akaike information criterion (WAIC) ...: 201.45
## Effective number of parameters .................: 19.59
## 
## Marginal log-Likelihood:  -122.10 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.2.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant  0.5quant 0.975quant      mode          kld
## (Intercept)  2.523164 0.2252490   2.087742  2.520414   2.974171  2.514827 2.391528e-07
## predpred    -2.435197 0.2876891  -2.996768 -2.437046  -1.863173 -2.440294 8.877976e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.2.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7815832 0.1385647 0.5384615 0.7711308 1.082806 0.7517625</code></pre>
</div>
</div>
<div id="varying-intercepts-size" class="section level2">
<h2>1.3 varying intercepts + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(0 , 0.5 ) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-2" class="section level3">
<h3>1.3 rethinking</h3>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs

# size
m1.3 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.3,  depth=2)</code></pre>
<pre><code>##              mean        sd       5.5%       94.5%     n_eff     Rhat4
## a[1]   2.18898817 0.9384156  0.7347717  3.76113136  711.5521 1.0020366
## a[2]   3.12198466 1.1685099  1.3778550  5.08065844 1027.7035 1.0030355
## a[3]   1.08852854 0.7801769 -0.1256012  2.31705640  660.6838 1.0039728
## a[4]   3.12313336 1.1589778  1.3660099  5.11659551  829.9941 1.0012942
## a[5]   1.95108515 0.9623239  0.4819281  3.55055731  840.4359 1.0012558
## a[6]   1.99735144 0.9734838  0.4917197  3.58878113  771.2402 1.0014136
## a[7]   2.96695092 1.1758170  1.2010503  4.92946192  986.8630 1.0027964
## a[8]   2.00759692 0.9782686  0.5160118  3.64664085  947.3894 1.0025943
## a[9]  -0.09117895 0.7416291 -1.2640746  1.07392296  629.2907 1.0071652
## a[10]  2.17947708 0.9326692  0.7392605  3.72507612  902.6823 1.0034595
## a[11]  1.07967828 0.8043697 -0.1120249  2.36207161  664.0967 1.0024082
## a[12]  0.67148655 0.7566807 -0.5077835  1.91553283  583.7025 1.0021641
## a[13]  0.83844784 0.7935057 -0.4158039  2.10687854  607.3823 1.0006897
## a[14]  0.01880965 0.7404162 -1.1567304  1.17857079  718.7508 1.0012194
## a[15]  2.00264149 0.9815170  0.4979337  3.61159590  902.6631 1.0009507
## a[16]  1.97402761 0.9593764  0.5361561  3.58365008  947.7739 1.0007476
## a[17]  2.99538980 0.8603466  1.6847150  4.42997343  653.4269 1.0046518
## a[18]  2.48448302 0.7557603  1.2846464  3.69950450  614.1017 1.0053764
## a[19]  2.09441448 0.6945814  0.9974654  3.21234692  540.3439 1.0029897
## a[20]  3.77206523 1.1393613  2.1388439  5.71512583  911.7986 1.0017168
## a[21]  2.21505409 0.7759598  1.0726394  3.51250350  671.4964 1.0003839
## a[22]  2.20535074 0.7686597  1.0431593  3.47590217  789.2993 0.9999141
## a[23]  2.23328288 0.7854437  1.0399233  3.55114144  619.8531 1.0039794
## a[24]  1.50350995 0.6692535  0.4718781  2.61002622  620.7920 0.9994823
## a[25] -0.91463798 0.6196250 -1.8974072  0.04141713  452.7935 1.0068627
## a[26]  0.26142680 0.5605704 -0.6566025  1.15649848  381.0940 1.0062854
## a[27] -1.34494780 0.6486263 -2.4140477 -0.32720217  471.4787 1.0065380
## a[28] -0.38463124 0.5906502 -1.3360435  0.55603088  397.6037 1.0117084
## a[29] -0.03925833 0.5802325 -0.9238482  0.92422046  461.3923 1.0007607
## a[30]  1.25882042 0.6315500  0.2797728  2.27675307  606.3561 0.9997124
## a[31] -0.83638743 0.5862206 -1.8026891  0.07455676  463.1839 1.0008355
## a[32] -0.51641484 0.5678540 -1.4346668  0.41038247  465.0183 1.0017870
## a[33]  3.27281485 0.8357646  2.0416361  4.64739430  737.4079 1.0098891
## a[34]  2.80182810 0.7318570  1.6489076  3.99204411  631.6691 1.0061263
## a[35]  2.82076457 0.7818760  1.6326460  4.12095306  511.0310 1.0041796
## a[36]  2.16661832 0.6695732  1.1162670  3.29300372  495.6910 1.0028439
## a[37]  1.86936283 0.6606978  0.8387403  2.93380794  532.4899 1.0002598
## a[38]  3.78103080 1.0845211  2.2813917  5.66369077 1003.1986 1.0008476
## a[39]  2.52268618 0.7583104  1.3797950  3.81822903  588.4218 1.0010955
## a[40]  2.16225483 0.6871946  1.1050263  3.30723835  655.7043 1.0023945
## a[41] -1.72381939 0.6404404 -2.7682613 -0.73272540  389.6836 1.0091801
## a[42] -0.47568019 0.5382679 -1.3091706  0.39481114  354.0477 1.0122767
## a[43] -0.34918596 0.5342353 -1.2010867  0.50110467  360.2174 1.0115292
## a[44] -0.24311203 0.5348933 -1.1182486  0.60009639  348.6286 1.0105824
## a[45]  0.37818788 0.5309857 -0.4495691  1.26485755  429.2635 1.0016115
## a[46] -0.77271757 0.5178076 -1.5845138  0.06622915  402.8541 1.0020265
## a[47]  1.86600383 0.6429408  0.8690730  2.93985862  553.2409 0.9998290
## a[48] -0.19640361 0.5215797 -1.0191611  0.65128006  395.0872 1.0031727
## s[1]   0.20886478 0.4115230 -0.4664083  0.84519525  279.7924 1.0029835
## s[2]  -0.10656738 0.4146616 -0.7951817  0.54751870  229.5102 1.0178210
## a_bar  1.30960463 0.4376978  0.6205985  2.01801916  280.2104 1.0069587
## sigma  1.63015577 0.2142267  1.3151002  2.00212809 1071.2558 0.9991454</code></pre>
</div>
<div id="inla-2" class="section level3">
<h3>1.3 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i &lt;- inla(surv ~ 1 + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 1/(0.5^2), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + f(tank, model = \&quot;iid\&quot;, &quot;, &quot; hyper = hcprior), family = 
##    \&quot;binomial\&quot;, data = d1.i, Ntrials = density, &quot;, &quot; control.compute = list(config = T, dic = TRUE, waic = TRUE), &quot;, 
##    &quot; control.predictor = list(link = 1, compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), 
##    &quot;, &quot; control.fixed = list(mean = 0, prec = 1/(0.5^2), mean.intercept = 0, &quot;, &quot; prec.intercept = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.45, Running = 0.183, Post = 0.202, Total = 1.83 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.272 0.418      0.455    1.271      2.095  1.268   0
## size.small   0.222 0.400     -0.564    0.223      1.007  0.223   0
## size.big    -0.081 0.400     -0.866   -0.082      0.705 -0.082   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.421 0.111      0.239    0.408      0.672 0.385
## 
## Expected number of effective parameters(stdev): 40.42(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.43
## Deviance Information Criterion (DIC, saturated) ....: 90.04
## Effective number of parameters .....................: 39.58
## 
## Watanabe-Akaike information criterion (WAIC) ...: 206.21
## Effective number of parameters .................: 22.90
## 
## Marginal log-Likelihood:  -142.21 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.3.i$summary.fixed</code></pre>
<pre><code>##                    mean        sd 0.025quant    0.5quant 0.975quant       mode          kld
## (Intercept)  1.27186665 0.4177740  0.4549238  1.27059867   2.095210  1.2681232 2.406671e-06
## size.small   0.22245891 0.4001376 -0.5640040  0.22269633   1.006898  0.2232035 1.123068e-07
## size.big    -0.08147702 0.4002655 -0.8664804 -0.08182627   0.704788 -0.0824901 6.786554e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.3.i)</code></pre>
<pre><code>##                 mean        sd   q0.025    q0.5   q0.975     mode
## SD for tank 1.582503 0.2100774 1.220948 1.56446 2.045071 1.529635</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size" class="section level2">
<h2>1.4 varying intercepts + predation + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size + <span class="math inline">\(\gamma\)</span>size</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5)</p>
<p><span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-3" class="section level3">
<h3>1.4 rethinking</h3>
<pre class="r"><code># pred + size 
m1.4 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: The largest R-hat is 1.06, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.4, depth=2)</code></pre>
<pre><code>##              mean        sd        5.5%      94.5%     n_eff    Rhat4
## a[1]   2.40593549 0.7273387  1.28531418  3.6211389 265.94536 1.017889
## a[2]   2.84943974 0.7645645  1.68114497  4.0693721 382.32978 1.014508
## a[3]   1.70527635 0.6839864  0.62213526  2.8012887 218.50593 1.020856
## a[4]   2.85545399 0.8068562  1.63788999  4.1498836 261.29965 1.016240
## a[5]   2.27400984 0.7799188  1.03291658  3.5531918 333.50734 1.009004
## a[6]   2.28198261 0.7768806  1.07448089  3.5006666 278.13881 1.015298
## a[7]   2.75229595 0.7899049  1.54903205  4.0926458 314.99198 1.014211
## a[8]   2.27664705 0.7845495  0.98054484  3.5276487 259.71745 1.014183
## a[9]   2.22172821 0.6726824  1.15495578  3.3391321 138.84121 1.029334
## a[10]  3.49095732 0.7144866  2.41171899  4.6898528 191.31451 1.023924
## a[11]  2.97332104 0.6697838  1.90230527  4.0538612 160.44454 1.025946
## a[12]  2.72619804 0.6785298  1.69635472  3.8221360 140.49215 1.036486
## a[13]  2.70992340 0.6725705  1.66396093  3.8006779 152.45007 1.027337
## a[14]  2.21036655 0.6606039  1.21738169  3.3104243 113.17073 1.030136
## a[15]  3.25745999 0.7229004  2.17106606  4.4296638 199.06733 1.024357
## a[16]  3.26193332 0.7159314  2.13461900  4.4415943 221.96654 1.014483
## a[17]  2.82794538 0.6617123  1.84536386  3.9661587 242.16980 1.026802
## a[18]  2.54175472 0.6521791  1.54342501  3.6079931 174.66856 1.029835
## a[19]  2.26083204 0.6160532  1.30391526  3.2695697 186.88325 1.024463
## a[20]  3.16270511 0.7191333  2.08112056  4.3625092 282.99895 1.020089
## a[21]  2.31792477 0.6683181  1.23436763  3.4018960 251.58700 1.016955
## a[22]  2.33063227 0.6752181  1.29066064  3.4306011 236.15493 1.020081
## a[23]  2.30393564 0.6910542  1.23740125  3.4459207 229.32284 1.015196
## a[24]  1.76555889 0.6178044  0.81183105  2.7411766 194.58798 1.022666
## a[25]  1.62015498 0.6067828  0.61208932  2.5641074 107.62456 1.037328
## a[26]  2.56080728 0.5803752  1.61117637  3.4728274 108.38602 1.033824
## a[27]  1.31774104 0.6174147  0.30186160  2.2646173 112.70661 1.032711
## a[28]  2.04507702 0.5948874  1.08616890  2.9861930 119.72179 1.037703
## a[29]  2.22596673 0.5858800  1.27702172  3.1738410 105.94089 1.031797
## a[30]  3.19414665 0.6260356  2.22500242  4.2375212 128.75779 1.029903
## a[31]  1.57412688 0.6053336  0.58917631  2.5380151 117.84328 1.030353
## a[32]  1.82852717 0.5882692  0.90029508  2.7755550 106.10283 1.032311
## a[33]  2.99303025 0.6376725  2.01904890  4.0786992 205.86149 1.027503
## a[34]  2.72866610 0.6303675  1.76123960  3.7835174 230.26940 1.023441
## a[35]  2.72174934 0.6372994  1.72621599  3.7482013 196.34628 1.020472
## a[36]  2.27583446 0.6052583  1.31401772  3.2818679 190.69932 1.022444
## a[37]  1.99998550 0.6159253  0.96062343  2.9517202 221.86880 1.018565
## a[38]  3.14268293 0.7622093  1.99504782  4.3488099 283.69718 1.020719
## a[39]  2.50162324 0.6407838  1.48070435  3.5517750 217.55517 1.018171
## a[40]  2.23744989 0.6372922  1.24525495  3.3009839 201.95666 1.017188
## a[41]  0.99017821 0.6265322 -0.03178578  1.9542844  99.23352 1.039141
## a[42]  1.96986051 0.5781497  1.01159994  2.8672618  98.48237 1.041143
## a[43]  2.06805630 0.5701951  1.15077718  2.9437762 121.42178 1.038625
## a[44]  2.16805824 0.5623423  1.25792323  3.0445500  90.36190 1.050211
## a[45]  2.57985720 0.5716371  1.67817139  3.4693599 108.52734 1.034481
## a[46]  1.58953034 0.5671241  0.67300720  2.4882596 109.78374 1.030088
## a[47]  3.66497535 0.6040904  2.68325967  4.6241715 137.94863 1.027617
## a[48]  2.09388390 0.5667674  1.20301463  3.0001882 112.58984 1.033994
## bp    -2.44379995 0.2935337 -2.90068264 -1.9743742 344.55517 1.012420
## s[1]   0.35563669 0.3849750 -0.25962426  0.9791268  97.31991 1.042479
## s[2]  -0.08770717 0.3732941 -0.66969755  0.5065023  90.70773 1.054477
## a_bar  2.40121164 0.4178624  1.73777906  3.0547659  69.89030 1.060742
## sigma  0.77978819 0.1497700  0.55509924  1.0350630 371.26773 1.009919</code></pre>
</div>
<div id="inla-3" class="section level3">
<h3>1.4 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i &lt;- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + size.small + size.big + f(tank, &quot;, &quot; model = \&quot;iid\&quot;, hyper = hcprior), family 
##    = \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = density, control.compute = list(config = T, dic = TRUE, &quot;, &quot; waic = 
##    TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec = 
##    list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.5, Running = 0.182, Post = 0.239, Total = 1.92 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.161 0.666      0.854    2.161      3.469  2.160   0
## predpred    -2.628 0.290     -3.204   -2.626     -2.062 -2.622   0
## size.small   0.729 0.656     -0.559    0.729      2.016  0.729   0
## size.big     0.231 0.656     -1.056    0.231      1.517  0.231   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.13 0.907      0.941     1.95       4.40 1.66
## 
## Expected number of effective parameters(stdev): 27.66(3.70)
## Number of equivalent replicates : 1.74 
## 
## Deviance Information Criterion (DIC) ...............: 204.82
## Deviance Information Criterion (DIC, saturated) ....: 80.44
## Effective number of parameters .....................: 27.99
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.79
## Effective number of parameters .................: 19.63
## 
## Marginal log-Likelihood:  -123.31 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.4.i$summary.fixed</code></pre>
<pre><code>##                   mean        sd 0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)  2.1613253 0.6663035  0.8539924  2.1610031   3.469238  2.1604153 1.094653e-07
## predpred    -2.6276156 0.2895818 -3.2042666 -2.6256993  -2.062158 -2.6218552 3.292691e-06
## size.small   0.7293355 0.6560320 -0.5587394  0.7293248   2.016310  0.7293579 4.990355e-07
## size.big     0.2308486 0.6557372 -1.0562819  0.2307203   1.517498  0.2305195 1.043209e-08</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.4.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7260825 0.1402651 0.4772367 0.7163201 1.028863 0.6986102</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size-predationsize" class="section level2">
<h2>1.5 varying intercepts + predation + size + predation*size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p><strong>this formula’s wrong</strong></p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>predation + <span class="math inline">\(\gamma\)</span>size + <span class="math inline">\(\eta\)</span>size*predation</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5) <span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]<br />
<span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank] σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-4" class="section level3">
<h3>1.5 rethinking</h3>
<pre class="r"><code># pred + size + interaction 
m1.5 &lt;- ulam(
alist(
S ~ binomial( n , p),
logit(p) &lt;- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)</code></pre>
<pre><code>##              mean        sd        5.5%       94.5%     n_eff     Rhat4
## z[1]  -0.05505751 0.8414668 -1.36160051  1.37542396 1888.4081 1.0005846
## z[2]   0.46407910 0.8800442 -0.95553079  1.91746363 2088.2293 1.0042641
## z[3]  -0.96038497 0.7934150 -2.22401432  0.33609919 1811.0128 0.9989574
## z[4]   0.48447866 0.9368421 -1.00908180  1.96904734 2180.7375 0.9990356
## z[5]  -0.02924825 0.8509092 -1.35782327  1.36083969 2304.8299 1.0007155
## z[6]   0.00429677 0.8286623 -1.28328078  1.36293250 1791.0587 0.9997902
## z[7]   0.47581803 0.8918619 -0.93125379  1.89540867 2542.2495 0.9994498
## z[8]  -0.03862281 0.8447005 -1.33987435  1.30414881 2614.1636 0.9996270
## z[9]  -0.10383142 0.6896333 -1.21092770  1.03789984 1518.4607 1.0025731
## z[10]  1.53658606 0.7113044  0.42471656  2.68184649 2365.7988 0.9994049
## z[11]  0.86130711 0.6896722 -0.24715414  2.01534488 1849.0944 0.9998587
## z[12]  0.53381186 0.6881794 -0.51313941  1.66017123 1832.9084 0.9989429
## z[13]  0.22882084 0.7320653 -0.94074922  1.42711730 2256.1497 1.0014339
## z[14] -0.44174913 0.6807436 -1.55840232  0.64239750 2091.6767 1.0004022
## z[15]  0.96111726 0.7646075 -0.22777492  2.24985465 2500.6192 0.9987967
## z[16]  0.95178223 0.7340392 -0.21171156  2.13172290 2663.3590 0.9998780
## z[17]  0.44169252 0.8040546 -0.80223314  1.75604803 2794.5455 0.9991080
## z[18]  0.06426809 0.7444372 -1.10035287  1.28388155 2462.3268 0.9996043
## z[19] -0.27300843 0.7015636 -1.41610644  0.87368493 1996.3143 0.9997163
## z[20]  0.88350614 0.8193057 -0.40157002  2.25858101 2882.9582 0.9999766
## z[21]  0.06901104 0.7442845 -1.06619265  1.30380467 2330.2530 1.0012207
## z[22]  0.08415857 0.7373584 -1.08037400  1.26713961 2059.0488 1.0003774
## z[23]  0.08590868 0.7152202 -1.03663683  1.26268704 2253.7275 1.0002601
## z[24] -0.58016629 0.6734948 -1.61255791  0.52599379 1915.8032 1.0007018
## z[25] -0.86245983 0.5702820 -1.76154691  0.02995247 1814.4802 0.9996075
## z[26]  0.39890133 0.5537093 -0.46072990  1.30514198 1573.3369 1.0003826
## z[27] -1.25730758 0.5734204 -2.21472882 -0.37921091 1818.4953 0.9982733
## z[28] -0.30061069 0.5487413 -1.15886015  0.59897120 1637.2475 0.9993352
## z[29] -0.51370182 0.5407130 -1.36860196  0.33573703 1296.4883 1.0009673
## z[30]  0.81737803 0.5963519 -0.09153100  1.82713082 1411.4696 1.0013588
## z[31] -1.36020316 0.5761349 -2.26314879 -0.45192879 1341.4580 1.0005101
## z[32] -1.01373970 0.5649942 -1.91006817 -0.14156621 1691.6273 0.9993652
## z[33]  0.64734100 0.7521526 -0.49843731  1.90797017 1876.0895 1.0006553
## z[34]  0.30853661 0.7405384 -0.84317519  1.53770392 2186.2265 0.9994367
## z[35]  0.31869940 0.7223603 -0.83219403  1.49929473 1842.0247 1.0031961
## z[36] -0.30217149 0.6474711 -1.31903218  0.73726380 1992.5398 0.9989524
## z[37] -0.26630132 0.6400686 -1.23862542  0.79821271 1793.4600 1.0018599
## z[38]  1.13707800 0.7809463 -0.04964148  2.42374569 2105.0129 0.9989650
## z[39]  0.33633054 0.7201782 -0.78346307  1.51029660 1740.8855 1.0007121
## z[40]  0.01695059 0.6656364 -1.03708955  1.09966390 2244.4976 0.9992437
## z[41] -1.69394603 0.5713443 -2.59964901 -0.80127019 1673.9970 1.0015450
## z[42] -0.40919906 0.5190374 -1.26256893  0.41388988 1501.0468 1.0003857
## z[43] -0.25688301 0.4997487 -1.06746375  0.54335084 1336.8184 0.9998689
## z[44] -0.12046742 0.5119636 -0.90650556  0.71169697 1283.0729 1.0020110
## z[45] -0.03139145 0.5209747 -0.82264128  0.80079472 1241.8069 1.0006945
## z[46] -1.35433596 0.5291976 -2.23716771 -0.55256124 1566.3839 1.0013019
## z[47]  1.42710169 0.6284646  0.44710411  2.46350563 1212.6553 1.0020980
## z[48] -0.70480023 0.5148718 -1.49150606  0.10595230 1172.4948 0.9992361
## bp[1] -1.85293241 0.3708686 -2.44137917 -1.26808200  660.3477 1.0063281
## bp[2] -2.76878637 0.3901926 -3.36162671 -2.12332337  857.7597 1.0019234
## s[1]   0.09765280 0.3866296 -0.53582543  0.71882524  933.4086 1.0045526
## s[2]   0.14208529 0.4001821 -0.51015801  0.78251666  853.2210 1.0082065
## a_bar  2.33165767 0.4002863  1.65552317  2.95076493  780.4183 1.0074677
## sigma  0.75342053 0.1484197  0.52700929  1.00113316  725.0585 1.0053401</code></pre>
<p>I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.</p>
</div>
<div id="inla-4" class="section level3">
<h3>1.5 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i &lt;- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + pred * size.small + &quot;, &quot; pred * size.big + f(tank, model = 
##    \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, control.compute = 
##    list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 1, &quot;, &quot; compute = T), 
##    control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, 
##    size.small = 0, &quot;, &quot; size.big = 0), prec = list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, 
##    prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.69, Running = 0.175, Post = 0.254, Total = 2.11 
## Fixed effects:
##                       mean     sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)          2.156  0.665      0.851    2.156      3.462  2.155   0
## size.small           0.406  0.675     -0.919    0.405      1.730  0.405   0
## size.big             0.551  0.676     -0.775    0.551      1.877  0.551   0
## predpred            -1.754 18.258    -37.601   -1.754     34.064 -1.754   0
## size.small:predpred -0.341 18.260    -36.191   -0.342     35.479 -0.341   0
## size.big:predpred   -1.411 18.260    -37.261   -1.412     34.409 -1.411   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.37 1.07       1.01     2.15       5.07 1.80
## 
## Expected number of effective parameters(stdev): 27.12(3.80)
## Number of equivalent replicates : 1.77 
## 
## Deviance Information Criterion (DIC) ...............: 203.91
## Deviance Information Criterion (DIC, saturated) ....: 79.54
## Effective number of parameters .....................: 27.31
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.42
## Effective number of parameters .................: 19.54
## 
## Marginal log-Likelihood:  -126.05 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.5.i$summary.fixed</code></pre>
<pre><code>##                           mean         sd  0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)          2.1559548  0.6651605   0.8508812  2.1556250   3.461695  2.1550209 5.782096e-07
## size.small           0.4056285  0.6747850  -0.9188799  0.4054782   1.729780  0.4052337 3.563633e-07
## size.big             0.5511408  0.6756415  -0.7748341  0.5509153   1.877178  0.5505204 5.496338e-07
## predpred            -1.7535485 18.2583848 -37.6008935 -1.7540628  34.063846 -1.7535481 6.603266e-10
## size.small:predpred -0.3409941 18.2597045 -36.1909263 -0.3415085  35.478979 -0.3409938 9.191619e-10
## size.big:predpred   -1.4114467 18.2597582 -37.2614792 -1.4119606  34.408614 -1.4114449 1.461611e-09</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.5.i)</code></pre>
<pre><code>##                  mean        sd    q0.025     q0.5    q0.975      mode
## SD for tank 0.6920361 0.1392399 0.4444564 0.682502 0.9922881 0.6654361</code></pre>
</div>
</div>
<div id="compare-using-waic" class="section level2">
<h2>compare using WAIC</h2>
<div id="compare-rethinking" class="section level4">
<h4>compare rethinking</h4>
<pre class="r"><code>rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )</code></pre>
<pre><code>##          WAIC       SE     dWAIC      dSE    pWAIC    weight
## m1.2 199.0354 8.907729 0.0000000       NA 19.30471 0.3103586
## m1.5 199.2108 9.019910 0.1754538 3.122824 18.95142 0.2842919
## m1.1 200.1510 7.249734 1.1156096 5.358995 20.99414 0.1776693
## m1.3 201.0310 7.137760 1.9956108 5.650050 21.39720 0.1144254
## m1.4 201.0515 8.857861 2.0161760 1.894344 19.58388 0.1132548</code></pre>
<p>These models are really very similar in expected out-of-sample accuracy. The tank variation is huge. But take a look at the posterior distributions for predation and size. You’ll see that predation does seem to matter, as you’d expect. Size matters a lot less. So while predation doesn’t explain much of the total variation, there is plenty of evidence that it is a real effect. Remember: We don’t select a model using WAIC (or LOO). A predictor can make little difference in total accuracy but still be a real causal effect.</p>
</div>
<div id="compare-inla" class="section level4">
<h4>compare inla</h4>
<pre class="r"><code>inla.models.8.1 &lt;- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic &lt;- function (x){
  x[[&quot;waic&quot;]][[&quot;waic&quot;]]
}

waic.8.1 &lt;- bind_cols(model = c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1</code></pre>
<pre><code>## # A tibble: 5 x 2
##   model   waic
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 m1.1.i  206.
## 2 m1.2.i  201.
## 3 m1.3.i  206.
## 4 m1.4.i  203.
## 5 m1.5.i  202.</code></pre>
<p>Let’s look at all the sigma posterior distributions: The two models that omit predation, m1.1 and m1.3, have larger values of sigma. This is because predation explains some of the variation among tanks. So when you add it to the model, the variation in the tank intercepts gets smaller.</p>
<pre class="r"><code>sigma.8.1 &lt;- bind_cols( model= c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), do.call(rbind.data.frame, lapply(inla.models.8.1 ,bri.hyperpar.summary)))


sigma.8.1</code></pre>
<pre><code>##               model      mean        sd    q0.025      q0.5    q0.975      mode
## SD for tank  m1.1.i 1.5919697 0.2096781 1.2310838 1.5739639 2.0536663 1.5392289
## SD for tank1 m1.2.i 0.7815832 0.1385647 0.5384615 0.7711308 1.0828062 0.7517625
## SD for tank2 m1.3.i 1.5825033 0.2100774 1.2209479 1.5644595 2.0450713 1.5296348
## SD for tank3 m1.4.i 0.7260825 0.1402651 0.4772367 0.7163201 1.0288635 0.6986102
## SD for tank4 m1.5.i 0.6920361 0.1392399 0.4444564 0.6825020 0.9922881 0.6654361</code></pre>
<pre class="r"><code>sigma.8.1.plot &lt;-  ggplot(data= sigma.8.1, aes(y=model, x=mean, label=model)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=q0.025, xmax=q0.975), height=.3) +
    coord_fixed(ratio=.3) +
    theme_bw()

sigma.8.1.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/sigma.8.1%20plot-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="databangladesh-predicted-proportions-of-women-in-each-district-using-contraception-for-both-the-fixed-effects-model-and-the-varying-effects-model" class="section level1">
<h1>2. data(bangladesh) predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model</h1>
<p><strong>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</strong></p>
<p><strong>(1) district: ID number of administrative district each woman resided in</strong></p>
<p><strong>(2) use.contraception: An indicator (0/1) of whether the woman was using contraception</strong></p>
<p><strong>Focus on predicting use.contraception, clustered by district_id. Fit both:</strong></p>
<p><strong>1) a traditional fixed-effects model that uses an index variable for district</strong></p>
<p><strong>2) a multilevel model with varying intercepts for district.</strong></p>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?**</p>
<pre class="r"><code>library(rethinking)
data(bangladesh)
d &lt;- bangladesh</code></pre>
<p>The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:</p>
<pre class="r"><code>sort(unique(d$district))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41
## [42] 42 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61</code></pre>
<p>District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:</p>
<pre class="r"><code>d$district_id &lt;- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41
## [42] 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</code></pre>
<p>Now there are 60 values, contiguous integers 1 to 60.</p>
<div id="traditional-fixed-effects-model-that-uses-an-index-variable-for-district" class="section level2">
<h2>2.1 traditional fixed-effects model that uses an index variable for district</h2>
<div id="rethinking-5" class="section level3">
<h3>2.1 rethinking</h3>
<pre class="r"><code>dat_list &lt;- list(
C = d$use.contraception, 
did = d$district_id
)

m2.1 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( 0 , 1.5 )
) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.1, depth = 2)</code></pre>
<pre><code>##               mean        sd       5.5%        94.5%    n_eff     Rhat4
## a[1]  -1.056229942 0.2027278 -1.3832583 -0.741222575 5133.708 0.9987508
## a[2]  -0.590919437 0.4412908 -1.3122533  0.134677232 4622.976 0.9984932
## a[3]   1.274289371 1.1776995 -0.5414590  3.266973479 4113.013 0.9985978
## a[4]   0.000230995 0.3564291 -0.5681111  0.569957357 4713.970 0.9982526
## a[5]  -0.572441551 0.3374540 -1.1113457 -0.032156396 4367.315 0.9990522
## a[6]  -0.873372487 0.2701230 -1.2998983 -0.448702334 3961.939 0.9995744
## a[7]  -0.909135096 0.5246766 -1.7378614 -0.116729968 4560.326 0.9989747
## a[8]  -0.480002393 0.3416765 -1.0148055  0.043857656 4747.748 0.9984573
## a[9]  -0.797354242 0.4483071 -1.5287202 -0.084692703 4809.162 1.0002549
## a[10] -1.953232027 0.7615950 -3.2740717 -0.814609919 4232.550 0.9991249
## a[11] -2.954803959 0.7924615 -4.3250268 -1.785247370 3227.178 0.9991405
## a[12] -0.617002607 0.3879747 -1.2543402 -0.023237057 5141.104 0.9987196
## a[13] -0.319020185 0.3992510 -0.9553125  0.304331524 4721.547 0.9983131
## a[14]  0.515065327 0.1848315  0.2243304  0.814309314 5650.245 0.9989741
## a[15] -0.546400235 0.4360233 -1.2312574  0.146600798 3835.882 0.9986636
## a[16]  0.185246510 0.4355383 -0.5065881  0.864500827 4535.954 0.9983995
## a[17] -0.858861159 0.4307924 -1.5428147 -0.185726784 4938.325 0.9994316
## a[18] -0.652946493 0.3038587 -1.1584405 -0.172040216 5051.365 0.9992700
## a[19] -0.458483678 0.4019240 -1.1042597  0.186069415 4527.081 0.9991381
## a[20] -0.389416366 0.4908838 -1.1823676  0.382811647 4707.306 0.9985049
## a[21] -0.437555446 0.4771477 -1.2250885  0.299017427 3547.829 0.9985330
## a[22] -1.283428601 0.5266078 -2.1965877 -0.455220812 5039.020 0.9992248
## a[23] -0.937244950 0.5604961 -1.8598387 -0.068542457 3747.843 0.9985464
## a[24] -2.004759998 0.7241338 -3.2521240 -0.951633874 5023.689 0.9987952
## a[25] -0.211116684 0.2377444 -0.5893007  0.165966125 4013.521 0.9987057
## a[26] -0.448506638 0.5537694 -1.3197758  0.417389945 4952.379 0.9993527
## a[27] -1.462485575 0.3757735 -2.0914015 -0.884328637 3919.385 0.9991995
## a[28] -1.100293100 0.3227424 -1.6318333 -0.602282732 3408.731 0.9986430
## a[29] -0.897660829 0.3867512 -1.5214938 -0.309753502 3796.772 0.9987833
## a[30] -0.032180454 0.2632259 -0.4485699  0.376538648 4250.674 0.9991806
## a[31] -0.182835930 0.3529099 -0.7406087  0.383568768 4573.645 0.9992484
## a[32] -1.254256977 0.4470493 -1.9998182 -0.576311812 4856.914 0.9987307
## a[33] -0.250056124 0.5095169 -1.0417041  0.540091064 5764.656 0.9986314
## a[34]  0.635054129 0.3390166  0.1007496  1.176425069 4576.194 1.0001456
## a[35] -0.004603432 0.2803860 -0.4730281  0.439575302 4785.346 0.9989153
## a[36] -0.571713863 0.5103521 -1.4258304  0.223669194 5290.274 0.9989177
## a[37]  0.140423039 0.5198337 -0.6671509  0.962717686 5231.384 0.9985082
## a[38] -0.833566757 0.5433627 -1.7189206  0.023432459 3498.329 0.9984259
## a[39]  0.010350610 0.3951277 -0.6140374  0.652714471 4663.214 0.9982927
## a[40] -0.144340118 0.3076102 -0.6453733  0.347975926 4030.063 0.9992747
## a[41]  0.011739181 0.3742146 -0.5709254  0.607678500 3236.079 1.0005505
## a[42]  0.163653610 0.5786274 -0.7282459  1.084512452 5446.547 0.9990190
## a[43]  0.141587374 0.3068185 -0.3326344  0.615925666 3178.543 0.9996005
## a[44] -1.196309694 0.4259560 -1.8798485 -0.536303321 3950.000 0.9994578
## a[45] -0.680081649 0.3361036 -1.2248295 -0.119898856 4803.474 0.9985531
## a[46]  0.090526983 0.2284096 -0.2855210  0.448363121 4533.297 0.9989051
## a[47] -0.133261965 0.5207246 -0.9546786  0.669977345 4843.234 0.9988161
## a[48]  0.107252383 0.3142933 -0.3892389  0.593019467 4759.978 0.9982296
## a[49] -1.728671869 1.0399521 -3.4569641 -0.148013716 4085.651 1.0001161
## a[50] -0.100259231 0.4546515 -0.8360347  0.630119902 3587.950 0.9986237
## a[51] -0.157712050 0.3257508 -0.6704297  0.369233628 4054.424 0.9992268
## a[52] -0.229511621 0.2537480 -0.6385613  0.191069468 5177.808 0.9987239
## a[53] -0.307761539 0.4628413 -1.0216826  0.423997356 4194.625 0.9988359
## a[54] -1.220002506 0.8076369 -2.5832564 -0.009626957 4663.553 0.9989140
## a[55]  0.308490990 0.2949176 -0.1703565  0.772944171 6576.228 0.9990377
## a[56] -1.401313376 0.4683922 -2.1901083 -0.684005809 3472.445 1.0013845
## a[57] -0.177704928 0.3573147 -0.7409950  0.380045747 4272.819 0.9989003
## a[58] -1.703811911 0.7561455 -2.9763956 -0.548538752 3893.687 0.9985490
## a[59] -1.220363060 0.4158585 -1.9027640 -0.602170515 3793.555 0.9987835
## a[60] -1.255876639 0.3661347 -1.8591092 -0.677794302 4814.427 0.9983906</code></pre>
</div>
<div id="inla-5" class="section level3">
<h3>2.1 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d2.i &lt;- d %&gt;% 
  mutate(did= paste(&quot;d&quot;, as.integer(d$district_id), sep= &quot;.&quot;), 
         d.value= 1
         ) %&gt;% 
  spread(did, d.value)

#use this to quickly make a list of the index vbles to include in the model 
did_formula &lt;- paste(&quot;d&quot;, 1:60, sep=&quot;.&quot;, collapse = &quot;+&quot;)


m2.1.i &lt;- inla(use.contraception ~ d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9+d.10+d.11+d.12+d.13+d.14+d.15+d.16+d.17+d.18+d.19+d.20+d.21+d.22+d.23+d.24+d.25+d.26+d.27+d.28+d.29+d.30+d.31+d.32+d.33+d.34+d.35+d.36+d.37+d.38+d.39+d.40+d.41+d.42+d.43+d.44+d.45+d.46+d.47+d.48+d.49+d.50+d.51+d.52+d.53+d.54+d.55+d.56+d.57+d.58+d.59+d.60, data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean=  0 ,
        prec= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ d.1 + d.2 + d.3 + d.4 + d.5 + &quot;, &quot; d.6 + d.7 + d.8 + d.9 + d.10 + d.11 + 
##    d.12 + d.13 + d.14 + &quot;, &quot; d.15 + d.16 + d.17 + d.18 + d.19 + d.20 + d.21 + d.22 + d.23 + &quot;, &quot; d.24 + d.25 + d.26 + 
##    d.27 + d.28 + d.29 + d.30 + d.31 + d.32 + &quot;, &quot; d.33 + d.34 + d.35 + d.36 + d.37 + d.38 + d.39 + d.40 + d.41 + &quot;, &quot; 
##    d.42 + d.43 + d.44 + d.45 + d.46 + d.47 + d.48 + d.49 + d.50 + &quot;, &quot; d.51 + d.52 + d.53 + d.54 + d.55 + d.56 + d.57 
##    + d.58 + d.59 + &quot;, &quot; d.60, family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, control.compute = list(config = T, &quot;, 
##    &quot; waic = TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = 
##    list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 4.91, Running = 0.506, Post = 0.805, Total = 6.22 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.634 0.204     -1.035   -0.634     -0.234 -0.634   0
## d.1         -0.427 0.290     -1.001   -0.425      0.136 -0.421   0
## d.2          0.009 0.484     -0.967    0.017      0.936  0.034   0
## d.3          1.421 1.074     -0.583    1.382      3.640  1.306   0
## d.4          0.599 0.404     -0.194    0.600      1.391  0.600   0
## d.5          0.048 0.380     -0.709    0.052      0.782  0.060   0
## d.6         -0.247 0.333     -0.911   -0.244      0.397 -0.237   0
## d.7         -0.294 0.525     -1.367   -0.279      0.695 -0.250   0
## d.8          0.128 0.384     -0.636    0.131      0.871  0.138   0
## d.9         -0.183 0.471     -1.136   -0.173      0.713 -0.153   0
## d.10        -1.349 0.750     -2.950   -1.302     -0.002 -1.208   0
## d.11        -2.343 0.845     -4.166   -2.281     -0.851 -2.155   0
## d.12        -0.012 0.424     -0.861   -0.006      0.803  0.006   0
## d.13         0.274 0.443     -0.606    0.278      1.132  0.286   0
## d.14         1.138 0.276      0.599    1.137      1.681  1.136   0
## d.15         0.064 0.465     -0.869    0.071      0.957  0.085   0
## d.16         0.767 0.469     -0.148    0.766      1.692  0.762   0
## d.17        -0.239 0.467     -1.187   -0.229      0.649 -0.208   0
## d.18        -0.030 0.360     -0.747   -0.027      0.666 -0.019   0
## d.19         0.150 0.434     -0.716    0.155      0.987  0.165   0
## d.20         0.200 0.530     -0.863    0.208      1.220  0.223   0
## d.21         0.161 0.497     -0.834    0.168      1.117  0.182   0
## d.22        -0.673 0.542     -1.796   -0.652      0.332 -0.611   0
## d.23        -0.337 0.566     -1.501   -0.318      0.724 -0.282   0
## d.24        -1.413 0.743     -3.001   -1.367     -0.079 -1.272   0
## d.25         0.413 0.314     -0.205    0.413      1.027  0.415   0
## d.26         0.139 0.563     -0.994    0.149      1.216  0.169   0
## d.27        -0.826 0.418     -1.679   -0.814     -0.037 -0.791   0
## d.28        -0.476 0.377     -1.235   -0.470      0.244 -0.456   0
## d.29        -0.291 0.424     -1.148   -0.283      0.517 -0.266   0
## d.30         0.585 0.321     -0.046    0.585      1.214  0.585   0
## d.31         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.32        -0.641 0.502     -1.676   -0.624      0.296 -0.590   0
## d.33         0.304 0.541     -0.775    0.310      1.348  0.322   0
## d.34         1.222 0.394      0.461    1.217      2.008  1.208   0
## d.35         0.612 0.345     -0.066    0.612      1.289  0.612   0
## d.36         0.020 0.514     -1.018    0.030      1.002  0.049   0
## d.37         0.693 0.551     -0.384    0.692      1.777  0.690   0
## d.38        -0.252 0.574     -1.428   -0.235      0.825 -0.200   0
## d.39         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.40         0.467 0.364     -0.251    0.468      1.178  0.470   0
## d.41         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.42         0.703 0.587     -0.445    0.701      1.859  0.698   0
## d.43         0.739 0.353      0.048    0.739      1.433  0.737   0
## d.44        -0.575 0.474     -1.546   -0.561      0.316 -0.532   0
## d.45        -0.061 0.384     -0.828   -0.056      0.680 -0.047   0
## d.46         0.713 0.293      0.139    0.713      1.288  0.712   0
## d.47         0.447 0.523     -0.590    0.450      1.464  0.456   0
## d.48         0.700 0.360     -0.006    0.700      1.408  0.699   0
## d.49        -1.230 1.047     -3.465   -1.166      0.651 -1.035   0
## d.50         0.483 0.478     -0.461    0.485      1.415  0.489   0
## d.51         0.449 0.377     -0.294    0.451      1.185  0.453   0
## d.52         0.391 0.323     -0.244    0.392      1.022  0.394   0
## d.53         0.286 0.482     -0.675    0.290      1.219  0.300   0
## d.54        -0.669 0.837     -2.437   -0.625      0.856 -0.537   0
## d.55         0.913 0.355      0.221    0.912      1.613  0.909   0
## d.56        -0.777 0.495     -1.799   -0.759      0.144 -0.724   0
## d.57         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.58        -1.120 0.776     -2.772   -1.074      0.278 -0.980   0
## d.59        -0.601 0.448     -1.514   -0.588      0.243 -0.563   0
## d.60        -0.635 0.409     -1.465   -0.625      0.140 -0.606   0
## 
## Expected number of effective parameters(stdev): 54.05(0.00)
## Number of equivalent replicates : 35.78 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2521.28
## Effective number of parameters .................: 52.54
## 
## Marginal log-Likelihood:  -1291.66 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="varying-intercepts-model-1" class="section level2">
<h2>2.2 varying intercepts model</h2>
<div id="rethinking-6" class="section level3">
<h3>2.2 rethinking</h3>
<pre class="r"><code>m2.2 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( a_bar , sigma ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
) ,data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.2, depth= 2)</code></pre>
<pre><code>##               mean         sd       5.5%        94.5%     n_eff     Rhat4
## a[1]  -0.997918746 0.20070119 -1.3233601 -0.686884287 3659.7191 0.9995059
## a[2]  -0.593493159 0.35883988 -1.1788903 -0.045022439 3618.8958 0.9989922
## a[3]  -0.213213354 0.51219313 -1.0148661  0.637179181 3756.8195 0.9990536
## a[4]  -0.181596989 0.29616882 -0.6521156  0.307733306 3309.2674 1.0004153
## a[5]  -0.570246434 0.29897331 -1.0446054 -0.108193993 3676.2130 0.9995453
## a[6]  -0.817090010 0.23888655 -1.2003115 -0.446521413 4136.7882 0.9993961
## a[7]  -0.755513370 0.36171101 -1.3513425 -0.201191855 3456.0341 0.9986461
## a[8]  -0.506001006 0.28963150 -0.9764131 -0.053656371 3920.2017 0.9997318
## a[9]  -0.711361359 0.33926624 -1.2542612 -0.178106607 4526.5345 1.0005058
## a[10] -1.151252796 0.43333033 -1.8820550 -0.494611226 2934.1400 0.9992576
## a[11] -1.568657427 0.45238230 -2.3237785 -0.877955800 1973.1567 1.0011623
## a[12] -0.610797073 0.30769647 -1.1024441 -0.125060458 4219.2993 0.9992056
## a[13] -0.423228142 0.32689907 -0.9681255  0.085558232 2999.8633 1.0000659
## a[14]  0.394205109 0.17883273  0.1179628  0.682306815 3023.2710 0.9987583
## a[15] -0.555711227 0.33315469 -1.0862433 -0.034510929 4226.8069 0.9984177
## a[16] -0.117615201 0.33803806 -0.6540658  0.417525686 2976.4770 0.9990324
## a[17] -0.747786403 0.33474377 -1.2910809 -0.217117388 3896.1410 0.9991467
## a[18] -0.644760109 0.27056960 -1.0776114 -0.226463703 4020.0370 0.9991624
## a[19] -0.506524148 0.32677962 -1.0317241 -0.003954864 3622.5632 0.9985593
## a[20] -0.487126120 0.35948241 -1.0718291  0.069261176 2920.1792 0.9984178
## a[21] -0.508174386 0.36537962 -1.0960221  0.055024442 4226.8651 0.9991905
## a[22] -0.963298522 0.37557736 -1.5750710 -0.381408272 3283.0425 0.9993771
## a[23] -0.761045794 0.38087895 -1.3824362 -0.166453426 2748.6823 0.9989318
## a[24] -1.191456657 0.44607224 -1.9574858 -0.526313293 2659.9718 0.9993301
## a[25] -0.269046948 0.21925588 -0.6223118  0.078076212 3884.6826 0.9983372
## a[26] -0.510940445 0.38550482 -1.1284808  0.091330806 4420.7195 0.9994603
## a[27] -1.187536338 0.31857010 -1.7177445 -0.697673371 3496.6682 0.9992525
## a[28] -0.977364255 0.27415193 -1.4150189 -0.540904499 3026.5503 1.0001625
## a[29] -0.807118629 0.31606741 -1.3372234 -0.305037284 4030.6770 0.9988724
## a[30] -0.137162805 0.24420470 -0.5293002  0.261218297 5660.3869 0.9984805
## a[31] -0.303869593 0.29779862 -0.7697238  0.159896604 5204.5567 0.9990337
## a[32] -0.975545130 0.34095240 -1.5267627 -0.459762449 2985.7722 0.9982869
## a[33] -0.412888858 0.37245834 -1.0074666  0.179996610 4485.0451 0.9984092
## a[34]  0.274163009 0.29341347 -0.1804029  0.750880002 3105.9551 0.9995564
## a[35] -0.140399374 0.24227383 -0.5130488  0.257210723 3236.9794 0.9995913
## a[36] -0.584824828 0.35759558 -1.1608350 -0.028140029 3262.7783 0.9984551
## a[37] -0.221854456 0.38651647 -0.8359836  0.390188491 3560.6777 0.9988424
## a[38] -0.721678208 0.40159672 -1.3865723 -0.109854667 3076.0425 0.9994931
## a[39] -0.209992004 0.29753994 -0.6824709  0.248942767 3299.1125 0.9988494
## a[40] -0.261856948 0.28090046 -0.6926568  0.169841737 3727.6154 0.9986490
## a[41] -0.202592636 0.32228466 -0.7133665  0.320598549 5070.9069 0.9991483
## a[42] -0.253483043 0.39865981 -0.8833923  0.399149847 3992.4629 0.9987588
## a[43] -0.033498976 0.25962407 -0.4412084  0.386957369 4117.5569 0.9988145
## a[44] -0.962724844 0.34514045 -1.5359653 -0.442207226 3443.3386 0.9993278
## a[45] -0.664751983 0.28928476 -1.1389667 -0.225422114 4836.6758 0.9984187
## a[46] -0.004399898 0.19779555 -0.3264712  0.323350569 3796.9409 0.9992232
## a[47] -0.347960272 0.35783394 -0.8997495  0.225886074 3436.8570 0.9985128
## a[48] -0.076904223 0.26363914 -0.5038752  0.354987898 3501.1839 0.9986739
## a[49] -0.872457850 0.48151404 -1.6906829 -0.129806786 2265.0583 0.9992160
## a[50] -0.281797007 0.36879554 -0.8403127  0.334285421 3697.6106 0.9997724
## a[51] -0.278269963 0.27552569 -0.7322514  0.164823082 3388.8534 0.9986412
## a[52] -0.295586293 0.23510042 -0.6704939  0.072218191 3548.5163 0.9986303
## a[53] -0.425652341 0.34906740 -0.9833649  0.133196761 3918.2125 0.9984446
## a[54] -0.796935687 0.44859535 -1.5377135 -0.110248784 3162.2654 1.0001959
## a[55]  0.094746581 0.26701684 -0.3201290  0.528632554 3026.6076 1.0000506
## a[56] -1.067998520 0.35396765 -1.6577139 -0.517225341 3174.7480 0.9993004
## a[57] -0.305821303 0.29313861 -0.7862973  0.159924627 3980.4047 0.9988730
## a[58] -1.014683019 0.44164676 -1.7458110 -0.330911523 2880.4681 0.9993410
## a[59] -1.003861608 0.33049646 -1.5418206 -0.480256366 3203.3854 0.9997313
## a[60] -1.063448476 0.29769137 -1.5813573 -0.606018429 3141.0958 0.9992162
## a_bar -0.541435526 0.08631125 -0.6815078 -0.410088088 1853.5731 0.9999968
## sigma  0.522143876 0.08342206  0.4011024  0.662021736  830.5539 1.0005203</code></pre>
</div>
<div id="inla-6" class="section level3">
<h3>2.2 inla</h3>
<pre class="r"><code>m2.2.i &lt;- inla(use.contraception ~ f(district_id, model=&quot;iid&quot;), data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean.intercept=  0 ,
        prec.intercept= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ f(district_id, model = \&quot;iid\&quot;), &quot;, &quot; family = \&quot;binomial\&quot;, data = d2.i, 
##    Ntrials = 1, control.compute = list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = 
##    T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean.intercept = 
##    0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.33, Running = 1.08, Post = 0.258, Total = 2.67 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.532 0.084     -0.701   -0.531     -0.371 -0.528   0
## 
## Random effects:
##   Name     Model
##     district_id IID model
## 
## Model hyperparameters:
##                           mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for district_id 4.72 1.64       2.38     4.43       8.72 3.95
## 
## Expected number of effective parameters(stdev): 34.00(4.22)
## Number of equivalent replicates : 56.89 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2514.73
## Effective number of parameters .................: 33.32
## 
## Marginal log-Likelihood:  -1278.85 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                         mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.4799466 0.07896703 0.3386615 0.4749306 0.6490223 0.4658743</code></pre>
<p>Side note: this is how you calculate the sd from the hyperprior (<span class="math inline">\(\sigma\)</span>)</p>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                         mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.4799466 0.07896703 0.3386615 0.4749306 0.6490223 0.4658743</code></pre>
<pre class="r"><code># hyperparameter of the precision
m2.2.i.prec &lt;- m2.2.i$internal.marginals.hyperpar

#transform precision to sd using inla.tmarginal
#m2.2.i.prec[[1]] is used to access the actual values inside the list
m2.2.i.sd &lt;- inla.tmarginal(function(x) sqrt(exp(-x)), m2.2.i.prec[[1]])
#plot the post of the sd per district (sigma)
plot(m2.2.i.sd)</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.2%20inla%20hyper%20sd-1.png" width="672" /></p>
<pre class="r"><code>#summary stats for the sd 
m2.2.i.sd.sum &lt;- inla.zmarginal(m2.2.i.sd)</code></pre>
<pre><code>## Mean            0.479947 
## Stdev           0.078967 
## Quantile  0.025 0.338662 
## Quantile  0.25  0.424476 
## Quantile  0.5   0.474931 
## Quantile  0.75  0.529814 
## Quantile  0.975 0.649022</code></pre>
<pre class="r"><code># this coincides perfectly with the result from bri.hyperpar.summary</code></pre>
</div>
</div>
<div id="plot-of-posterior-mean-probabilities-in-each-district" class="section level2">
<h2>2.3 plot of posterior mean probabilities in each district</h2>
<p>Now let’s extract the samples, compute posterior mean probabilities in each district, and plot it all:</p>
<div id="plot-rethinking" class="section level3">
<h3>plot rethinking</h3>
<pre class="r"><code>post1 &lt;- extract.samples( m2.1 ) 
post2 &lt;- extract.samples( m2.2 )
p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) 
p2 &lt;- apply( inv_logit(post2$a) , 2 , mean )
nd &lt;- max(dat_list$did)
plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; )
points( 1:nd , p1 , pch=16 , col=rangi2 ) 
points( 1:nd , p2 )
abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20rethinking-1.png" width="672" /></p>
</div>
<div id="plot-inla" class="section level3">
<h3>plot inla</h3>
<p><a href="https://people.bath.ac.uk/jjf23/inla/oneway.html" class="uri">https://people.bath.ac.uk/jjf23/inla/oneway.html</a></p>
<p><a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>posterior mean for each district a for the idex fixed effect model m2.1:</strong></p>
<pre class="r"><code># m2.2.i$summary.fixed[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
# we eliminate the first element of this list, the intercept.
m2.1.i.fix&lt;- lapply(m2.1.i$marginals.fixed, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))[-1]</code></pre>
<p><strong>posterior mean for each district a for the varying intercept model m2.2:</strong></p>
<pre class="r"><code># m2.2.i$summary.random[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
m2.2.i.rand&lt;- lapply(m2.2.i$marginals.random$district_id, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))</code></pre>
<pre class="r"><code># sapply(m2.2.i.rand, function(x) x[1]) extracts the first element (the mean) from the summary of the posterior of each district
m2.i.mean &lt;- bind_cols(district= 1:60,mean.m2.1= unlist(sapply(m2.1.i.fix, function(x) x[1])), mean.m2.2=unlist(sapply(m2.2.i.rand, function(x) x[1])))

m2.2.i.abar &lt;- inla.zmarginal( inla.tmarginal (inverse_logit, m2.2.i$marginals.fixed[[&quot;(Intercept)&quot;]] ))</code></pre>
<pre><code>## Mean            0.370148 
## Stdev           0.0193723 
## Quantile  0.025 0.331662 
## Quantile  0.25  0.357163 
## Quantile  0.5   0.370219 
## Quantile  0.75  0.383175 
## Quantile  0.975 0.407995</code></pre>
<pre class="r"><code>m2.i.district.plot &lt;- ggplot() +
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.1), color= &quot;blue&quot;, alpha= 0.5)+
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.2), color= &quot;black&quot;, alpha= 0.5, shape= 1)+
  geom_hline(yintercept=m2.2.i.abar[[1]], linetype=&#39;longdash&#39;) +
  ylim(0,1)+
  labs(y = &quot;prob use contraception&quot;)+
  theme_bw()
  

m2.i.district.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20inla-1.png" width="672" /></p>
<p>The blue points are the fixed estimations. The open points are the varying effects. As you’d expect, they are shrunk towards the mean (the dashed line). Some are shrunk more than others. The third district from the left shrunk a lot.</p>
<p>Let’s look at the sample size in each district:</p>
<pre class="r"><code> table(d$district_id)</code></pre>
<pre><code>## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 
## 117  20   2  30  39  65  18  37  23  13  21  29  24 118  22  20  24  47  26  15  18  20  15  14  67  13  44  49  32  61  33  24 
##  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  14  35  48  17  13  14  26  41  26  11  45  27  39  86  15  42   4  19  37  61  19   6  45  27  33  10  32  42</code></pre>
<p>District 3 has only 2 women sampled. So it shrinks a lot. There are couple of other districts, like 49 and 54, that also have very few women sampled. But their fixed estimates aren’t as extreme, so they don’t shrink as much as district 3 does. All of this is explained by partial pooling, of course.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
