<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 8 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 8 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="section" class="section level1">
<h1>1.</h1>
<p><strong>Revisit the Reed frog survival data, data(reedfrogs),and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.</strong></p>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs</code></pre>
<div id="varying-intercepts-model" class="section level2">
<h2>1.1 varying intercepts model</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i]</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<pre class="r"><code>dat &lt;- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), 
size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L )
)</code></pre>
<pre class="r"><code>m1.1 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1, depth= 2)</code></pre>
<pre><code>##              mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]   2.15428308 0.8912847  0.86322150  3.668441694 3461.255 0.9983576
## a[2]   3.05655903 1.1128363  1.50396626  4.954485115 3047.561 0.9992739
## a[3]   0.99107510 0.6735035 -0.02474948  2.120675933 5972.429 0.9988629
## a[4]   3.05461826 1.0994497  1.46434845  4.865752434 3215.238 0.9993096
## a[5]   2.15015547 0.8745421  0.83620037  3.641660341 2974.718 0.9996261
## a[6]   2.13383903 0.8804661  0.85444560  3.607463740 4307.126 0.9993048
## a[7]   3.05965148 1.0616579  1.57812606  4.850792394 2759.322 0.9993005
## a[8]   2.13315884 0.8451090  0.93390984  3.546327131 3230.979 0.9993195
## a[9]  -0.17515624 0.6238448 -1.17423071  0.800989018 3659.714 0.9988829
## a[10]  2.14075116 0.8784266  0.85600387  3.629149849 3973.153 0.9993491
## a[11]  1.00019144 0.6829086 -0.05158321  2.101773250 4525.237 0.9985810
## a[12]  0.57709133 0.6441155 -0.42821825  1.633441485 5203.354 0.9988605
## a[13]  0.98619578 0.6255823  0.03061681  2.004859323 3272.173 0.9996434
## a[14]  0.19364163 0.6150979 -0.76324119  1.146426441 5941.633 0.9993453
## a[15]  2.12730092 0.8474332  0.91244099  3.552070349 2829.699 0.9993742
## a[16]  2.14607755 0.8932158  0.83140146  3.723775000 3493.903 0.9985590
## a[17]  2.89020748 0.7980523  1.73329659  4.249993712 2891.856 0.9987994
## a[18]  2.41961535 0.7085371  1.37073930  3.601580223 2985.672 0.9993848
## a[19]  2.01871247 0.5743398  1.14190401  2.989032739 3421.287 0.9993024
## a[20]  3.63818978 0.9459387  2.26683193  5.244103816 2958.627 1.0000556
## a[21]  2.40286728 0.6845099  1.38943755  3.579502613 3263.455 0.9991647
## a[22]  2.38704425 0.6420474  1.44902033  3.473829142 3445.162 0.9987158
## a[23]  2.40295178 0.6663500  1.42442952  3.555030908 3023.642 0.9987774
## a[24]  1.70356317 0.5483335  0.86890318  2.618401344 4790.159 0.9987281
## a[25] -1.00251227 0.4462403 -1.72124247 -0.318782072 3686.414 0.9989411
## a[26]  0.16431999 0.3967367 -0.47251819  0.801009728 5020.923 0.9983493
## a[27] -1.42378632 0.4876080 -2.25254850 -0.691506506 3115.217 0.9985725
## a[28] -0.46323004 0.4105835 -1.12642703  0.183677472 5759.562 0.9986174
## a[29]  0.16198651 0.3842462 -0.45862558  0.766977326 5276.944 0.9982048
## a[30]  1.42617013 0.4709038  0.69932303  2.223903575 4945.468 0.9990276
## a[31] -0.64125598 0.4058475 -1.28827158  0.014901113 4183.592 0.9988688
## a[32] -0.30774938 0.3994338 -0.95702000  0.327434967 5315.536 0.9984811
## a[33]  3.17535132 0.7541571  2.05358502  4.444160789 3207.037 0.9996354
## a[34]  2.70194174 0.6676784  1.75835686  3.846073730 3244.804 1.0015299
## a[35]  2.70057090 0.6171761  1.84048554  3.761598975 3089.262 0.9998303
## a[36]  2.06027517 0.5290679  1.28836133  2.913890653 5117.252 0.9994629
## a[37]  2.05407072 0.5017061  1.30601062  2.879913686 3809.511 0.9990078
## a[38]  3.87671371 0.9461057  2.55696583  5.490770253 3225.327 0.9993852
## a[39]  2.70468531 0.6383311  1.80142638  3.776789397 3090.727 0.9987787
## a[40]  2.34782500 0.5876169  1.49062361  3.299194636 3453.694 0.9984275
## a[41] -1.81180378 0.4735571 -2.61350162 -1.105096151 4001.622 0.9987044
## a[42] -0.57032642 0.3546803 -1.15815348 -0.006914465 4214.460 0.9990684
## a[43] -0.45020449 0.3352835 -0.96962358  0.071945369 5330.866 0.9982384
## a[44] -0.33227399 0.3446688 -0.88519691  0.205979951 4767.721 0.9987787
## a[45]  0.58235808 0.3640928  0.02035984  1.180935841 5082.026 0.9990753
## a[46] -0.57134147 0.3390275 -1.12379547 -0.027647416 4573.472 0.9989132
## a[47]  2.05554181 0.4984718  1.27075733  2.901245509 3791.272 1.0006031
## a[48]  0.00412481 0.3243621 -0.51726603  0.519349440 4824.078 0.9992825
## a_bar  1.33408733 0.2519962  0.92964751  1.738269552 2886.880 0.9993711
## sigma  1.61019214 0.2054208  1.29932416  1.963054874 1374.998 1.0001616</code></pre>
</div>
<div id="inla" class="section level3">
<h3>1.1 INLA</h3>
<p>following example: <a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>Here I’m missing custom priors</strong> I’ll use a half cauchy prior for the <span class="math inline">\(\sigma\)</span> to constrain it to &gt;0 numbers, which is what the exponential does as well.</p>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i &lt;- inla(surv ~ 1 + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, 
##    Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 
##    1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)))&quot; ) 
## Time used:
##     Pre = 1.43, Running = 0.247, Post = 0.206, Total = 1.89 
## Fixed effects:
##             mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 1.38 0.256       0.89    1.375      1.901 1.364   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.415 0.109      0.237    0.404      0.661 0.381
## 
## Expected number of effective parameters(stdev): 40.36(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.00
## Deviance Information Criterion (DIC, saturated) ....: 89.62
## Effective number of parameters .....................: 39.50
## 
## Watanabe-Akaike information criterion (WAIC) ...: 205.61
## Effective number of parameters .................: 22.72
## 
## Marginal log-Likelihood:  -140.19 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.1.i$summary.fixed</code></pre>
<pre><code>##                 mean        sd 0.025quant 0.5quant 0.975quant     mode          kld
## (Intercept) 1.380249 0.2562768  0.8904279 1.374944    1.90069 1.364469 1.902443e-05</code></pre>
<pre class="r"><code>m1.1.i$summary.hyperpar</code></pre>
<pre><code>##                         mean        sd 0.025quant  0.5quant 0.975quant      mode
## Precision for tank 0.4152469 0.1088217  0.2366839 0.4035007  0.6608823 0.3807222</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.1.i)</code></pre>
<pre><code>##                mean        sd   q0.025     q0.5   q0.975     mode
## SD for tank 1.59197 0.2096781 1.231084 1.573964 2.053666 1.539229</code></pre>
<p>it looks like the intercept mean and sd correspond to the <span class="math inline">\(\bar{\alpha}\)</span> mean and sd, and the SD for tank corresponds to the <span class="math inline">\(\sigma\)</span>. this makes sense, because the <span class="math inline">\(\bar{\alpha}\)</span> is the average baseline survival for all the tadpoles, which is what the intercept is. <strong>BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION</strong>.</p>
</div>
</div>
<div id="varying-intercepts-predation" class="section level2">
<h2>1.2 varying intercepts + predation</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>[pred]</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(-0.5,1)</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-1" class="section level3">
<h3>1.2 rethinking</h3>
<pre class="r"><code># pred
m1.2 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis(m1.2)</code></pre>
<pre><code>##             mean        sd       5.5%     94.5%    n_eff    Rhat4
## bp    -2.4093816 0.3092314 -2.9127700 -1.921901 328.7049 1.013251
## a_bar  2.5200226 0.2400479  2.1512958  2.926463 348.3109 1.010694
## sigma  0.8265152 0.1485577  0.6134606  1.076466 562.9340 1.000236</code></pre>
</div>
<div id="inla-1" class="section level3">
<h3>1.2 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i &lt;- inla(surv ~ 1 + pred + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = 
##    d1.i, Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = 
##    list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = 
##    list(mean = -0.5, prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1.5))&quot;) 
## Time used:
##     Pre = 1.95, Running = 0.225, Post = 0.227, Total = 2.4 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.393 0.217      1.967    2.393      2.822  2.391   0
## predpred    -2.310 0.285     -2.859   -2.314     -1.736 -2.321   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 1.80 0.681      0.843     1.68       3.46 1.47
## 
## Expected number of effective parameters(stdev): 29.26(3.39)
## Number of equivalent replicates : 1.64 
## 
## Deviance Information Criterion (DIC) ...............: 205.43
## Deviance Information Criterion (DIC, saturated) ....: 78.17
## Effective number of parameters .....................: 29.34
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.53
## Effective number of parameters .................: 19.86
## 
## Marginal log-Likelihood:  -124.71 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.2.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant  0.5quant 0.975quant      mode          kld
## (Intercept)  2.393389 0.2173783   1.966560  2.392846   2.822296  2.391477 1.097535e-07
## predpred    -2.309696 0.2849520  -2.858789 -2.313815  -1.736031 -2.321433 2.538474e-06</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.2.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7826381 0.1398744 0.5378891 0.7718996 1.087421 0.7524708</code></pre>
</div>
</div>
<div id="varying-intercepts-size" class="section level2">
<h2>1.3 varying intercepts + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(0 , 0.5 ) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-2" class="section level3">
<h3>1.3 rethinking</h3>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs

# size
m1.3 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.3,  depth=2)</code></pre>
<pre><code>##              mean        sd       5.5%         94.5%     n_eff    Rhat4
## a[1]   2.11347438 0.9247389  0.7133500  3.7171720587  603.8163 1.006181
## a[2]   3.01844004 1.1620653  1.3600596  4.9947046474 1026.4367 1.002656
## a[3]   1.01431385 0.7736142 -0.2123964  2.2627322776  530.8039 1.004078
## a[4]   3.03108219 1.1581502  1.3486222  5.0008007553  806.3866 1.002093
## a[5]   1.92210360 0.9423181  0.5157592  3.4931773849  849.4834 1.001220
## a[6]   1.88370809 0.9643460  0.4384943  3.4804408563  607.4394 1.004466
## a[7]   2.90970961 1.2374882  1.0973471  5.0718639977  972.7301 1.003031
## a[8]   1.89933449 0.9698059  0.4725670  3.5310836523  659.7527 1.002834
## a[9]  -0.12452456 0.7136795 -1.2945394  0.9775212432  439.4870 1.006757
## a[10]  2.14528511 0.9600583  0.7016629  3.7572976810  699.2003 1.001712
## a[11]  1.00649359 0.7763457 -0.1962238  2.2363180314  482.4405 1.007458
## a[12]  0.58061682 0.7402018 -0.5841744  1.7548666403  467.9219 1.008803
## a[13]  0.76783480 0.7717796 -0.4017126  2.0508628497  407.5541 1.006848
## a[14] -0.03785144 0.7534107 -1.2412909  1.1786063695  345.4762 1.008688
## a[15]  1.91130299 1.0013374  0.4002292  3.5984901333  687.7731 1.004066
## a[16]  1.89432099 0.9329945  0.4684744  3.4703416597  620.6095 1.009157
## a[17]  2.86440650 0.8616480  1.5710954  4.2503585173  782.0022 1.005306
## a[18]  2.40190144 0.7901323  1.2487558  3.7105250172  480.0172 1.006010
## a[19]  2.01537877 0.6861942  0.9795817  3.1615029144  445.4344 1.006626
## a[20]  3.66613615 1.0839197  2.1441149  5.4638579522  759.9131 1.005774
## a[21]  2.16157989 0.7430869  1.0126652  3.4227518104  481.5436 1.007773
## a[22]  2.16167688 0.7825883  1.0244514  3.4592533973  398.3567 1.006481
## a[23]  2.13416352 0.7633702  0.9684067  3.3871002862  476.6139 1.006050
## a[24]  1.45259139 0.6645029  0.4261401  2.5302802108  346.5997 1.006525
## a[25] -0.98005433 0.6107086 -1.9839616  0.0045896919  323.1909 1.007211
## a[26]  0.19710136 0.5685627 -0.6780552  1.1125486130  331.5179 1.008574
## a[27] -1.39340999 0.6281315 -2.3893990 -0.3664353785  370.1514 1.008889
## a[28] -0.43481500 0.5898181 -1.3700885  0.4931076429  326.4396 1.008935
## a[29] -0.08676978 0.5909532 -1.0208585  0.8429054941  308.0682 1.013641
## a[30]  1.18864297 0.6422605  0.2010861  2.2450536854  334.4129 1.009413
## a[31] -0.89750749 0.5764162 -1.8194947  0.0000721023  260.8200 1.008756
## a[32] -0.55876003 0.5556451 -1.4570028  0.3007406089  314.2912 1.010823
## a[33]  3.17289950 0.8525066  1.8870633  4.6461088268  654.4450 1.008566
## a[34]  2.68756762 0.7502601  1.5579661  3.9142389802  550.2541 1.006339
## a[35]  2.71935304 0.7543426  1.5508778  3.9718705375  542.4806 1.005864
## a[36]  2.08023115 0.6545220  1.0562874  3.1572574193  360.7384 1.009561
## a[37]  1.81928093 0.6682488  0.7950139  2.9161144466  324.5965 1.011432
## a[38]  3.63493249 1.0383270  2.0588866  5.3477592720  602.8961 1.003862
## a[39]  2.45470422 0.7400377  1.3591536  3.6457559811  434.1870 1.007163
## a[40]  2.09826781 0.7063496  0.9993852  3.2267743229  406.4773 1.012451
## a[41] -1.77511972 0.6259647 -2.7853703 -0.8123799174  435.5914 1.006158
## a[42] -0.54755104 0.5383917 -1.4062118  0.3175668033  267.2158 1.013566
## a[43] -0.42039217 0.5252587 -1.2316464  0.4273863286  265.0510 1.008646
## a[44] -0.30822902 0.5255845 -1.1584145  0.5288873196  275.6777 1.010073
## a[45]  0.32546994 0.5399704 -0.5575943  1.1876817801  277.7419 1.014544
## a[46] -0.82323748 0.5347589 -1.6807348  0.0265577211  254.1305 1.011743
## a[47]  1.81167931 0.6474581  0.8253674  2.8976325200  389.5375 1.004721
## a[48] -0.25649297 0.5229840 -1.0693125  0.5907428174  239.6558 1.018790
## s[1]   0.26399406 0.4143758 -0.4109362  0.9211181520  168.8626 1.022358
## s[2]  -0.03749999 0.4107795 -0.7129886  0.6131065577  177.2545 1.020249
## a_bar  1.22962461 0.4307406  0.5448272  1.9331196203  181.2869 1.023268
## sigma  1.60452376 0.2194889  1.2807781  1.9694563379 1101.7847 1.000951</code></pre>
</div>
<div id="inla-2" class="section level3">
<h3>1.3 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i &lt;- inla(surv ~ 1 + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 0.5, 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + f(tank, model = \&quot;iid\&quot;, &quot;, &quot; hyper = hcprior), family = 
##    \&quot;binomial\&quot;, data = d1.i, Ntrials = density, &quot;, &quot; control.compute = list(config = T, dic = TRUE, waic = TRUE), &quot;, &quot; 
##    control.predictor = list(link = 1, compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; 
##    control.fixed = list(mean = 0, prec = 0.5, mean.intercept = 0, &quot;, &quot; prec.intercept = 1.5))&quot;) 
## Time used:
##     Pre = 1.64, Running = 0.203, Post = 0.221, Total = 2.06 
## Fixed effects:
##              mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 0.531 0.640     -0.726    0.531      1.787 0.531   0
## size.small  1.011 0.694     -0.350    1.011      2.373 1.010   0
## size.big    0.582 0.694     -0.779    0.582      1.945 0.581   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.42 0.111      0.238    0.408      0.671 0.384
## 
## Expected number of effective parameters(stdev): 40.48(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.60
## Deviance Information Criterion (DIC, saturated) ....: 90.19
## Effective number of parameters .....................: 39.64
## 
## Watanabe-Akaike information criterion (WAIC) ...: 206.43
## Effective number of parameters .................: 22.98
## 
## Marginal log-Likelihood:  -142.95 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.3.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant 0.5quant 0.975quant      mode          kld
## (Intercept) 0.5313388 0.6402100 -0.7255154 0.531285   1.787366 0.5312310 1.117130e-07
## size.small  1.0110090 0.6937269 -0.3504100 1.010716   2.372870 1.0101886 4.481129e-07
## size.big    0.5822069 0.6939421 -0.7788480 0.581643   1.945235 0.5805791 8.751102e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.3.i)</code></pre>
<pre><code>##                 mean        sd   q0.025     q0.5 q0.975     mode
## SD for tank 1.584281 0.2109564 1.221524 1.566055 2.0491 1.530878</code></pre>
<p>** these estimates are super off, probably something is wrong**</p>
</div>
</div>
<div id="varying-intercepts-predation-size" class="section level2">
<h2>1.4 varying intercepts + predation + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size + <span class="math inline">\(\gamma\)</span>size</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5)</p>
<p><span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-3" class="section level3">
<h3>1.4 rethinking</h3>
<pre class="r"><code># pred + size 
m1.4 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.4, depth=2)</code></pre>
<pre><code>##             mean        sd         5.5%      94.5%    n_eff    Rhat4
## a[1]   2.4579656 0.7330614  1.325475844  3.6620506 437.7391 1.006476
## a[2]   2.8492861 0.7712522  1.625029048  4.1088901 494.3059 1.007867
## a[3]   1.7253771 0.6950504  0.635255526  2.8533906 368.2276 1.009261
## a[4]   2.8383353 0.7646027  1.619208657  4.0888956 531.2002 1.006413
## a[5]   2.2971910 0.7565910  1.096885817  3.5189068 364.8748 1.004660
## a[6]   2.2974179 0.7682329  1.075167484  3.5287955 385.9223 1.007747
## a[7]   2.7375402 0.7762854  1.540510318  4.0159008 510.5235 1.008090
## a[8]   2.2911324 0.7577026  1.078061866  3.4716400 452.5593 1.008304
## a[9]   2.2515459 0.6731030  1.147597793  3.3111219 288.0229 1.013561
## a[10]  3.5296158 0.6990055  2.454250686  4.6610373 321.8667 1.014994
## a[11]  2.9928010 0.6469506  1.983063334  4.0445625 295.3332 1.013242
## a[12]  2.7552377 0.6349688  1.743119741  3.7528602 260.2500 1.018633
## a[13]  2.7602714 0.6702863  1.702997054  3.8348155 277.6413 1.012382
## a[14]  2.2456441 0.6564364  1.161581739  3.2465075 280.4972 1.011907
## a[15]  3.3040390 0.7082293  2.248223621  4.4542940 347.7010 1.012910
## a[16]  3.2976578 0.6817665  2.213016991  4.3929520 338.6645 1.013241
## a[17]  2.8413755 0.6947344  1.818027996  3.9485850 397.4241 1.014190
## a[18]  2.5593246 0.6107554  1.621318896  3.5547679 387.0001 1.008820
## a[19]  2.2891907 0.6066704  1.339380465  3.2567590 359.9383 1.009125
## a[20]  3.1827166 0.7298559  2.045085701  4.3637802 484.3670 1.011881
## a[21]  2.3470122 0.6535167  1.326850398  3.3993720 380.9617 1.007004
## a[22]  2.3450146 0.6767916  1.243907213  3.4084782 350.5701 1.014278
## a[23]  2.3230196 0.6503329  1.337085269  3.3694296 355.7544 1.008866
## a[24]  1.7886604 0.6150779  0.775094299  2.7500515 301.3555 1.007205
## a[25]  1.6447109 0.5991164  0.670193911  2.5546393 203.4831 1.013456
## a[26]  2.5873929 0.5639028  1.698838360  3.5011733 227.8522 1.020054
## a[27]  1.3482751 0.6190623  0.328691481  2.2803109 224.3765 1.014815
## a[28]  2.0708040 0.5916779  1.094630057  2.9882831 226.7591 1.017545
## a[29]  2.2604899 0.5781861  1.303290832  3.1592312 198.3505 1.016517
## a[30]  3.2207305 0.6114603  2.251155100  4.1968552 244.7510 1.016680
## a[31]  1.6052891 0.5952307  0.646061789  2.5341539 214.1643 1.013111
## a[32]  1.8681253 0.5801088  0.908764427  2.7546563 207.1383 1.015495
## a[33]  3.0192052 0.6519192  2.033920311  4.0807697 318.4510 1.016528
## a[34]  2.7511938 0.6389815  1.742917745  3.7919804 316.7842 1.012696
## a[35]  2.7332250 0.6346301  1.776657197  3.7585160 339.3467 1.011473
## a[36]  2.3005420 0.5892936  1.388092588  3.2564223 289.0198 1.016133
## a[37]  2.0172955 0.6004177  1.086622073  3.0128452 291.2129 1.010623
## a[38]  3.1609862 0.7279270  2.027554091  4.3646966 393.1177 1.012894
## a[39]  2.5088798 0.6350022  1.529161117  3.5356805 347.5110 1.009322
## a[40]  2.2618408 0.6069103  1.328617355  3.2465583 331.9893 1.009664
## a[41]  1.0270315 0.6247272 -0.002321739  1.9922887 225.3827 1.014765
## a[42]  1.9904344 0.5611506  1.093927976  2.8693483 195.6534 1.021632
## a[43]  2.0895472 0.5606129  1.213356777  2.9711137 186.8009 1.019128
## a[44]  2.1988613 0.5634799  1.261339903  3.0870989 190.4319 1.023105
## a[45]  2.6248844 0.5605048  1.696612676  3.4876832 189.2359 1.017043
## a[46]  1.6321177 0.5729979  0.673808417  2.4850994 192.4577 1.016072
## a[47]  3.6950610 0.6043722  2.756528312  4.6530466 261.3659 1.011572
## a[48]  2.1304003 0.5516673  1.256464102  2.9905601 195.7136 1.017154
## bp    -2.4554577 0.2938290 -2.903413334 -1.9746796 514.4669 1.003027
## s[1]   0.3330326 0.3718010 -0.238341967  0.9662793 168.4509 1.022599
## s[2]  -0.1082726 0.3766256 -0.702397262  0.4999938 156.7734 1.032908
## a_bar  2.4239759 0.4119176  1.759472026  3.0529844 133.3000 1.029710
## sigma  0.7756987 0.1464167  0.554500707  1.0274788 520.9809 1.007046</code></pre>
</div>
<div id="inla-3" class="section level3">
<h3>1.4 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i &lt;- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + size.small + size.big + f(tank, &quot;, &quot; model = \&quot;iid\&quot;, hyper = hcprior), family = 
##    \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = density, control.compute = list(config = T, dic = TRUE, &quot;, &quot; waic = TRUE), 
##    control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), 
##    &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec = list(pred = 1, 
##    size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 1.5))&quot; ) 
## Time used:
##     Pre = 1.55, Running = 0.175, Post = 0.223, Total = 1.95 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.471 0.549      0.393    1.471      2.549  1.471   0
## predpred    -2.561 0.285     -3.125   -2.561     -2.000 -2.560   0
## size.small   1.352 0.561      0.251    1.351      2.452  1.351   0
## size.big     0.855 0.560     -0.244    0.854      1.953  0.854   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.15 0.917      0.951     1.97       4.45 1.68
## 
## Expected number of effective parameters(stdev): 27.70(3.71)
## Number of equivalent replicates : 1.73 
## 
## Deviance Information Criterion (DIC) ...............: 205.18
## Deviance Information Criterion (DIC, saturated) ....: 80.80
## Effective number of parameters .....................: 28.06
## 
## Watanabe-Akaike information criterion (WAIC) ...: 203.18
## Effective number of parameters .................: 19.69
## 
## Marginal log-Likelihood:  -124.56 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.4.i$summary.fixed</code></pre>
<pre><code>##                   mean        sd 0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)  1.4711483 0.5493772  0.3928099  1.4710314   2.549145  1.4708434 1.338582e-07
## predpred    -2.5613067 0.2853384 -3.1252940 -2.5608941  -2.000232 -2.5598428 4.159984e-06
## size.small   1.3515418 0.5607661  0.2506563  1.3514723   2.451813  1.3513794 8.502856e-07
## size.big     0.8546349 0.5598008 -0.2437793  0.8543749   1.953462  0.8539036 2.894649e-08</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.4.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7223338 0.1395241 0.4747482 0.7126375 1.023499 0.6951084</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size-predationsize" class="section level2">
<h2>1.5 varying intercepts + predation + size + predation*size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p><strong>this formula’s wrong</strong></p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>predation + <span class="math inline">\(\gamma\)</span>size + <span class="math inline">\(\eta\)</span>size*predation</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5) <span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]<br />
<span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank] σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-4" class="section level3">
<h3>1.5 rethinking</h3>
<pre class="r"><code># pred + size + interaction 
m1.5 &lt;- ulam(
alist(
S ~ binomial( n , p),
logit(p) &lt;- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)</code></pre>
<pre><code>##              mean        sd        5.5%       94.5%     n_eff     Rhat4
## z[1]  -0.02888246 0.8404046 -1.32747662  1.31291250 2988.4840 0.9985868
## z[2]   0.50263598 0.8651835 -0.84576835  1.91282509 2246.4716 0.9987318
## z[3]  -0.95711936 0.7937510 -2.22009992  0.33668908 2731.7762 1.0004672
## z[4]   0.49098618 0.8593978 -0.84502327  1.91341197 2875.5471 1.0007419
## z[5]  -0.03464505 0.8383920 -1.36866266  1.30670543 2917.9500 0.9989515
## z[6]  -0.03284169 0.8198111 -1.29057272  1.26451492 2652.0554 0.9981502
## z[7]   0.48988950 0.8375334 -0.82873910  1.84761549 2319.2978 1.0004364
## z[8]  -0.01940671 0.8492499 -1.36239636  1.35952382 2349.6730 0.9989202
## z[9]  -0.11523888 0.6630032 -1.16923660  0.91444216 2070.9837 0.9995881
## z[10]  1.51005825 0.7145847  0.36312511  2.66056783 1885.5939 1.0006547
## z[11]  0.82794920 0.6922554 -0.24924303  1.96101236 2243.6428 1.0001855
## z[12]  0.51344165 0.7197049 -0.60809728  1.66556123 2490.2055 0.9995235
## z[13]  0.23816040 0.7180877 -0.89932145  1.40505338 2495.6178 1.0003087
## z[14] -0.42291880 0.6725185 -1.47373581  0.65215781 2490.7767 1.0005476
## z[15]  0.95356263 0.7627069 -0.24761980  2.18142033 2757.6788 0.9993780
## z[16]  0.96356190 0.7233970 -0.21400807  2.14184950 3024.9963 0.9987073
## z[17]  0.45623941 0.7807687 -0.77651907  1.72511919 3289.9197 0.9993901
## z[18]  0.07362038 0.7353850 -1.05999707  1.29248200 2199.2285 0.9989577
## z[19] -0.28561297 0.7181330 -1.46962398  0.83054819 2900.7915 0.9997123
## z[20]  0.87838447 0.8261966 -0.38421775  2.23305628 2396.3215 0.9983036
## z[21]  0.07562618 0.7643950 -1.12514921  1.30667624 2359.8559 0.9992687
## z[22]  0.07053525 0.7225555 -1.05602175  1.28443958 2671.5914 0.9985132
## z[23]  0.08421050 0.7294464 -1.05060014  1.30166151 2328.6024 1.0002060
## z[24] -0.54486299 0.6809594 -1.59599036  0.55890278 2010.6780 0.9998541
## z[25] -0.87638688 0.5801153 -1.83743120  0.03157768 1676.9755 0.9991796
## z[26]  0.36027710 0.5738938 -0.55475016  1.29770775 1448.2107 1.0014677
## z[27] -1.28734877 0.6066414 -2.28506746 -0.34347970 1815.8999 1.0009520
## z[28] -0.32071478 0.5569508 -1.22176917  0.55336682 1402.1457 0.9987733
## z[29] -0.48059498 0.5385375 -1.33401602  0.39392519 1860.2536 1.0012899
## z[30]  0.81086463 0.6038863 -0.09941858  1.81793407 1673.0752 1.0006198
## z[31] -1.32966733 0.5515415 -2.25570595 -0.45990007 1778.9299 1.0022807
## z[32] -0.99929098 0.5484733 -1.90067218 -0.13773041 1888.6184 0.9996798
## z[33]  0.66598119 0.7437891 -0.48313471  1.86495018 2578.8964 0.9995099
## z[34]  0.32269317 0.6944090 -0.72719889  1.47058405 2820.2452 0.9987568
## z[35]  0.31247204 0.7208840 -0.85780865  1.49707459 2378.7214 1.0003342
## z[36] -0.28952571 0.6303076 -1.29159728  0.76648674 2486.5362 0.9990342
## z[37] -0.26044995 0.6585279 -1.32793103  0.75765593 2314.0675 0.9993763
## z[38]  1.08377068 0.7826437 -0.14097164  2.41131758 2408.4076 0.9993736
## z[39]  0.31608027 0.6728123 -0.71946974  1.39794722 2020.4719 0.9985546
## z[40]  0.01446648 0.6828458 -1.04990497  1.09291386 2277.6216 0.9993772
## z[41] -1.71949498 0.5935593 -2.68298895 -0.82008127 1546.1882 0.9993456
## z[42] -0.41051703 0.5252086 -1.24534346  0.41236519 1520.7648 1.0008798
## z[43] -0.29165459 0.5251117 -1.15312438  0.53479553 1314.9938 1.0001761
## z[44] -0.14888903 0.5232853 -0.99939186  0.71041276 1454.0309 0.9991397
## z[45] -0.02191828 0.5015772 -0.82386992  0.78128964 1600.4686 1.0009260
## z[46] -1.33867295 0.5287597 -2.21117108 -0.53798803 1331.4350 1.0019680
## z[47]  1.44602586 0.6130391  0.51420886  2.44764406 1675.1634 1.0024050
## z[48] -0.68629763 0.5094692 -1.49009963  0.10845591 1683.6099 0.9999710
## bp[1] -1.86609397 0.3713066 -2.44863056 -1.28333487  972.9955 1.0045223
## bp[2] -2.73117449 0.3860812 -3.33072638 -2.11193511  736.2196 1.0019508
## s[1]   0.12106588 0.3892339 -0.51192663  0.75175870 1138.1957 0.9997352
## s[2]   0.14680620 0.3763601 -0.47491308  0.74344919 1035.2636 1.0027557
## a_bar  2.30740795 0.3857250  1.70515378  2.94477227  899.1069 1.0009318
## sigma  0.75721825 0.1503184  0.54337905  1.01902138  699.8894 1.0016293</code></pre>
<p>I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.</p>
</div>
<div id="inla-4" class="section level3">
<h3>1.5 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i &lt;- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1.5),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + pred * size.small + &quot;, &quot; pred * size.big + f(tank, model = 
##    \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, control.compute = list(config 
##    = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = 
##    list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; 
##    size.big = 0), prec = list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 1.5))&quot; 
##    ) 
## Time used:
##     Pre = 1.69, Running = 0.178, Post = 0.258, Total = 2.12 
## Fixed effects:
##                       mean     sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)          1.469  0.549      0.392    1.469      2.546  1.469   0
## size.small           1.030  0.581     -0.110    1.029      2.171  1.029   0
## size.big             1.171  0.583      0.028    1.171      2.317  1.170   0
## predpred            -1.711 18.258    -37.558   -1.712     34.106 -1.711   0
## size.small:predpred -0.322 18.260    -36.172   -0.323     35.498 -0.322   0
## size.big:predpred   -1.388 18.260    -37.238   -1.389     34.432 -1.388   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.40 1.08       1.02     2.17       5.14 1.82
## 
## Expected number of effective parameters(stdev): 27.14(3.83)
## Number of equivalent replicates : 1.77 
## 
## Deviance Information Criterion (DIC) ...............: 204.28
## Deviance Information Criterion (DIC, saturated) ....: 44.90
## Effective number of parameters .....................: 27.35
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.82
## Effective number of parameters .................: 19.60
## 
## Marginal log-Likelihood:  -127.30 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.5.i$summary.fixed</code></pre>
<pre><code>##                           mean         sd  0.025quant   0.5quant 0.975quant      mode          kld
## (Intercept)          1.4691156  0.5487191   0.3920854  1.4689927   2.545859  1.468792 3.122017e-07
## size.small           1.0299129  0.5809366  -0.1095596  1.0294821   2.170743  1.028668 6.547191e-07
## size.big             1.1712589  0.5830451   0.0280044  1.1707051   2.316552  1.169646 8.373431e-07
## predpred            -1.7112074 18.2583559 -37.5584985 -1.7117217  34.106139 -1.711207 4.773591e-10
## size.small:predpred -0.3220429 18.2596700 -36.1719087 -0.3225574  35.497868 -0.322043 7.925479e-10
## size.big:predpred   -1.3880774 18.2597165 -37.2380297 -1.3885914  34.431907 -1.388076 1.278905e-09</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.5.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5  q0.975      mode
## SD for tank 0.6878866 0.1385057 0.4415024 0.6784345 0.98649 0.6615539</code></pre>
</div>
</div>
<div id="compare-using-waic" class="section level2">
<h2>compare using WAIC</h2>
<div id="compare-rethinking" class="section level4">
<h4>compare rethinking</h4>
<pre class="r"><code>rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )</code></pre>
<pre><code>##          WAIC       SE    dWAIC      dSE    pWAIC     weight
## m1.2 198.6259 8.837299 0.000000       NA 18.97851 0.32498460
## m1.5 198.9680 8.939538 0.342044 2.987600 18.75819 0.27389800
## m1.1 200.0485 7.342007 1.422565 5.698355 20.90917 0.15957201
## m1.4 200.1878 8.772340 1.561930 1.909658 19.26895 0.14883117
## m1.3 201.1344 7.236553 2.508512 5.557839 21.29859 0.09271422</code></pre>
<p>These models are really very similar in expected out-of-sample accuracy. The tank variation is huge. But take a look at the posterior distributions for predation and size. You’ll see that predation does seem to matter, as you’d expect. Size matters a lot less. So while predation doesn’t explain much of the total variation, there is plenty of evidence that it is a real effect. Remember: We don’t select a model using WAIC (or LOO). A predictor can make little difference in total accuracy but still be a real causal effect.</p>
</div>
<div id="compare-inla" class="section level4">
<h4>compare inla</h4>
<pre class="r"><code>inla.models.8.1 &lt;- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic &lt;- function (x){
  x[[&quot;waic&quot;]][[&quot;waic&quot;]]
}

waic.8.1 &lt;- bind_cols(model = c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1</code></pre>
<pre><code>## # A tibble: 5 x 2
##   model   waic
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 m1.1.i  206.
## 2 m1.2.i  203.
## 3 m1.3.i  206.
## 4 m1.4.i  203.
## 5 m1.5.i  203.</code></pre>
<p>Let’s look at all the sigma posterior distributions: The two models that omit predation, m1.1 and m1.3, have larger values of sigma. This is because predation explains some of the variation among tanks. So when you add it to the model, the variation in the tank intercepts gets smaller.</p>
<pre class="r"><code>sigma.8.1 &lt;- bind_cols( model= c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), do.call(rbind.data.frame, lapply(inla.models.8.1 ,bri.hyperpar.summary)))


sigma.8.1</code></pre>
<pre><code>##               model      mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank  m1.1.i 1.5919697 0.2096781 1.2310838 1.5739639 2.053666 1.5392289
## SD for tank1 m1.2.i 0.7826381 0.1398744 0.5378891 0.7718996 1.087421 0.7524708
## SD for tank2 m1.3.i 1.5842814 0.2109564 1.2215237 1.5660554 2.049100 1.5308777
## SD for tank3 m1.4.i 0.7223338 0.1395241 0.4747482 0.7126375 1.023499 0.6951084
## SD for tank4 m1.5.i 0.6878866 0.1385057 0.4415024 0.6784345 0.986490 0.6615539</code></pre>
<pre class="r"><code>sigma.8.1.plot &lt;-  ggplot(data= sigma.8.1, aes(y=model, x=mean, label=model)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=q0.025, xmax=q0.975), height=.3) +
    coord_fixed(ratio=.3) +
    theme_bw()

sigma.8.1.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/sigma.8.1%20plot-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p><strong>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</strong></p>
<p><strong>(1) district: ID number of administrative district each woman resided in</strong></p>
<p><strong>(2) use.contraception: An indicator (0/1) of whether the woman was using contraception</strong></p>
<p><strong>Focus on predicting use.contraception, clustered by district_id. Fit both:</strong></p>
<p><strong>1) a traditional fixed-effects model that uses an index variable for district</strong></p>
<p><strong>2) a multilevel model with varying intercepts for district.</strong></p>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?**</p>
<pre class="r"><code>library(rethinking)
data(bangladesh)
d &lt;- bangladesh</code></pre>
<p>The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:</p>
<pre class="r"><code>sort(unique(d$district))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42
## [43] 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61</code></pre>
<p>District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:</p>
<pre class="r"><code>d$district_id &lt;- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42
## [43] 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</code></pre>
<p>Now there are 60 values, contiguous integers 1 to 60.</p>
<div id="traditional-fixed-effects-model-that-uses-an-index-variable-for-district" class="section level2">
<h2>2.1 traditional fixed-effects model that uses an index variable for district</h2>
<div id="rethinking-5" class="section level3">
<h3>2.1 rethinking</h3>
<pre class="r"><code>dat_list &lt;- list(
C = d$use.contraception, 
did = d$district_id
)

m2.1 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( 0 , 1.5 )
) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.1, depth = 2)</code></pre>
<pre><code>##               mean        sd        5.5%        94.5%    n_eff     Rhat4
## a[1]  -1.058509818 0.2024233 -1.38350536 -0.741299198 4301.957 0.9990664
## a[2]  -0.594439868 0.4673922 -1.34755713  0.135006580 5027.151 0.9987441
## a[3]   1.212539943 1.1058588 -0.47657821  2.976092348 5488.472 0.9987932
## a[4]  -0.002310187 0.3625893 -0.56277037  0.603094797 4431.175 1.0001435
## a[5]  -0.570359513 0.3119290 -1.05887594 -0.089045952 5014.183 0.9986644
## a[6]  -0.863026227 0.2663939 -1.29666378 -0.456120965 4544.040 0.9988095
## a[7]  -0.890889393 0.5155354 -1.69794902 -0.107391584 5636.830 0.9982049
## a[8]  -0.478353334 0.3351002 -1.04058508  0.059218405 4815.072 0.9985891
## a[9]  -0.792465320 0.4778007 -1.56847915 -0.030904172 4437.205 0.9988901
## a[10] -1.960052820 0.7366337 -3.23913439 -0.864216552 3292.787 0.9987995
## a[11] -2.965388362 0.8604671 -4.48895850 -1.739695408 3406.031 0.9987697
## a[12] -0.620803409 0.3829237 -1.22548602 -0.011826693 4593.254 0.9986530
## a[13] -0.321724937 0.3974905 -0.97539347  0.282927129 3755.787 0.9990419
## a[14]  0.521711718 0.1913980  0.22190711  0.830423905 3689.628 0.9984180
## a[15] -0.532740494 0.4480991 -1.25714677  0.178923072 4289.812 0.9985942
## a[16]  0.193786953 0.4444082 -0.50920154  0.912095157 6005.061 0.9984552
## a[17] -0.848611485 0.4361030 -1.56343478 -0.168360119 5075.938 0.9985811
## a[18] -0.643637040 0.3031752 -1.15006541 -0.157040193 4921.654 0.9991886
## a[19] -0.438571043 0.4016482 -1.08111593  0.176802802 5840.306 0.9983735
## a[20] -0.371450707 0.5148285 -1.22984761  0.459124130 4613.887 0.9994652
## a[21] -0.435162731 0.4606781 -1.19031932  0.312633322 4182.377 0.9985866
## a[22] -1.291387423 0.5320234 -2.17678295 -0.493558127 3728.932 0.9984926
## a[23] -0.926461804 0.5623798 -1.87164976 -0.045141890 3854.576 0.9993953
## a[24] -2.029096497 0.7375098 -3.30952182 -0.929618845 3119.024 0.9985147
## a[25] -0.211662510 0.2483479 -0.61109825  0.176703119 3855.269 0.9992852
## a[26] -0.433338933 0.5536723 -1.33369917  0.414188114 3835.926 0.9987575
## a[27] -1.455629462 0.3652016 -2.06770189 -0.881785687 4521.761 0.9986991
## a[28] -1.097712571 0.3380567 -1.64321170 -0.573783526 3391.561 0.9988518
## a[29] -0.911001441 0.3881658 -1.54670205 -0.330868471 4968.829 0.9983432
## a[30] -0.029803829 0.2471023 -0.43474889  0.368357124 5238.247 0.9994436
## a[31] -0.173559094 0.3628392 -0.75678312  0.415763059 5967.146 0.9988625
## a[32] -1.244667922 0.5006268 -2.07550144 -0.485108419 4677.307 0.9988052
## a[33] -0.259588118 0.5166419 -1.09354261  0.557261866 6602.060 0.9985454
## a[34]  0.628487149 0.3535116  0.08015547  1.197251322 5249.620 0.9992562
## a[35]  0.006193485 0.2718522 -0.41436242  0.449062332 4120.844 0.9990586
## a[36] -0.578956983 0.4926025 -1.38730789  0.199275673 5102.596 0.9993550
## a[37]  0.144014749 0.5035983 -0.63892617  0.958713253 3440.248 0.9986543
## a[38] -0.846201450 0.5487679 -1.75693317  0.004379047 5362.627 0.9990243
## a[39]  0.002955411 0.4059664 -0.62942107  0.639763708 3830.900 0.9989152
## a[40] -0.138774293 0.3032757 -0.61756251  0.326918017 2681.177 0.9985997
## a[41] -0.002500453 0.3939926 -0.62497244  0.644946342 4335.229 0.9986939
## a[42]  0.174413967 0.5942287 -0.78330287  1.111860728 4235.019 0.9985323
## a[43]  0.132911188 0.2989399 -0.35137975  0.617112323 4641.081 0.9983526
## a[44] -1.187593519 0.4244791 -1.89875423 -0.537020750 3912.905 0.9995447
## a[45] -0.678912194 0.3422674 -1.24045505 -0.149665470 3790.677 0.9986015
## a[46]  0.093369428 0.2120438 -0.24570473  0.425263824 3335.352 0.9996998
## a[47] -0.126537094 0.5000257 -0.90299381  0.657382266 5067.630 0.9986876
## a[48]  0.095105334 0.3015368 -0.38561408  0.581769656 4144.310 1.0007846
## a[49] -1.725066052 1.0724358 -3.48830214 -0.085807655 4409.648 0.9987354
## a[50] -0.107076429 0.4489512 -0.83537228  0.615100725 4100.247 0.9987050
## a[51] -0.164498153 0.3270104 -0.66681924  0.347720280 4752.969 0.9987827
## a[52] -0.221505023 0.2575402 -0.63216231  0.182691127 4226.009 0.9987104
## a[53] -0.295483920 0.4465892 -1.02619157  0.413365860 4341.331 0.9989927
## a[54] -1.245914548 0.8215143 -2.59492701  0.022646682 3758.760 0.9988385
## a[55]  0.301088265 0.3151099 -0.18546813  0.827868630 5333.340 0.9987500
## a[56] -1.390475844 0.4502284 -2.11919216 -0.711650484 4238.068 0.9990045
## a[57] -0.174010240 0.3663794 -0.77550822  0.397352736 4021.672 0.9991639
## a[58] -1.719850060 0.7237320 -2.91970152 -0.639548347 3865.707 0.9986491
## a[59] -1.221327075 0.4242492 -1.91565320 -0.584648510 4550.192 0.9988270
## a[60] -1.260800391 0.3537035 -1.85058803 -0.707759352 6355.755 0.9988003</code></pre>
</div>
<div id="inla-5" class="section level3">
<h3>2.1 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d2.i &lt;- d %&gt;% 
  mutate(did= paste(&quot;d&quot;, as.integer(d$district_id), sep= &quot;.&quot;), 
         d.value= 1
         ) %&gt;% 
  spread(did, d.value)

#use this to quickly make a list of the index vbles to include in the model 
did_formula &lt;- paste(&quot;d&quot;, 1:60, sep=&quot;.&quot;, collapse = &quot;+&quot;)


m2.1.i &lt;- inla(use.contraception ~ d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9+d.10+d.11+d.12+d.13+d.14+d.15+d.16+d.17+d.18+d.19+d.20+d.21+d.22+d.23+d.24+d.25+d.26+d.27+d.28+d.29+d.30+d.31+d.32+d.33+d.34+d.35+d.36+d.37+d.38+d.39+d.40+d.41+d.42+d.43+d.44+d.45+d.46+d.47+d.48+d.49+d.50+d.51+d.52+d.53+d.54+d.55+d.56+d.57+d.58+d.59+d.60, data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean=  0 ,
        prec= 1.5),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ d.1 + d.2 + d.3 + d.4 + d.5 + &quot;, &quot; d.6 + d.7 + d.8 + d.9 + d.10 + d.11 + d.12 + 
##    d.13 + d.14 + &quot;, &quot; d.15 + d.16 + d.17 + d.18 + d.19 + d.20 + d.21 + d.22 + d.23 + &quot;, &quot; d.24 + d.25 + d.26 + d.27 + 
##    d.28 + d.29 + d.30 + d.31 + d.32 + &quot;, &quot; d.33 + d.34 + d.35 + d.36 + d.37 + d.38 + d.39 + d.40 + d.41 + &quot;, &quot; d.42 + 
##    d.43 + d.44 + d.45 + d.46 + d.47 + d.48 + d.49 + d.50 + &quot;, &quot; d.51 + d.52 + d.53 + d.54 + d.55 + d.56 + d.57 + d.58 + 
##    d.59 + &quot;, &quot; d.60, family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, control.compute = list(config = T, &quot;, &quot; waic = 
##    TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = list(model = 
##    \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = 0, prec = 1.5))&quot;) 
## Time used:
##     Pre = 4.95, Running = 0.497, Post = 0.826, Total = 6.27 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.582 0.121     -0.818   -0.582     -0.345 -0.581   0
## d.1         -0.455 0.233     -0.920   -0.453     -0.005 -0.447   0
## d.2         -0.030 0.416     -0.865   -0.023      0.769 -0.011   0
## d.3          0.645 0.708     -0.743    0.644      2.036  0.642   0
## d.4          0.485 0.349     -0.201    0.486      1.167  0.487   0
## d.5          0.000 0.326     -0.650    0.004      0.629  0.011   0
## d.6         -0.275 0.279     -0.833   -0.271      0.263 -0.263   0
## d.7         -0.268 0.443     -1.166   -0.259      0.575 -0.240   0
## d.8          0.071 0.330     -0.586    0.075      0.709  0.082   0
## d.9         -0.190 0.403     -1.004   -0.182      0.581 -0.167   0
## d.10        -0.920 0.542     -2.037   -0.902      0.091 -0.865   0
## d.11        -1.527 0.535     -2.639   -1.505     -0.539 -1.461   0
## d.12        -0.051 0.365     -0.783   -0.045      0.652 -0.035   0
## d.13         0.194 0.383     -0.568    0.198      0.935  0.205   0
## d.14         1.046 0.217      0.623    1.045      1.474  1.043   0
## d.15         0.015 0.400     -0.787    0.021      0.786  0.032   0
## d.16         0.601 0.403     -0.190    0.601      1.393  0.600   0
## d.17        -0.238 0.400     -1.045   -0.230      0.525 -0.214   0
## d.18        -0.072 0.306     -0.684   -0.068      0.520 -0.061   0
## d.19         0.088 0.375     -0.660    0.093      0.812  0.101   0
## d.20         0.123 0.452     -0.781    0.129      0.996  0.140   0
## d.21         0.095 0.426     -0.758    0.100      0.917  0.111   0
## d.22        -0.563 0.447     -1.476   -0.551      0.280 -0.526   0
## d.23        -0.290 0.472     -1.246   -0.279      0.606 -0.258   0
## d.24        -0.972 0.536     -2.077   -0.953      0.027 -0.916   0
## d.25         0.341 0.260     -0.173    0.342      0.849  0.344   0
## d.26         0.074 0.475     -0.879    0.080      0.989  0.093   0
## d.27        -0.761 0.352     -1.479   -0.752     -0.096 -0.734   0
## d.28        -0.471 0.320     -1.117   -0.465      0.139 -0.453   0
## d.29        -0.293 0.363     -1.026   -0.286      0.401 -0.272   0
## d.30         0.500 0.268     -0.027    0.500      1.025  0.501   0
## d.31         0.337 0.338     -0.332    0.339      0.996  0.342   0
## d.32        -0.558 0.420     -1.414   -0.547      0.235 -0.524   0
## d.33         0.203 0.460     -0.714    0.208      1.094  0.217   0
## d.34         1.041 0.335      0.393    1.038      1.709  1.032   0
## d.35         0.517 0.293     -0.058    0.518      1.091  0.518   0
## d.36        -0.019 0.439     -0.901   -0.012      0.824  0.001   0
## d.37         0.504 0.467     -0.415    0.505      1.417  0.507   0
## d.38        -0.223 0.478     -1.191   -0.213      0.687 -0.193   0
## d.39         0.473 0.367     -0.251    0.474      1.191  0.476   0
## d.40         0.379 0.311     -0.236    0.380      0.987  0.383   0
## d.41         0.473 0.367     -0.251    0.474      1.191  0.476   0
## d.42         0.495 0.492     -0.474    0.496      1.456  0.498   0
## d.43         0.631 0.300      0.043    0.631      1.220  0.630   0
## d.44        -0.516 0.400     -1.331   -0.506      0.242 -0.486   0
## d.45        -0.097 0.329     -0.756   -0.092      0.537 -0.083   0
## d.46         0.631 0.237      0.166    0.631      1.096  0.630   0
## d.47         0.319 0.447     -0.569    0.322      1.188  0.328   0
## d.48         0.592 0.307     -0.011    0.592      1.195  0.592   0
## d.49        -0.619 0.672     -1.981   -0.605      0.660 -0.576   0
## d.50         0.361 0.412     -0.455    0.364      1.164  0.368   0
## d.51         0.360 0.324     -0.280    0.361      0.991  0.364   0
## d.52         0.319 0.270     -0.213    0.320      0.845  0.322   0
## d.53         0.198 0.416     -0.630    0.202      1.002  0.210   0
## d.54        -0.415 0.612     -1.656   -0.401      0.747 -0.374   0
## d.55         0.789 0.301      0.202    0.788      1.382  0.785   0
## d.56        -0.674 0.412     -1.516   -0.662      0.102 -0.639   0
## d.57         0.337 0.338     -0.332    0.339      0.996  0.342   0
## d.58        -0.742 0.564     -1.899   -0.724      0.318 -0.690   0
## d.59        -0.550 0.379     -1.321   -0.541      0.168 -0.522   0
## d.60        -0.599 0.347     -1.302   -0.591      0.059 -0.575   0
## 
## Expected number of effective parameters(stdev): 46.14(0.00)
## Number of equivalent replicates : 41.92 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2515.50
## Effective number of parameters .................: 44.53
## 
## Marginal log-Likelihood:  -1273.53 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="varying-intercepts-model-1" class="section level2">
<h2>2.2 varying intercepts model</h2>
<div id="rethinking-6" class="section level3">
<h3>2.2 rethinking</h3>
<pre class="r"><code>m2.2 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( a_bar , sigma ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
) ,data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.2, depth= 2)</code></pre>
<pre><code>##               mean         sd       5.5%        94.5%     n_eff     Rhat4
## a[1]  -0.988967240 0.20277626 -1.3095585 -0.666312008 3601.0457 1.0001431
## a[2]  -0.597565001 0.34634832 -1.1561365 -0.047512906 4205.8198 0.9984997
## a[3]  -0.239722745 0.49816958 -0.9989331  0.563485488 3073.8676 0.9987417
## a[4]  -0.191327260 0.31461116 -0.6962561  0.322792059 4935.5904 0.9983345
## a[5]  -0.578376805 0.28020609 -1.0279583 -0.144404431 2691.8047 1.0011177
## a[6]  -0.807017864 0.23626345 -1.1871125 -0.442212689 3965.6478 0.9993583
## a[7]  -0.748493066 0.35339092 -1.3026979 -0.201348146 4062.0991 0.9987231
## a[8]  -0.518682449 0.28031479 -0.9680701 -0.074209172 4442.5866 0.9983895
## a[9]  -0.706002372 0.32898246 -1.2306463 -0.190392935 4791.4007 0.9993511
## a[10] -1.119445805 0.40420319 -1.7925733 -0.482713414 3026.1411 0.9987202
## a[11] -1.533631020 0.43652380 -2.2620058 -0.892120002 1512.0731 1.0006081
## a[12] -0.623397536 0.31101143 -1.1354823 -0.114651445 4357.7048 0.9986578
## a[13] -0.417675724 0.32775014 -0.9401161  0.112243183 3724.6347 0.9999256
## a[14]  0.383023813 0.17530132  0.1004583  0.656962871 3085.7334 0.9986715
## a[15] -0.551942972 0.34025314 -1.1180938 -0.017613089 4728.5948 0.9988208
## a[16] -0.129222361 0.34899523 -0.6793070  0.431955627 3516.3900 0.9982266
## a[17] -0.743581487 0.32767590 -1.2834632 -0.225998064 4286.9192 0.9987991
## a[18] -0.632486039 0.27208064 -1.0794562 -0.204334178 3506.4590 0.9991231
## a[19] -0.504312618 0.30736105 -0.9965505 -0.005112522 4087.3486 0.9996233
## a[20] -0.488816083 0.36168841 -1.0684193  0.078834594 3109.2757 1.0017900
## a[21] -0.499883072 0.34740213 -1.0659874  0.044178016 3913.8392 0.9986199
## a[22] -0.950933070 0.35807645 -1.5210023 -0.415887265 2980.0645 0.9992905
## a[23] -0.762958032 0.39203100 -1.4266004 -0.134796345 4133.3195 0.9982111
## a[24] -1.149223719 0.40285136 -1.8110555 -0.548970100 2130.9777 1.0006029
## a[25] -0.281232605 0.22351966 -0.6637149  0.083945641 4260.8962 0.9985947
## a[26] -0.513953269 0.38650886 -1.1339318  0.104208486 2613.4381 0.9989470
## a[27] -1.174183818 0.29951840 -1.6679441 -0.697758776 3026.7057 0.9986243
## a[28] -0.964836994 0.25594823 -1.3646236 -0.561932680 2516.1877 0.9999114
## a[29] -0.799381553 0.31117085 -1.3205180 -0.322839122 4694.5867 0.9992505
## a[30] -0.143671532 0.22852821 -0.5102696  0.220709792 4411.0647 0.9984769
## a[31] -0.309929508 0.29765777 -0.7774037  0.148949307 4042.1490 0.9992144
## a[32] -0.966185241 0.35486980 -1.5468226 -0.429583612 3293.6113 0.9988311
## a[33] -0.427091047 0.37739555 -1.0307997  0.188574809 4551.5019 0.9989711
## a[34]  0.264333458 0.28827702 -0.1843817  0.729637806 3089.8298 0.9983378
## a[35] -0.138647336 0.25786560 -0.5563803  0.283039169 3165.2193 0.9991096
## a[36] -0.581279305 0.35115664 -1.1403207 -0.023393776 3492.9796 0.9997484
## a[37] -0.232987551 0.39244256 -0.8677667  0.404262712 3856.6120 0.9984121
## a[38] -0.721499671 0.39168848 -1.3569887 -0.112092602 3615.8198 0.9982627
## a[39] -0.208908897 0.31921604 -0.7012013  0.295409051 2975.7670 0.9987610
## a[40] -0.260834265 0.26801123 -0.6885165  0.170279229 4543.8485 0.9983775
## a[41] -0.211333523 0.31637157 -0.7262940  0.287772682 4676.8107 0.9984405
## a[42] -0.239670239 0.39142345 -0.8724315  0.395883123 4410.8850 0.9990053
## a[43] -0.049189558 0.25993251 -0.4579570  0.361589590 3909.0594 0.9992449
## a[44] -0.949332280 0.32808050 -1.4722744 -0.441556175 3318.9228 0.9989648
## a[45] -0.647213976 0.28051864 -1.0969279 -0.221873830 4422.7785 0.9986668
## a[46] -0.007803194 0.20039804 -0.3293266  0.302732626 5061.8990 0.9988411
## a[47] -0.348111712 0.36535233 -0.9166964  0.243928814 2959.0763 0.9983514
## a[48] -0.087239740 0.26144618 -0.5031730  0.336949265 3530.2089 0.9987927
## a[49] -0.850799704 0.49490689 -1.6791685 -0.067318749 2159.0019 0.9996339
## a[50] -0.319420030 0.34884263 -0.8773046  0.234008757 3245.3814 1.0007332
## a[51] -0.283301836 0.27884341 -0.7380029  0.155256438 4188.2615 0.9992429
## a[52] -0.299030019 0.23443300 -0.6738843  0.076202858 4261.9800 0.9986779
## a[53] -0.422484774 0.35503802 -1.0119494  0.134743529 3685.8840 0.9987663
## a[54] -0.768121009 0.45802251 -1.5128229 -0.047494848 3064.2514 0.9985496
## a[55]  0.087674088 0.25728757 -0.3196817  0.495259750 2645.9864 0.9990217
## a[56] -1.069039116 0.35103981 -1.6496168 -0.533728538 2719.2691 1.0003793
## a[57] -0.306503160 0.27908001 -0.7554898  0.130357780 4090.3261 0.9987209
## a[58] -0.991575611 0.43533818 -1.7103570 -0.313720879 2443.8224 0.9992296
## a[59] -0.986951815 0.31267791 -1.4970251 -0.508022284 4361.5869 0.9988942
## a[60] -1.052447665 0.30058796 -1.5596872 -0.588709404 3605.7213 0.9991795
## a_bar -0.538136644 0.08646549 -0.6755201 -0.399128709 1550.8861 0.9996833
## sigma  0.510310174 0.08221621  0.3884734  0.648692130  663.6208 1.0044833</code></pre>
</div>
<div id="inla-6" class="section level3">
<h3>2.2 inla</h3>
<pre class="r"><code>m2.2.i &lt;- inla(use.contraception ~ f(district_id, model=&quot;iid&quot;), data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean.intercept=  0 ,
        prec.intercept= 1.5),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ f(district_id, model = \&quot;iid\&quot;), &quot;, &quot; family = \&quot;binomial\&quot;, data = d2.i, 
##    Ntrials = 1, control.compute = list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = T), 
##    &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean.intercept = 0, 
##    prec.intercept = 1.5))&quot; ) 
## Time used:
##     Pre = 1.39, Running = 1.03, Post = 0.266, Total = 2.68 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.529 0.083     -0.696   -0.527     -0.368 -0.525   0
## 
## Random effects:
##   Name     Model
##     district_id IID model
## 
## Model hyperparameters:
##                           mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for district_id 4.73 1.64       2.38     4.44       8.75 3.96
## 
## Expected number of effective parameters(stdev): 33.96(4.23)
## Number of equivalent replicates : 56.94 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2514.77
## Effective number of parameters .................: 33.28
## 
## Marginal log-Likelihood:  -1278.40 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479287 0.07887218 0.3381725 0.4742629 0.6481624 0.4651482</code></pre>
<p>Side note: this is how you calculate the sd from the hyperprior (<span class="math inline">\(\sigma\)</span>)</p>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                        mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.479287 0.07887218 0.3381725 0.4742629 0.6481624 0.4651482</code></pre>
<pre class="r"><code># hyperparameter of the precision
m2.2.i.prec &lt;- m2.2.i$internal.marginals.hyperpar

#transform precision to sd using inla.tmarginal
#m2.2.i.prec[[1]] is used to access the actual values inside the list
m2.2.i.sd &lt;- inla.tmarginal(function(x) sqrt(exp(-x)), m2.2.i.prec[[1]])
#plot the post of the sd per district (sigma)
plot(m2.2.i.sd)</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.2%20inla%20hyper%20sd-1.png" width="672" /></p>
<pre class="r"><code>#summary stats for the sd 
m2.2.i.sd.sum &lt;- inla.zmarginal(m2.2.i.sd)</code></pre>
<pre><code>## Mean            0.479287 
## Stdev           0.0788722 
## Quantile  0.025 0.338172 
## Quantile  0.25  0.423885 
## Quantile  0.5   0.474263 
## Quantile  0.75  0.529096 
## Quantile  0.975 0.648162</code></pre>
<pre class="r"><code># this coincides perfectly with the result from bri.hyperpar.summary</code></pre>
</div>
</div>
<div id="plot-of-posterior-mean-probabilities-in-each-district" class="section level2">
<h2>2.3 plot of posterior mean probabilities in each district</h2>
<p>Now let’s extract the samples, compute posterior mean probabilities in each district, and plot it all:</p>
<div id="plot-rethinking" class="section level3">
<h3>plot rethinking</h3>
<pre class="r"><code>post1 &lt;- extract.samples( m2.1 ) 
post2 &lt;- extract.samples( m2.2 )
p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) 
p2 &lt;- apply( inv_logit(post2$a) , 2 , mean )
nd &lt;- max(dat_list$did)
plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; )
points( 1:nd , p1 , pch=16 , col=rangi2 ) 
points( 1:nd , p2 )
abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20rethinking-1.png" width="672" /></p>
</div>
<div id="plot-inla" class="section level3">
<h3>plot inla</h3>
<p><a href="https://people.bath.ac.uk/jjf23/inla/oneway.html" class="uri">https://people.bath.ac.uk/jjf23/inla/oneway.html</a></p>
<p><a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>posterior mean for each district a for the idex fixed effect model m2.1:</strong></p>
<pre class="r"><code># m2.2.i$summary.fixed[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
# we eliminate the first element of this list, the intercept.
m2.1.i.fix&lt;- lapply(m2.1.i$marginals.fixed, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))[-1]</code></pre>
<p><strong>posterior mean for each district a for the varying intercept model m2.2:</strong></p>
<pre class="r"><code># m2.2.i$summary.random[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
m2.2.i.rand&lt;- lapply(m2.2.i$marginals.random$district_id, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))</code></pre>
<pre class="r"><code># sapply(m2.2.i.rand, function(x) x[1]) extracts the first element (the mean) from the summary of the posterior of each district
m2.i.mean &lt;- bind_cols(district= 1:60,mean.m2.1= unlist(sapply(m2.1.i.fix, function(x) x[1])), mean.m2.2=unlist(sapply(m2.2.i.rand, function(x) x[1])))

m2.2.i.abar &lt;- inla.zmarginal( inla.tmarginal (inverse_logit, m2.2.i$marginals.fixed[[&quot;(Intercept)&quot;]] ))</code></pre>
<pre><code>## Mean            0.371072 
## Stdev           0.0192745 
## Quantile  0.025 0.332831 
## Quantile  0.25  0.358142 
## Quantile  0.5   0.371125 
## Quantile  0.75  0.384025 
## Quantile  0.975 0.408777</code></pre>
<pre class="r"><code>m2.i.district.plot &lt;- ggplot() +
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.1), color= &quot;blue&quot;, alpha= 0.5)+
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.2), color= &quot;black&quot;, alpha= 0.5, shape= 1)+
  geom_hline(yintercept=m2.2.i.abar[[1]], linetype=&#39;longdash&#39;) +
  ylim(0,1)+
  labs(y = &quot;prob use contraception&quot;)+
  theme_bw()
  

m2.i.district.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20inla-1.png" width="672" /></p>
<p>The blue points are the fixed estimations. The open points are the varying effects. As you’d expect, they are shrunk towards the mean (the dashed line). Some are shrunk more than others. The third district from the left shrunk a lot. Let’s look at the sample size in each district:</p>
<pre class="r"><code> table(d$district_id)</code></pre>
<pre><code>## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33 
## 117  20   2  30  39  65  18  37  23  13  21  29  24 118  22  20  24  47  26  15  18  20  15  14  67  13  44  49  32  61  33  24  14 
##  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  35  48  17  13  14  26  41  26  11  45  27  39  86  15  42   4  19  37  61  19   6  45  27  33  10  32  42</code></pre>
<p>District 3 has only 2 women sampled. So it shrinks a lot. There are couple of other districts, like 49 and 54, that also have very few women sampled. But their fixed estimates aren’t as extreme, so they don’t shrink as much as district 3 does. All of this is explained by partial pooling, of course.</p>
</div>
</div>
</div>
<div id="section-2" class="section level1">
<h1>3.</h1>
<p>I don’t really care about ordered categorical data so i’m going to skip exercise 3.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
