<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Rethinking 2nd edition Homework 8 in INLA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="rethinkingINLA_HW2.html">Homework 2</a>
</li>
<li>
  <a href="rethinkingINLA_HW3.html">Homework 3</a>
</li>
<li>
  <a href="rethinkingINLA_HW4.html">Homework 4</a>
</li>
<li>
  <a href="rethinkingINLA_HW5.html">Homework 5</a>
</li>
<li>
  <a href="rethinkingINLA_HW6.html">Homework 6</a>
</li>
<li>
  <a href="rethinkingINLA_HW8.html">Homework 8</a>
</li>
<li>
  <a href="rethinkingINLA_HW9.html">Homework 9</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical Rethinking 2nd edition Homework 8 in INLA</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)</code></pre>
<div id="section" class="section level1">
<h1>1.</h1>
<p><strong>Revisit the Reed frog survival data, data(reedfrogs),and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the σ across tanks). Explain why it changes as it does across models with different predictors included.</strong></p>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs</code></pre>
<div id="varying-intercepts-model" class="section level2">
<h2>1.1 varying intercepts model</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i]</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking" class="section level3">
<h3>1.1 rethinking</h3>
<pre class="r"><code>dat &lt;- list(
S = d$surv,
n = d$density,
tank = 1:nrow(d),
pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), 
size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L )
)</code></pre>
<pre class="r"><code>m1.1 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank],
a[tank] ~ normal( a_bar , sigma ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )
precis(m1.1, depth= 2)</code></pre>
<pre><code>##               mean        sd         5.5%        94.5%    n_eff     Rhat4
## a[1]   2.132686301 0.8653871  0.880339953  3.602653843 2606.619 1.0018433
## a[2]   3.054723325 1.0555594  1.536873972  4.897660876 3645.642 0.9990371
## a[3]   0.985651852 0.6682581 -0.027381839  2.081910911 4887.494 0.9988877
## a[4]   3.033400098 1.1226418  1.414773972  4.957261689 3421.554 0.9986900
## a[5]   2.151807471 0.8657385  0.912857713  3.631660970 2490.696 1.0019974
## a[6]   2.144854844 0.8979006  0.839098538  3.669096209 3382.299 0.9986957
## a[7]   3.080027597 1.1033895  1.543817560  4.970281393 2606.705 0.9991082
## a[8]   2.134188252 0.8865789  0.807924270  3.671372814 4402.373 0.9990896
## a[9]  -0.169803975 0.6257065 -1.187010526  0.834499616 4198.334 1.0002909
## a[10]  2.115418523 0.7893158  0.931881409  3.436617897 3713.569 0.9984290
## a[11]  1.006749525 0.6670074 -0.004355151  2.088034894 4488.129 0.9984987
## a[12]  0.600852212 0.6237593 -0.380912848  1.620448022 3478.721 0.9995685
## a[13]  1.000347730 0.6627114 -0.041421482  2.119778674 3882.797 0.9985176
## a[14]  0.209072416 0.6190961 -0.777682308  1.199184289 5583.089 0.9996903
## a[15]  2.112063075 0.8368414  0.903519897  3.497692578 3570.948 0.9985201
## a[16]  2.127649262 0.8858648  0.833753691  3.662055789 3669.219 0.9995648
## a[17]  2.904886735 0.8060631  1.756331599  4.285865887 2674.414 0.9990197
## a[18]  2.392551200 0.6651561  1.423517822  3.554973254 3920.878 0.9997532
## a[19]  2.003351079 0.5730673  1.140855633  2.949501788 4136.344 0.9982105
## a[20]  3.666209871 0.9988552  2.247438948  5.368007580 3344.322 0.9990186
## a[21]  2.402168052 0.6647918  1.416719857  3.522408022 3748.974 0.9986468
## a[22]  2.386274894 0.6464756  1.428669022  3.442615520 2761.610 1.0007508
## a[23]  2.397871589 0.6937663  1.384201448  3.601776517 3693.154 0.9988302
## a[24]  1.695485388 0.5096441  0.936653947  2.524813585 3703.923 0.9985435
## a[25] -1.003444990 0.4405017 -1.747865698 -0.323861988 3681.427 0.9998120
## a[26]  0.155564940 0.3897798 -0.490760544  0.751553439 5423.927 0.9990328
## a[27] -1.411463161 0.4930014 -2.271328929 -0.648200678 3506.913 0.9995836
## a[28] -0.481370332 0.4229778 -1.132332459  0.178277074 3773.013 0.9988514
## a[29]  0.165150494 0.4107839 -0.474680107  0.827099662 4379.621 0.9982883
## a[30]  1.449495611 0.4956000  0.697700502  2.299117243 5324.645 0.9987687
## a[31] -0.636854635 0.4241882 -1.330883632  0.021556589 5404.817 0.9990942
## a[32] -0.300419894 0.4176870 -0.961681442  0.346255038 3562.643 0.9988598
## a[33]  3.182752492 0.7911357  2.004921856  4.514142310 3174.377 1.0020357
## a[34]  2.705107792 0.6246406  1.795100267  3.729174022 3331.737 0.9982578
## a[35]  2.711211190 0.6380341  1.780440608  3.745074832 3009.830 0.9997170
## a[36]  2.061809091 0.5297369  1.278955402  2.960974608 3383.804 0.9997371
## a[37]  2.050415478 0.5364943  1.235773293  2.937251757 3396.702 0.9990883
## a[38]  3.886561769 0.9678300  2.513494972  5.531030954 2434.360 1.0022627
## a[39]  2.710387705 0.6665550  1.732916258  3.877169810 2678.475 0.9995161
## a[40]  2.345085218 0.5282347  1.576538848  3.242025927 3005.754 0.9984450
## a[41] -1.802947789 0.4742799 -2.599158891 -1.091026127 3307.311 0.9985758
## a[42] -0.570717197 0.3511880 -1.150962795 -0.008323939 5950.096 0.9987528
## a[43] -0.448359373 0.3344753 -1.000863829  0.078441834 3859.469 0.9991632
## a[44] -0.334810108 0.3460984 -0.884345433  0.210653121 5593.936 0.9984521
## a[45]  0.574777272 0.3511018  0.031766472  1.148238037 5417.196 0.9985805
## a[46] -0.576075524 0.3556605 -1.126153981 -0.032015376 5536.224 0.9985362
## a[47]  2.049374166 0.5143725  1.252350850  2.926651373 4295.242 0.9989109
## a[48]  0.003767783 0.3507474 -0.553312832  0.547469647 4407.655 0.9985416
## a_bar  1.343550487 0.2564948  0.959469579  1.778955622 2586.798 1.0013769
## sigma  1.612791086 0.2114743  1.305960345  1.962856375 1667.671 1.0022255</code></pre>
</div>
<div id="inla" class="section level3">
<h3>1.1 INLA</h3>
<p>following example: <a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>Here I’m missing custom priors</strong> I’ll use a half cauchy prior for the <span class="math inline">\(\sigma\)</span> to constrain it to &gt;0 numbers, which is what the exponential does as well.</p>
<pre class="r"><code>library(brinla)
library(INLA)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.1.i &lt;- inla(surv ~ 1 + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials 
##    = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 1, &quot;, &quot; 
##    compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)))&quot; ) 
## Time used:
##     Pre = 1.61, Running = 0.187, Post = 0.209, Total = 2.01 
## Fixed effects:
##             mean    sd 0.025quant 0.5quant 0.975quant  mode kld
## (Intercept) 1.38 0.256       0.89    1.375      1.901 1.364   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.415 0.109      0.237    0.404      0.661 0.381
## 
## Expected number of effective parameters(stdev): 40.36(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.00
## Deviance Information Criterion (DIC, saturated) ....: 89.62
## Effective number of parameters .....................: 39.50
## 
## Watanabe-Akaike information criterion (WAIC) ...: 205.61
## Effective number of parameters .................: 22.72
## 
## Marginal log-Likelihood:  -140.19 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.1.i$summary.fixed</code></pre>
<pre><code>##                 mean        sd 0.025quant 0.5quant 0.975quant     mode          kld
## (Intercept) 1.380249 0.2562768  0.8904279 1.374944    1.90069 1.364469 1.902443e-05</code></pre>
<pre class="r"><code>m1.1.i$summary.hyperpar</code></pre>
<pre><code>##                         mean        sd 0.025quant  0.5quant 0.975quant      mode
## Precision for tank 0.4152469 0.1088217  0.2366839 0.4035007  0.6608823 0.3807222</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.1.i)</code></pre>
<pre><code>##                mean        sd   q0.025     q0.5   q0.975     mode
## SD for tank 1.59197 0.2096781 1.231084 1.573964 2.053666 1.539229</code></pre>
<p>it looks like the intercept mean and sd correspond to the <span class="math inline">\(\bar{\alpha}\)</span> mean and sd, and the SD for tank corresponds to the <span class="math inline">\(\sigma\)</span>. this makes sense, because the <span class="math inline">\(\bar{\alpha}\)</span> is the average baseline survival for all the tadpoles, which is what the intercept is. <strong>BUT I WOULD LOVE IF SOMEONE ELSE CONFIRMED THIS INTERPRETATION</strong>.</p>
</div>
</div>
<div id="varying-intercepts-predation" class="section level2">
<h2>1.2 varying intercepts + predation</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>[pred]</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(-0.5,1)</p>
<p>αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-1" class="section level3">
<h3>1.2 rethinking</h3>
<pre class="r"><code># pred
m1.2 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred, 
a[tank] ~ normal( a_bar , sigma ), 
bp ~ normal( -0.5 , 1 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE ) </code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis(m1.2)</code></pre>
<pre><code>##             mean        sd       5.5%     94.5%    n_eff    Rhat4
## bp    -2.4499979 0.2824940 -2.8976268 -1.993377 283.4918 1.016149
## a_bar  2.5500432 0.2238603  2.2001573  2.911799 327.2110 1.012061
## sigma  0.8244361 0.1458854  0.6058309  1.073014 668.8228 1.001003</code></pre>
</div>
<div id="inla-1" class="section level3">
<h3>1.2 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.2.i &lt;- inla(surv ~ 1 + pred + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= -0.5,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + f(tank, model = \&quot;iid\&quot;, hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, 
##    Ntrials = density, control.compute = list(config = T, &quot;, &quot; dic = TRUE, waic = TRUE), control.predictor = list(link = 1, 
##    &quot;, &quot; compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = -0.5, 
##    prec = 1, mean.intercept = 0, &quot;, &quot; prec.intercept = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.61, Running = 0.222, Post = 0.214, Total = 2.04 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.523 0.225      2.088    2.520      2.974  2.515   0
## predpred    -2.435 0.288     -2.997   -2.437     -1.863 -2.440   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 1.80 0.677       0.85     1.68       3.46 1.48
## 
## Expected number of effective parameters(stdev): 28.98(3.36)
## Number of equivalent replicates : 1.66 
## 
## Deviance Information Criterion (DIC) ...............: 204.42
## Deviance Information Criterion (DIC, saturated) ....: 79.99
## Effective number of parameters .....................: 29.04
## 
## Watanabe-Akaike information criterion (WAIC) ...: 201.45
## Effective number of parameters .................: 19.59
## 
## Marginal log-Likelihood:  -122.10 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.2.i$summary.fixed</code></pre>
<pre><code>##                  mean        sd 0.025quant  0.5quant 0.975quant      mode          kld
## (Intercept)  2.523164 0.2252490   2.087742  2.520414   2.974171  2.514827 2.391528e-07
## predpred    -2.435197 0.2876891  -2.996768 -2.437046  -1.863173 -2.440294 8.877976e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.2.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7815832 0.1385647 0.5384615 0.7711308 1.082806 0.7517625</code></pre>
</div>
</div>
<div id="varying-intercepts-size" class="section level2">
<h2>1.3 varying intercepts + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size</p>
<p><span class="math inline">\(\beta\)</span>∼ Normal(0 , 0.5 ) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-2" class="section level3">
<h3>1.3 rethinking</h3>
<pre class="r"><code>library(rethinking) 
data(reedfrogs)
d &lt;- reedfrogs

# size
m1.3 &lt;- ulam( alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + s[size_], 
a[tank] ~ normal( a_bar , sigma ), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>precis(m1.3,  depth=2)</code></pre>
<pre><code>##               mean        sd       5.5%       94.5%     n_eff     Rhat4
## a[1]   2.197506324 0.9499100  0.7485411  3.80673738  955.1148 0.9988392
## a[2]   3.088576570 1.1648834  1.3601772  5.11508955 1124.7453 1.0010026
## a[3]   1.072842621 0.7627631 -0.1270107  2.37139564  909.9865 1.0000421
## a[4]   3.077360404 1.1760779  1.3815738  5.09853641 1205.3237 1.0013206
## a[5]   1.938277128 0.9632119  0.5153681  3.57512682 1181.6902 1.0010529
## a[6]   1.943071095 0.9457098  0.5949319  3.51876119  977.1174 0.9998075
## a[7]   2.901904053 1.1550490  1.2056980  4.90390869 1418.1526 0.9999478
## a[8]   1.966351650 0.9545009  0.5916199  3.57619584 1051.3805 1.0017151
## a[9]  -0.102805223 0.7260615 -1.3077153  1.02538606  751.2821 0.9984545
## a[10]  2.173043553 0.9559318  0.7659900  3.76661519  989.5960 1.0002921
## a[11]  1.059473653 0.7752771 -0.1528344  2.28305271  766.8291 0.9991918
## a[12]  0.672504899 0.7717865 -0.5116655  1.90181033  775.7228 0.9986377
## a[13]  0.816014346 0.7697902 -0.3628321  2.06314447  914.0439 0.9999053
## a[14]  0.002774086 0.7099353 -1.1499594  1.14078496  724.6203 1.0017529
## a[15]  1.963570590 0.9716574  0.5557071  3.59934229 1021.9369 0.9988782
## a[16]  1.941628718 0.9417726  0.5408949  3.52251157 1277.2354 1.0000498
## a[17]  2.987401889 0.8620618  1.7098190  4.42616579 1055.3572 1.0001384
## a[18]  2.433939569 0.7681489  1.2876025  3.70754782  718.2256 1.0003840
## a[19]  2.113287600 0.7031183  1.0077118  3.30946797  641.9568 0.9994715
## a[20]  3.689821266 1.0219254  2.1808346  5.41731879  872.0969 1.0016034
## a[21]  2.200472652 0.7551383  1.0547903  3.48848831  969.7904 0.9996601
## a[22]  2.209998318 0.7807875  1.0439126  3.46469802  951.0248 0.9997247
## a[23]  2.184238559 0.7565816  0.9756890  3.46809279  866.2753 1.0009869
## a[24]  1.504369395 0.6569488  0.5114656  2.59460211  816.1928 1.0010388
## a[25] -0.921381810 0.5999282 -1.9170423 -0.01868743  504.9917 0.9991182
## a[26]  0.256460385 0.5607802 -0.6297994  1.17161888  432.1162 1.0003116
## a[27] -1.354235605 0.6155001 -2.3165856 -0.37303621  634.4698 0.9997999
## a[28] -0.384676316 0.5674255 -1.2971025  0.55019279  509.0148 0.9998421
## a[29] -0.048723238 0.5505116 -0.9081755  0.83564198  548.0334 1.0015135
## a[30]  1.250809851 0.6196817  0.2884836  2.26568100  989.5706 1.0008501
## a[31] -0.845390847 0.5591508 -1.7466410  0.02263527  537.5603 1.0029661
## a[32] -0.514107777 0.5390878 -1.3614275  0.35151142  572.2137 1.0023426
## a[33]  3.247577483 0.8758439  1.9134057  4.74600704  780.7928 0.9989084
## a[34]  2.783185179 0.7695015  1.5750363  4.05139714  630.2269 1.0011041
## a[35]  2.776319414 0.7363325  1.6304794  3.97064445  801.7078 0.9993082
## a[36]  2.139096229 0.6536301  1.1150388  3.19739687  577.7917 0.9992283
## a[37]  1.841677006 0.6403133  0.8800781  2.90735921  583.4924 1.0031073
## a[38]  3.714976593 1.0410542  2.2089310  5.46795509  989.6796 1.0007129
## a[39]  2.511981053 0.7532256  1.3788933  3.73153888  852.5990 1.0005838
## a[40]  2.148684070 0.6908480  1.0755096  3.36063857  799.0477 1.0000572
## a[41] -1.716005987 0.6152844 -2.6933704 -0.73264292  681.3258 0.9998437
## a[42] -0.484070058 0.5265303 -1.3379615  0.35324972  454.4420 0.9992206
## a[43] -0.362804554 0.5243562 -1.2268633  0.47216550  483.0082 0.9995159
## a[44] -0.252856920 0.5239716 -1.1014729  0.56704517  425.3779 1.0005629
## a[45]  0.363380897 0.5093978 -0.4261705  1.20820670  501.2853 1.0030155
## a[46] -0.784374393 0.5097681 -1.5685315  0.05365970  403.1380 1.0034001
## a[47]  1.876147635 0.6374260  0.8827423  2.93837433  804.2376 1.0011199
## a[48] -0.212545872 0.5039062 -1.0058177  0.59829042  443.9235 1.0017242
## s[1]   0.223479503 0.3841970 -0.4012152  0.80889210  283.2921 1.0038767
## s[2]  -0.090046429 0.3970223 -0.7177059  0.53521248  288.1970 0.9999603
## a_bar  1.282379089 0.4278177  0.5897153  1.97395232  314.6562 1.0008144
## sigma  1.608088157 0.2105686  1.2964823  1.96099656 1156.7820 1.0011899</code></pre>
</div>
<div id="inla-2" class="section level3">
<h3>1.3 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d1.i &lt;- d %&gt;% 
  mutate(tank = row_number(), 
         pred.no= na_if(if_else(pred==&quot;no&quot;, 1, 0), 0),
         pred.yes= na_if(if_else(pred==&quot;pred&quot;, 1, 0), 0),
         size.small= na_if(if_else(size==&quot;small&quot;, 1, 0), 0),
         size.big= na_if(if_else(size==&quot;big&quot;, 1, 0), 0)
         ) 

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.3.i &lt;- inla(surv ~ 1 + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= 0,
        prec= 1/(0.5^2), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.3.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + f(tank, model = \&quot;iid\&quot;, &quot;, &quot; hyper = hcprior), family = 
##    \&quot;binomial\&quot;, data = d1.i, Ntrials = density, &quot;, &quot; control.compute = list(config = T, dic = TRUE, waic = TRUE), &quot;, &quot; 
##    control.predictor = list(link = 1, compute = T), control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; 
##    control.fixed = list(mean = 0, prec = 1/(0.5^2), mean.intercept = 0, &quot;, &quot; prec.intercept = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 1.56, Running = 0.187, Post = 0.212, Total = 1.96 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  1.272 0.418      0.455    1.271      2.095  1.268   0
## size.small   0.222 0.400     -0.564    0.223      1.007  0.223   0
## size.big    -0.081 0.400     -0.866   -0.082      0.705 -0.082   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                     mean    sd 0.025quant 0.5quant 0.975quant  mode
## Precision for tank 0.421 0.111      0.239    0.408      0.672 0.385
## 
## Expected number of effective parameters(stdev): 40.42(1.26)
## Number of equivalent replicates : 1.19 
## 
## Deviance Information Criterion (DIC) ...............: 214.43
## Deviance Information Criterion (DIC, saturated) ....: 90.04
## Effective number of parameters .....................: 39.58
## 
## Watanabe-Akaike information criterion (WAIC) ...: 206.21
## Effective number of parameters .................: 22.90
## 
## Marginal log-Likelihood:  -142.21 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.3.i$summary.fixed</code></pre>
<pre><code>##                    mean        sd 0.025quant    0.5quant 0.975quant       mode          kld
## (Intercept)  1.27186665 0.4177740  0.4549238  1.27059867   2.095210  1.2681232 2.406671e-06
## size.small   0.22245891 0.4001376 -0.5640040  0.22269633   1.006898  0.2232035 1.123068e-07
## size.big    -0.08147702 0.4002655 -0.8664804 -0.08182627   0.704788 -0.0824901 6.786554e-07</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.3.i)</code></pre>
<pre><code>##                 mean        sd   q0.025    q0.5   q0.975     mode
## SD for tank 1.582503 0.2100774 1.220948 1.56446 2.045071 1.529635</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size" class="section level2">
<h2>1.4 varying intercepts + predation + size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>size + <span class="math inline">\(\gamma\)</span>size</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5)</p>
<p><span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]</p>
<p><span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank]</p>
<p>σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-3" class="section level3">
<h3>1.4 rethinking</h3>
<pre class="r"><code># pred + size 
m1.4 &lt;- ulam(
alist(
S ~ binomial( n , p ),
logit(p) &lt;- a[tank] + bp*pred + s[size_], 
a[tank] ~ normal( a_bar , sigma ),
bp ~ normal( -0.5 , 1 ),
s[size_] ~ normal( 0 , 0.5 ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )</code></pre>
<pre><code>## Warning: The largest R-hat is 1.06, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<pre class="r"><code>precis(m1.4, depth=2)</code></pre>
<pre><code>##              mean        sd        5.5%      94.5%    n_eff    Rhat4
## a[1]   2.43420608 0.7752339  1.23915631  3.7296375 391.4530 1.013130
## a[2]   2.83723027 0.7644984  1.67260075  4.0893148 498.2116 1.012676
## a[3]   1.70193757 0.6966978  0.62051549  2.8141146 403.7823 1.010826
## a[4]   2.85942265 0.7946288  1.61207288  4.1233062 400.8361 1.018311
## a[5]   2.29894961 0.7239815  1.14385305  3.4587095 423.3734 1.016089
## a[6]   2.29338059 0.7484419  1.14463938  3.5185422 436.5487 1.016629
## a[7]   2.75028370 0.8379842  1.44572848  4.1018005 447.3293 1.014019
## a[8]   2.29900429 0.7588296  1.08928620  3.5268769 432.3685 1.013621
## a[9]   2.21784175 0.6542278  1.13333476  3.2307630 307.1139 1.016936
## a[10]  3.50224571 0.7034358  2.37362203  4.6625021 356.3316 1.016296
## a[11]  2.96583305 0.6370700  1.95698602  3.9796937 286.9350 1.020772
## a[12]  2.70827558 0.6560545  1.68448280  3.7662471 275.4402 1.020747
## a[13]  2.73180499 0.6528065  1.73334051  3.7954161 310.6072 1.021792
## a[14]  2.20761726 0.6441541  1.15505411  3.2369645 272.3970 1.024633
## a[15]  3.28320150 0.6959429  2.23104748  4.4208733 291.4004 1.024573
## a[16]  3.27932159 0.6950201  2.17000366  4.3569554 299.1415 1.020174
## a[17]  2.84512531 0.7073135  1.77051173  4.0196124 394.4861 1.017956
## a[18]  2.54415118 0.6358731  1.58050162  3.5840399 366.3694 1.017095
## a[19]  2.27001282 0.6050631  1.31957079  3.2753504 374.6048 1.012951
## a[20]  3.17505689 0.7186157  2.08236200  4.3777515 352.1157 1.017380
## a[21]  2.32538170 0.6559008  1.29302194  3.4092178 442.9914 1.019440
## a[22]  2.32989416 0.6496759  1.33309654  3.3933764 377.2906 1.021194
## a[23]  2.34458399 0.6717665  1.33635186  3.4431893 363.0210 1.017602
## a[24]  1.77884583 0.6024776  0.80864743  2.7302582 372.3380 1.021625
## a[25]  1.61162511 0.5924106  0.67505324  2.5453744 239.6147 1.025472
## a[26]  2.56910478 0.5719939  1.67938055  3.4656988 228.8687 1.023566
## a[27]  1.29198947 0.6207120  0.34530646  2.3084431 252.0324 1.021521
## a[28]  2.03374973 0.5744595  1.12521000  2.9683059 219.3018 1.030963
## a[29]  2.21557359 0.5688729  1.30952812  3.1230437 216.9426 1.033852
## a[30]  3.19042173 0.5831936  2.26521854  4.1263320 211.1636 1.037288
## a[31]  1.58400316 0.5704081  0.69434848  2.4919988 218.4541 1.030041
## a[32]  1.83765391 0.5608689  0.95621775  2.7623027 207.1040 1.032620
## a[33]  3.00796663 0.6346627  2.04825252  4.0608777 273.4684 1.026023
## a[34]  2.73490724 0.6243817  1.76202858  3.7294015 381.2433 1.015736
## a[35]  2.74437661 0.6238664  1.80815166  3.8188085 295.0745 1.023396
## a[36]  2.27031840 0.5966624  1.34601388  3.2546747 341.8144 1.016383
## a[37]  2.02146318 0.5953977  1.05952368  2.9910101 333.2109 1.016829
## a[38]  3.14808716 0.7181842  2.03393848  4.3085190 401.1917 1.017102
## a[39]  2.51002212 0.6343971  1.52087530  3.5126089 294.8324 1.025425
## a[40]  2.24936895 0.6189904  1.27116204  3.2473467 368.6675 1.015443
## a[41]  0.97329298 0.6106500 -0.01260277  1.9353081 239.3791 1.023538
## a[42]  1.94917781 0.5456372  1.08783586  2.7919930 195.5430 1.028597
## a[43]  2.05438995 0.5479737  1.15964868  2.9314894 202.0918 1.021962
## a[44]  2.15299842 0.5316928  1.31093176  2.9981549 202.7469 1.029341
## a[45]  2.59147825 0.5475667  1.73809627  3.4769002 189.6223 1.036629
## a[46]  1.60179818 0.5498207  0.76971294  2.4990597 203.3914 1.031783
## a[47]  3.68426855 0.6008893  2.77921505  4.6462604 220.4859 1.032045
## a[48]  2.09820905 0.5548793  1.19908287  3.0007276 186.7242 1.036701
## bp    -2.42879341 0.2906159 -2.89215014 -1.9883698 526.3187 1.006167
## s[1]   0.33641443 0.3728226 -0.25042388  0.9447840 168.4214 1.045141
## s[2]  -0.09053568 0.3678471 -0.68591368  0.5048887 178.7630 1.036544
## a_bar  2.40117815 0.4010629  1.76477316  3.0616616 127.1062 1.057349
## sigma  0.78515972 0.1513028  0.56541671  1.0430321 516.0548 1.006137</code></pre>
</div>
<div id="inla-3" class="section level3">
<h3>1.4 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)

# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.4.i &lt;- inla(surv ~ 1 + pred + size.small+ size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.4.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + pred + size.small + size.big + f(tank, &quot;, &quot; model = \&quot;iid\&quot;, hyper = hcprior), family = 
##    \&quot;binomial\&quot;, data = d1.i, &quot;, &quot; Ntrials = density, control.compute = list(config = T, dic = TRUE, &quot;, &quot; waic = TRUE), 
##    control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, 
##    &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec = list(pred = 1, size.small = 
##    1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.66, Running = 0.186, Post = 0.231, Total = 2.07 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)  2.161 0.666      0.854    2.161      3.469  2.160   0
## predpred    -2.628 0.290     -3.204   -2.626     -2.062 -2.622   0
## size.small   0.729 0.656     -0.559    0.729      2.016  0.729   0
## size.big     0.231 0.656     -1.056    0.231      1.517  0.231   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean    sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.13 0.907      0.941     1.95       4.40 1.66
## 
## Expected number of effective parameters(stdev): 27.66(3.70)
## Number of equivalent replicates : 1.74 
## 
## Deviance Information Criterion (DIC) ...............: 204.82
## Deviance Information Criterion (DIC, saturated) ....: 80.44
## Effective number of parameters .....................: 27.99
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.79
## Effective number of parameters .................: 19.63
## 
## Marginal log-Likelihood:  -123.31 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.4.i$summary.fixed</code></pre>
<pre><code>##                   mean        sd 0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)  2.1613253 0.6663035  0.8539924  2.1610031   3.469238  2.1604153 1.094653e-07
## predpred    -2.6276156 0.2895818 -3.2042666 -2.6256993  -2.062158 -2.6218552 3.292691e-06
## size.small   0.7293355 0.6560320 -0.5587394  0.7293248   2.016310  0.7293579 4.990355e-07
## size.big     0.2308486 0.6557372 -1.0562819  0.2307203   1.517498  0.2305195 1.043209e-08</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.4.i)</code></pre>
<pre><code>##                  mean        sd    q0.025      q0.5   q0.975      mode
## SD for tank 0.7260825 0.1402651 0.4772367 0.7163201 1.028863 0.6986102</code></pre>
</div>
</div>
<div id="varying-intercepts-predation-size-predationsize" class="section level2">
<h2>1.5 varying intercepts + predation + size + predation*size</h2>
<p>Si ∼ Binomial(Ni, pi)</p>
<p><strong>this formula’s wrong</strong></p>
<p>logit(pi) = αtank[i] + <span class="math inline">\(\beta\)</span>predation + <span class="math inline">\(\gamma\)</span>size + <span class="math inline">\(\eta\)</span>size*predation</p>
<p><span class="math inline">\(\gamma\)</span> ∼ Normal(0 , 0.5) <span class="math inline">\(\beta\)</span> ∼ Normal(-0.5,1) αj ∼ Normal(<span class="math inline">\(\bar{\alpha}\)</span>, σ) [adaptive prior]<br />
<span class="math inline">\(\bar{\alpha}\)</span> ∼ Normal(0, 1.5) [prior for average tank] σ ∼ Exponential(1) [prior for standard deviation of tanks]</p>
<div id="rethinking-4" class="section level3">
<h3>1.5 rethinking</h3>
<pre class="r"><code># pred + size + interaction 
m1.5 &lt;- ulam(
alist(
S ~ binomial( n , p),
logit(p) &lt;- a_bar + z[tank]*sigma + s[size_]+ bp[size_]*pred , 
z[tank] ~ normal( 0, 1), 
bp[size_] ~ normal(-0.5,1), 
s[size_] ~ normal( 0 , 0.5 ), 
a_bar ~ normal( 0 , 1.5 ), 
sigma ~ exponential( 1 )
), data=dat , chains=4 , cores=4 , log_lik=TRUE )



precis(m1.5, depth=2)</code></pre>
<pre><code>##              mean        sd       5.5%       94.5%     n_eff     Rhat4
## z[1]  -0.05403372 0.8143521 -1.3474856  1.28246932 2397.1762 0.9990619
## z[2]   0.49934409 0.8668694 -0.8415782  1.89422919 2226.1010 0.9993135
## z[3]  -0.95044341 0.7872243 -2.2113549  0.32215574 2013.9198 1.0007348
## z[4]   0.51654347 0.9146607 -0.9120633  2.01435075 2313.3375 0.9995453
## z[5]  -0.05119246 0.8237869 -1.2983263  1.27405313 2243.0221 1.0013714
## z[6]  -0.02792382 0.8550900 -1.3765524  1.36911820 2337.9888 0.9990667
## z[7]   0.50558350 0.8677930 -0.8213988  1.90776451 2004.3169 0.9998722
## z[8]  -0.03245645 0.8646931 -1.3752636  1.38251496 2531.6162 0.9987457
## z[9]  -0.09835835 0.6956276 -1.1847207  1.01890093 2348.5399 1.0004040
## z[10]  1.52377458 0.7144146  0.4084932  2.68840061 2055.2753 0.9995028
## z[11]  0.82813920 0.7072754 -0.2708850  1.95029524 2315.4550 0.9995368
## z[12]  0.54358540 0.6949952 -0.5532284  1.65374597 2176.0679 1.0003747
## z[13]  0.24349144 0.7202254 -0.9063028  1.43738973 1927.3799 1.0000960
## z[14] -0.43220430 0.6989120 -1.5136789  0.71326998 1807.3174 0.9987307
## z[15]  0.94285993 0.7304137 -0.2086810  2.08421011 1698.5980 0.9994804
## z[16]  0.93259588 0.7350724 -0.2117084  2.08354253 2159.6698 0.9998845
## z[17]  0.45739161 0.7789112 -0.7685739  1.73743796 2195.7220 0.9986299
## z[18]  0.04945514 0.7529392 -1.1332893  1.26182793 1646.8793 1.0009280
## z[19] -0.27468174 0.7279373 -1.4327533  0.91789555 2200.7684 0.9993516
## z[20]  0.86253618 0.7872917 -0.3703862  2.17160806 1931.4559 0.9983093
## z[21]  0.08987722 0.7399239 -1.0552880  1.27653614 2534.8881 0.9990153
## z[22]  0.09200037 0.7236505 -1.0292950  1.24595242 1831.6275 1.0009007
## z[23]  0.07556259 0.7532290 -1.1165097  1.30259022 2094.4427 1.0003864
## z[24] -0.58194037 0.6792555 -1.6468430  0.51727393 1833.3157 0.9998284
## z[25] -0.84262896 0.5904714 -1.8130233  0.09717988 1831.3960 1.0006496
## z[26]  0.40736033 0.5871186 -0.4777117  1.35633837 1803.1552 0.9993594
## z[27] -1.28239825 0.6081899 -2.2582096 -0.29701505 2047.3035 1.0000524
## z[28] -0.28217540 0.5733943 -1.1739755  0.62137881 1698.5938 1.0007227
## z[29] -0.49217970 0.5516445 -1.3462282  0.37446178 1778.0960 1.0001867
## z[30]  0.83785362 0.6115428 -0.1174848  1.83553619 1823.1140 1.0021259
## z[31] -1.32733542 0.5862080 -2.2541431 -0.42370748 1499.1485 0.9998025
## z[32] -0.99964017 0.5610670 -1.8855081 -0.12531880 1414.8832 1.0003420
## z[33]  0.68399711 0.7508491 -0.5015763  1.85342453 1814.0867 0.9996960
## z[34]  0.31952127 0.7132528 -0.7749966  1.51593642 1948.7179 1.0014917
## z[35]  0.30247743 0.7288848 -0.7898484  1.48318215 2378.0658 0.9998662
## z[36] -0.30122471 0.6668925 -1.3500003  0.77646348 1963.5225 0.9989617
## z[37] -0.29220334 0.6447405 -1.2890368  0.76176833 1371.4157 1.0007812
## z[38]  1.06950630 0.7921516 -0.1832675  2.30842917 1754.3410 1.0017334
## z[39]  0.32552226 0.7146474 -0.7696986  1.51166795 2091.1243 0.9989049
## z[40]  0.01002381 0.6895082 -1.0412241  1.12216313 1994.0206 1.0009685
## z[41] -1.68360002 0.5764028 -2.6200845 -0.77299273 1612.7857 0.9986313
## z[42] -0.40116120 0.5278093 -1.2512287  0.44001172 1393.1903 1.0034557
## z[43] -0.27785037 0.5229054 -1.1171785  0.52101212 1467.3750 1.0013085
## z[44] -0.11220223 0.5125384 -0.8889420  0.74365683 1654.1812 1.0022780
## z[45] -0.02250468 0.5302779 -0.8494119  0.83623143 1291.7603 1.0003084
## z[46] -1.33768909 0.5354407 -2.1874436 -0.50839079 1351.8861 1.0005032
## z[47]  1.44601085 0.6084491  0.5084444  2.45914874 1788.7036 1.0019825
## z[48] -0.68111163 0.5066750 -1.4722855  0.10895210 1498.2982 1.0005353
## bp[1] -1.87468204 0.3747698 -2.4867724 -1.29077977  902.5056 1.0032632
## bp[2] -2.76574879 0.3889136 -3.3722087 -2.15747625  855.5639 1.0021128
## s[1]   0.09997377 0.3936657 -0.5442883  0.72570120 1199.9562 1.0025907
## s[2]   0.13220651 0.3830576 -0.4720784  0.74621247 1257.4945 1.0028241
## a_bar  2.33697960 0.4076075  1.7033160  2.98521775 1034.3903 1.0021848
## sigma  0.74957619 0.1477115  0.5328813  1.00016808  725.8507 1.0017633</code></pre>
<p>I coded the interaction model using a non-centered parameterization. The interaction itself is done by creating a bp parameter for each size value. In this way, the effect of pred depends upon size.</p>
</div>
<div id="inla-4" class="section level3">
<h3>1.5 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)


# number of trials is d1.i$density

halfcauchy = &quot;expression:
              lambda = 0.022;
              precision = exp(log_precision);
              logdens = -1.5*log_precision-log(pi*lambda)-log(1+1/(precision*lambda^2));
              log_jacobian = log_precision;
              return(logdens+log_jacobian);&quot;

hcprior = list(prec = list(prior = halfcauchy))
  
m1.5.i &lt;- inla(surv ~ 1 + size.small+ size.big + pred*size.small + pred*size.big + f(tank, model=&quot;iid&quot;, hyper = hcprior), data= d1.i, family = &quot;binomial&quot;, 
              Ntrials = density, 
              control.fixed = list(
        mean= list(pred= -0.5,size.small= 0, size.big= 0 ),
        prec= list(pred= 1,size.small= 1, size.big= 1 ), 
        mean.intercept= 0, 
        prec.intercept= 1/(1.5^2)),
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, dic=TRUE, waic= TRUE))
summary(m1.5.i )</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = surv ~ 1 + size.small + size.big + pred * size.small + &quot;, &quot; pred * size.big + f(tank, model = \&quot;iid\&quot;, 
##    hyper = hcprior), &quot;, &quot; family = \&quot;binomial\&quot;, data = d1.i, Ntrials = density, control.compute = list(config = T, &quot;, &quot; 
##    dic = TRUE, waic = TRUE), control.predictor = list(link = 1, &quot;, &quot; compute = T), control.family = list(control.link = 
##    list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean = list(pred = -0.5, size.small = 0, &quot;, &quot; size.big = 0), prec = 
##    list(pred = 1, size.small = 1, &quot;, &quot; size.big = 1), mean.intercept = 0, prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.81, Running = 0.191, Post = 0.24, Total = 2.24 
## Fixed effects:
##                       mean     sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept)          2.156  0.665      0.851    2.156      3.462  2.155   0
## size.small           0.406  0.675     -0.919    0.405      1.730  0.405   0
## size.big             0.551  0.676     -0.775    0.551      1.877  0.551   0
## predpred            -1.754 18.258    -37.601   -1.754     34.064 -1.754   0
## size.small:predpred -0.341 18.260    -36.191   -0.342     35.479 -0.341   0
## size.big:predpred   -1.411 18.260    -37.261   -1.412     34.409 -1.411   0
## 
## Random effects:
##   Name     Model
##     tank IID model
## 
## Model hyperparameters:
##                    mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for tank 2.37 1.07       1.01     2.15       5.07 1.80
## 
## Expected number of effective parameters(stdev): 27.12(3.80)
## Number of equivalent replicates : 1.77 
## 
## Deviance Information Criterion (DIC) ...............: 203.91
## Deviance Information Criterion (DIC, saturated) ....: 79.54
## Effective number of parameters .....................: 27.31
## 
## Watanabe-Akaike information criterion (WAIC) ...: 202.42
## Effective number of parameters .................: 19.54
## 
## Marginal log-Likelihood:  -126.05 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>m1.5.i$summary.fixed</code></pre>
<pre><code>##                           mean         sd  0.025quant   0.5quant 0.975quant       mode          kld
## (Intercept)          2.1559548  0.6651605   0.8508812  2.1556250   3.461695  2.1550209 5.782096e-07
## size.small           0.4056285  0.6747850  -0.9188799  0.4054782   1.729780  0.4052337 3.563633e-07
## size.big             0.5511408  0.6756415  -0.7748341  0.5509153   1.877178  0.5505204 5.496338e-07
## predpred            -1.7535485 18.2583848 -37.6008935 -1.7540628  34.063846 -1.7535481 6.603266e-10
## size.small:predpred -0.3409941 18.2597045 -36.1909263 -0.3415085  35.478979 -0.3409938 9.191619e-10
## size.big:predpred   -1.4114467 18.2597582 -37.2614792 -1.4119606  34.408614 -1.4114449 1.461611e-09</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m1.5.i)</code></pre>
<pre><code>##                  mean        sd    q0.025     q0.5    q0.975      mode
## SD for tank 0.6920361 0.1392399 0.4444564 0.682502 0.9922881 0.6654361</code></pre>
</div>
</div>
<div id="compare-using-waic" class="section level2">
<h2>compare using WAIC</h2>
<div id="compare-rethinking" class="section level4">
<h4>compare rethinking</h4>
<pre class="r"><code>rethinking::compare( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 )</code></pre>
<pre><code>##          WAIC       SE    dWAIC      dSE    pWAIC     weight
## m1.2 198.1337 9.033778 0.000000       NA 18.84908 0.46233114
## m1.4 199.7722 8.711599 1.638541 2.045845 19.07846 0.20377385
## m1.5 200.2115 9.145801 2.077817 3.467222 19.15636 0.16359156
## m1.1 201.2157 7.511427 3.082043 5.841137 21.39367 0.09901386
## m1.3 201.8728 7.229095 3.739062 5.718157 21.58826 0.07128959</code></pre>
<p>These models are really very similar in expected out-of-sample accuracy. The tank variation is huge. But take a look at the posterior distributions for predation and size. You’ll see that predation does seem to matter, as you’d expect. Size matters a lot less. So while predation doesn’t explain much of the total variation, there is plenty of evidence that it is a real effect. Remember: We don’t select a model using WAIC (or LOO). A predictor can make little difference in total accuracy but still be a real causal effect.</p>
</div>
<div id="compare-inla" class="section level4">
<h4>compare inla</h4>
<pre class="r"><code>inla.models.8.1 &lt;- list(m1.1.i, m1.2.i,m1.3.i, m1.4.i, m1.5.i )

extract.waic &lt;- function (x){
  x[[&quot;waic&quot;]][[&quot;waic&quot;]]
}

waic.8.1 &lt;- bind_cols(model = c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), waic = sapply(inla.models.8.1 ,extract.waic))

waic.8.1</code></pre>
<pre><code>## # A tibble: 5 x 2
##   model   waic
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 m1.1.i  206.
## 2 m1.2.i  201.
## 3 m1.3.i  206.
## 4 m1.4.i  203.
## 5 m1.5.i  202.</code></pre>
<p>Let’s look at all the sigma posterior distributions: The two models that omit predation, m1.1 and m1.3, have larger values of sigma. This is because predation explains some of the variation among tanks. So when you add it to the model, the variation in the tank intercepts gets smaller.</p>
<pre class="r"><code>sigma.8.1 &lt;- bind_cols( model= c(&quot;m1.1.i&quot;,&quot;m1.2.i&quot;,&quot;m1.3.i&quot;, &quot;m1.4.i&quot;, &quot;m1.5.i&quot; ), do.call(rbind.data.frame, lapply(inla.models.8.1 ,bri.hyperpar.summary)))


sigma.8.1</code></pre>
<pre><code>##               model      mean        sd    q0.025      q0.5    q0.975      mode
## SD for tank  m1.1.i 1.5919697 0.2096781 1.2310838 1.5739639 2.0536663 1.5392289
## SD for tank1 m1.2.i 0.7815832 0.1385647 0.5384615 0.7711308 1.0828062 0.7517625
## SD for tank2 m1.3.i 1.5825033 0.2100774 1.2209479 1.5644595 2.0450713 1.5296348
## SD for tank3 m1.4.i 0.7260825 0.1402651 0.4772367 0.7163201 1.0288635 0.6986102
## SD for tank4 m1.5.i 0.6920361 0.1392399 0.4444564 0.6825020 0.9922881 0.6654361</code></pre>
<pre class="r"><code>sigma.8.1.plot &lt;-  ggplot(data= sigma.8.1, aes(y=model, x=mean, label=model)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=q0.025, xmax=q0.975), height=.3) +
    coord_fixed(ratio=.3) +
    theme_bw()

sigma.8.1.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/sigma.8.1%20plot-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="section-1" class="section level1">
<h1>2.</h1>
<p><strong>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</strong></p>
<p><strong>(1) district: ID number of administrative district each woman resided in</strong></p>
<p><strong>(2) use.contraception: An indicator (0/1) of whether the woman was using contraception</strong></p>
<p><strong>Focus on predicting use.contraception, clustered by district_id. Fit both:</strong></p>
<p><strong>1) a traditional fixed-effects model that uses an index variable for district</strong></p>
<p><strong>2) a multilevel model with varying intercepts for district.</strong></p>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences?**</p>
<pre class="r"><code>library(rethinking)
data(bangladesh)
d &lt;- bangladesh</code></pre>
<p>The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable:</p>
<pre class="r"><code>sort(unique(d$district))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43
## [44] 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61</code></pre>
<p>District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:</p>
<pre class="r"><code>d$district_id &lt;- as.integer(as.factor(d$district)) 
sort(unique(d$district_id))</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43
## [44] 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</code></pre>
<p>Now there are 60 values, contiguous integers 1 to 60.</p>
<div id="traditional-fixed-effects-model-that-uses-an-index-variable-for-district" class="section level2">
<h2>2.1 traditional fixed-effects model that uses an index variable for district</h2>
<div id="rethinking-5" class="section level3">
<h3>2.1 rethinking</h3>
<pre class="r"><code>dat_list &lt;- list(
C = d$use.contraception, 
did = d$district_id
)

m2.1 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( 0 , 1.5 )
) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.1, depth = 2)</code></pre>
<pre><code>##                mean        sd        5.5%       94.5%    n_eff     Rhat4
## a[1]  -1.0574210198 0.2032203 -1.38116441 -0.74319652 4940.262 0.9987527
## a[2]  -0.5883688356 0.4481568 -1.31532200  0.12055764 4613.215 0.9983967
## a[3]   1.2019354428 1.1561479 -0.58469872  3.09133661 5841.845 0.9986671
## a[4]  -0.0104704525 0.3420895 -0.54059738  0.53853209 4975.114 0.9999590
## a[5]  -0.5676118558 0.3190185 -1.08485676 -0.07094106 4904.701 0.9988733
## a[6]  -0.8631977113 0.2694420 -1.30995620 -0.43213754 4476.029 0.9983473
## a[7]  -0.9075085183 0.5306981 -1.78682092 -0.10401548 5385.656 0.9990495
## a[8]  -0.4761588348 0.3378921 -1.01291541  0.04784864 4663.603 0.9987355
## a[9]  -0.7982374691 0.4307301 -1.46836578 -0.14273694 4437.402 0.9985569
## a[10] -1.9458156755 0.7304662 -3.17003636 -0.89464941 3896.143 0.9987973
## a[11] -2.9707923444 0.8239677 -4.36522747 -1.77649631 3372.270 0.9985145
## a[12] -0.6216576936 0.3748387 -1.22276087 -0.05818588 4676.846 0.9985326
## a[13] -0.3225762365 0.3945695 -0.93388674  0.31075267 3951.442 0.9997865
## a[14]  0.5162236342 0.1984458  0.20568165  0.82748723 5784.561 0.9993143
## a[15] -0.5391553836 0.4191350 -1.20495536  0.11207094 4969.996 0.9987389
## a[16]  0.1962122193 0.4711940 -0.56171556  0.94598441 5101.559 0.9988388
## a[17] -0.8505392569 0.4278789 -1.57410756 -0.19483164 3751.108 0.9984864
## a[18] -0.6530581755 0.3027784 -1.13298539 -0.19450129 4495.072 0.9992128
## a[19] -0.4589661030 0.3919585 -1.09747509  0.17372542 4166.266 0.9985628
## a[20] -0.3906649803 0.5414898 -1.25330358  0.43840930 3423.607 0.9991982
## a[21] -0.4202249990 0.4593467 -1.16892423  0.32166951 5809.203 0.9988763
## a[22] -1.2905146120 0.5134852 -2.14475694 -0.51731853 4261.444 0.9986089
## a[23] -0.9167076267 0.5580934 -1.82281647 -0.05562923 5570.857 0.9990691
## a[24] -2.0472460298 0.7773090 -3.37133606 -0.87333546 4323.796 0.9982002
## a[25] -0.2106443904 0.2363377 -0.58554230  0.14804398 5198.440 0.9985576
## a[26] -0.4323147351 0.5261736 -1.27453242  0.39848696 4427.443 0.9992514
## a[27] -1.4539387500 0.3824115 -2.09970697 -0.86327944 4174.245 0.9991223
## a[28] -1.0996417173 0.3200466 -1.62391163 -0.60292123 4865.389 0.9988564
## a[29] -0.9092851311 0.3760813 -1.52588783 -0.31924478 4983.410 0.9985468
## a[30] -0.0346920260 0.2544857 -0.43646605  0.38158194 6514.971 0.9982629
## a[31] -0.1804721488 0.3561842 -0.75406443  0.39880388 4617.519 0.9999847
## a[32] -1.2603540421 0.4772292 -2.03816802 -0.54780384 4690.835 0.9991583
## a[33] -0.2779964364 0.5235575 -1.14017513  0.56091283 5078.489 0.9983407
## a[34]  0.6296346680 0.3422802  0.09718881  1.18275490 4526.607 0.9983728
## a[35]  0.0032604135 0.2899443 -0.45276643  0.46828466 5182.599 0.9988738
## a[36] -0.5830507983 0.4905251 -1.39029327  0.19831014 4585.489 0.9990756
## a[37]  0.1473015700 0.5309032 -0.67974087  0.99665524 4763.795 0.9984276
## a[38] -0.8491113300 0.5556402 -1.76990662  0.04353355 5049.661 0.9981089
## a[39]  0.0003444822 0.3859412 -0.60084387  0.60123813 5420.887 0.9984759
## a[40] -0.1439590051 0.3109660 -0.63799678  0.35172454 5030.846 0.9987404
## a[41] -0.0044770190 0.3803634 -0.60508117  0.60473343 4394.453 0.9985861
## a[42]  0.1665852881 0.5819842 -0.72972811  1.13148462 4154.005 0.9991229
## a[43]  0.1232415091 0.2945974 -0.34242273  0.59141721 4619.440 0.9986498
## a[44] -1.1862192027 0.4298010 -1.90990254 -0.52013626 3421.631 0.9989474
## a[45] -0.6787303123 0.3329820 -1.20766749 -0.15922637 4742.192 0.9995021
## a[46]  0.0893602547 0.2201813 -0.26722891  0.42742276 5303.838 0.9983987
## a[47] -0.1199059814 0.5185701 -0.94257865  0.74578341 4181.434 0.9987637
## a[48]  0.0929782065 0.3152451 -0.41107705  0.59772263 4615.773 0.9988748
## a[49] -1.7209514934 1.0023221 -3.42021951 -0.20016287 4362.830 0.9992119
## a[50] -0.1007863209 0.4456581 -0.80250210  0.60395258 4775.530 0.9988338
## a[51] -0.1602867655 0.3360753 -0.72062674  0.36592660 4917.384 0.9993494
## a[52] -0.2289947664 0.2666204 -0.66465427  0.18570167 3925.794 0.9984009
## a[53] -0.3043230933 0.4571530 -1.02082240  0.43050483 4942.503 0.9988633
## a[54] -1.2271822218 0.8324373 -2.61908484  0.04997479 4476.383 0.9986932
## a[55]  0.3129155494 0.2761646 -0.12711797  0.73967591 5238.225 0.9990594
## a[56] -1.3805436508 0.4563072 -2.13829240 -0.66774865 3790.820 0.9992000
## a[57] -0.1823575640 0.3461849 -0.75087119  0.36953881 5617.061 0.9994905
## a[58] -1.7196443569 0.7603990 -2.95260475 -0.56976105 3616.211 0.9987724
## a[59] -1.2111966282 0.3915285 -1.83486243 -0.60504103 4843.496 0.9986928
## a[60] -1.2614208764 0.3690788 -1.87099492 -0.68496394 5775.217 0.9987821</code></pre>
</div>
<div id="inla-5" class="section level3">
<h3>2.1 inla</h3>
<pre class="r"><code>library(brinla)
library(INLA)
library(tidyverse)

d2.i &lt;- d %&gt;% 
  mutate(did= paste(&quot;d&quot;, as.integer(d$district_id), sep= &quot;.&quot;), 
         d.value= 1
         ) %&gt;% 
  spread(did, d.value)

#use this to quickly make a list of the index vbles to include in the model 
did_formula &lt;- paste(&quot;d&quot;, 1:60, sep=&quot;.&quot;, collapse = &quot;+&quot;)


m2.1.i &lt;- inla(use.contraception ~ d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9+d.10+d.11+d.12+d.13+d.14+d.15+d.16+d.17+d.18+d.19+d.20+d.21+d.22+d.23+d.24+d.25+d.26+d.27+d.28+d.29+d.30+d.31+d.32+d.33+d.34+d.35+d.36+d.37+d.38+d.39+d.40+d.41+d.42+d.43+d.44+d.45+d.46+d.47+d.48+d.49+d.50+d.51+d.52+d.53+d.54+d.55+d.56+d.57+d.58+d.59+d.60, data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean=  0 ,
        prec= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.1.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ d.1 + d.2 + d.3 + d.4 + d.5 + &quot;, &quot; d.6 + d.7 + d.8 + d.9 + d.10 + d.11 + d.12 + 
##    d.13 + d.14 + &quot;, &quot; d.15 + d.16 + d.17 + d.18 + d.19 + d.20 + d.21 + d.22 + d.23 + &quot;, &quot; d.24 + d.25 + d.26 + d.27 + d.28 
##    + d.29 + d.30 + d.31 + d.32 + &quot;, &quot; d.33 + d.34 + d.35 + d.36 + d.37 + d.38 + d.39 + d.40 + d.41 + &quot;, &quot; d.42 + d.43 + 
##    d.44 + d.45 + d.46 + d.47 + d.48 + d.49 + d.50 + &quot;, &quot; d.51 + d.52 + d.53 + d.54 + d.55 + d.56 + d.57 + d.58 + d.59 + &quot;, 
##    &quot; d.60, family = \&quot;binomial\&quot;, data = d2.i, Ntrials = 1, control.compute = list(config = T, &quot;, &quot; waic = TRUE), 
##    control.predictor = list(link = 1, compute = T), &quot;, &quot; control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, 
##    &quot; control.fixed = list(mean = 0, prec = 1/(1.5^2)))&quot;) 
## Time used:
##     Pre = 5.02, Running = 0.512, Post = 0.805, Total = 6.34 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.634 0.204     -1.035   -0.634     -0.234 -0.634   0
## d.1         -0.427 0.290     -1.001   -0.425      0.136 -0.421   0
## d.2          0.009 0.484     -0.967    0.017      0.936  0.034   0
## d.3          1.421 1.074     -0.583    1.382      3.640  1.306   0
## d.4          0.599 0.404     -0.194    0.600      1.391  0.600   0
## d.5          0.048 0.380     -0.709    0.052      0.782  0.060   0
## d.6         -0.247 0.333     -0.911   -0.244      0.397 -0.237   0
## d.7         -0.294 0.525     -1.367   -0.279      0.695 -0.250   0
## d.8          0.128 0.384     -0.636    0.131      0.871  0.138   0
## d.9         -0.183 0.471     -1.136   -0.173      0.713 -0.153   0
## d.10        -1.349 0.750     -2.950   -1.302     -0.002 -1.208   0
## d.11        -2.343 0.845     -4.166   -2.281     -0.851 -2.155   0
## d.12        -0.012 0.424     -0.861   -0.006      0.803  0.006   0
## d.13         0.274 0.443     -0.606    0.278      1.132  0.286   0
## d.14         1.138 0.276      0.599    1.137      1.681  1.136   0
## d.15         0.064 0.465     -0.869    0.071      0.957  0.085   0
## d.16         0.767 0.469     -0.148    0.766      1.692  0.762   0
## d.17        -0.239 0.467     -1.187   -0.229      0.649 -0.208   0
## d.18        -0.030 0.360     -0.747   -0.027      0.666 -0.019   0
## d.19         0.150 0.434     -0.716    0.155      0.987  0.165   0
## d.20         0.200 0.530     -0.863    0.208      1.220  0.223   0
## d.21         0.161 0.497     -0.834    0.168      1.117  0.182   0
## d.22        -0.673 0.542     -1.796   -0.652      0.332 -0.611   0
## d.23        -0.337 0.566     -1.501   -0.318      0.724 -0.282   0
## d.24        -1.413 0.743     -3.001   -1.367     -0.079 -1.272   0
## d.25         0.413 0.314     -0.205    0.413      1.027  0.415   0
## d.26         0.139 0.563     -0.994    0.149      1.216  0.169   0
## d.27        -0.826 0.418     -1.679   -0.814     -0.037 -0.791   0
## d.28        -0.476 0.377     -1.235   -0.470      0.244 -0.456   0
## d.29        -0.291 0.424     -1.148   -0.283      0.517 -0.266   0
## d.30         0.585 0.321     -0.046    0.585      1.214  0.585   0
## d.31         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.32        -0.641 0.502     -1.676   -0.624      0.296 -0.590   0
## d.33         0.304 0.541     -0.775    0.310      1.348  0.322   0
## d.34         1.222 0.394      0.461    1.217      2.008  1.208   0
## d.35         0.612 0.345     -0.066    0.612      1.289  0.612   0
## d.36         0.020 0.514     -1.018    0.030      1.002  0.049   0
## d.37         0.693 0.551     -0.384    0.692      1.777  0.690   0
## d.38        -0.252 0.574     -1.428   -0.235      0.825 -0.200   0
## d.39         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.40         0.467 0.364     -0.251    0.468      1.178  0.470   0
## d.41         0.594 0.425     -0.241    0.595      1.427  0.595   0
## d.42         0.703 0.587     -0.445    0.701      1.859  0.698   0
## d.43         0.739 0.353      0.048    0.739      1.433  0.737   0
## d.44        -0.575 0.474     -1.546   -0.561      0.316 -0.532   0
## d.45        -0.061 0.384     -0.828   -0.056      0.680 -0.047   0
## d.46         0.713 0.293      0.139    0.713      1.288  0.712   0
## d.47         0.447 0.523     -0.590    0.450      1.464  0.456   0
## d.48         0.700 0.360     -0.006    0.700      1.408  0.699   0
## d.49        -1.230 1.047     -3.465   -1.166      0.651 -1.035   0
## d.50         0.483 0.478     -0.461    0.485      1.415  0.489   0
## d.51         0.449 0.377     -0.294    0.451      1.185  0.453   0
## d.52         0.391 0.323     -0.244    0.392      1.022  0.394   0
## d.53         0.286 0.482     -0.675    0.290      1.219  0.300   0
## d.54        -0.669 0.837     -2.437   -0.625      0.856 -0.537   0
## d.55         0.913 0.355      0.221    0.912      1.613  0.909   0
## d.56        -0.777 0.495     -1.799   -0.759      0.144 -0.724   0
## d.57         0.428 0.392     -0.347    0.429      1.193  0.432   0
## d.58        -1.120 0.776     -2.772   -1.074      0.278 -0.980   0
## d.59        -0.601 0.448     -1.514   -0.588      0.243 -0.563   0
## d.60        -0.635 0.409     -1.465   -0.625      0.140 -0.606   0
## 
## Expected number of effective parameters(stdev): 54.05(0.00)
## Number of equivalent replicates : 35.78 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2521.28
## Effective number of parameters .................: 52.54
## 
## Marginal log-Likelihood:  -1291.66 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
</div>
</div>
<div id="varying-intercepts-model-1" class="section level2">
<h2>2.2 varying intercepts model</h2>
<div id="rethinking-6" class="section level3">
<h3>2.2 rethinking</h3>
<pre class="r"><code>m2.2 &lt;- ulam( alist(
C ~ bernoulli( p ),
logit(p) &lt;- a[did],
a[did] ~ normal( a_bar , sigma ),
a_bar ~ normal( 0 , 1.5 ),
sigma ~ exponential( 1 )
) ,data=dat_list , chains=4 , cores=4 , log_lik=TRUE )

precis(m2.2, depth= 2)</code></pre>
<pre><code>##               mean         sd       5.5%        94.5%     n_eff     Rhat4
## a[1]  -0.997141324 0.20032128 -1.3176990 -0.683552803 3330.9198 0.9986410
## a[2]  -0.583550038 0.34635071 -1.1433127 -0.025564335 4215.3502 0.9985219
## a[3]  -0.236028424 0.49359837 -1.0118678  0.583549822 3080.6929 0.9991575
## a[4]  -0.188091536 0.31774196 -0.6855768  0.314631931 3044.0537 0.9994738
## a[5]  -0.577634204 0.28801456 -1.0498506 -0.124583947 2723.0054 0.9988699
## a[6]  -0.814470343 0.23845475 -1.2055656 -0.440667452 2707.7182 0.9998485
## a[7]  -0.771061140 0.35532729 -1.3661397 -0.212134284 2799.4004 0.9996399
## a[8]  -0.524988123 0.30407635 -1.0043393 -0.052942005 3765.2069 0.9987820
## a[9]  -0.714822938 0.33905876 -1.2552476 -0.193185062 3524.4993 0.9986281
## a[10] -1.136284804 0.42099995 -1.8485748 -0.478041067 2470.5557 0.9992511
## a[11] -1.557741900 0.43419217 -2.2519223 -0.891486283 1608.5779 0.9994910
## a[12] -0.606848765 0.31237689 -1.1122144 -0.116302849 3857.0564 0.9987674
## a[13] -0.431361134 0.31330605 -0.9306376  0.068675371 3038.8202 0.9987938
## a[14]  0.391085430 0.17819919  0.1112582  0.673188436 2718.7206 0.9984566
## a[15] -0.565681208 0.33819829 -1.1105938 -0.020115330 3325.5505 1.0011317
## a[16] -0.120037267 0.35494857 -0.6833841  0.430466336 3237.9865 0.9985821
## a[17] -0.747223171 0.31624228 -1.2635272 -0.254058375 3774.0649 0.9981083
## a[18] -0.638559004 0.27881281 -1.0839450 -0.189010807 4899.9265 0.9985230
## a[19] -0.499255743 0.30671868 -0.9942115 -0.015849087 3176.8881 1.0014729
## a[20] -0.483635772 0.37935875 -1.0725934  0.128095919 3231.7757 0.9991624
## a[21] -0.508016597 0.37005541 -1.1081372  0.080990432 3697.5420 1.0006605
## a[22] -0.970145096 0.36014618 -1.5554630 -0.417736007 2512.2077 0.9990145
## a[23] -0.765118087 0.38445582 -1.3876302 -0.164841298 3730.1409 0.9996247
## a[24] -1.193671084 0.42986123 -1.9112785 -0.529538209 2191.2479 0.9996998
## a[25] -0.279305086 0.22680565 -0.6347727  0.082766749 3177.1930 0.9993759
## a[26] -0.521420572 0.37106057 -1.1144443  0.065545318 4579.5000 0.9993963
## a[27] -1.195441852 0.30937439 -1.6924272 -0.719516474 3035.4430 0.9986251
## a[28] -0.964443663 0.26409219 -1.3988300 -0.550804627 2896.3360 0.9984767
## a[29] -0.796461096 0.32224244 -1.3291728 -0.286912897 3820.6532 1.0003943
## a[30] -0.132333445 0.22695164 -0.5114133  0.231967989 2815.4668 0.9985978
## a[31] -0.307790205 0.30170151 -0.7837685  0.158449566 3206.6414 1.0000137
## a[32] -0.983473041 0.35336922 -1.5522535 -0.430753918 2032.4720 0.9994201
## a[33] -0.425306838 0.37249514 -1.0178787  0.159852513 3699.1170 0.9992333
## a[34]  0.272966902 0.30269044 -0.1935569  0.758651495 2988.2578 0.9992547
## a[35] -0.130488073 0.25610184 -0.5288819  0.288216162 3185.4433 0.9987611
## a[36] -0.579233086 0.35831797 -1.1707532  0.006079621 3130.6270 0.9995577
## a[37] -0.226952617 0.38493354 -0.8490031  0.388281841 4060.3045 0.9987991
## a[38] -0.716655343 0.40741026 -1.3781736 -0.079459712 3104.1983 0.9990846
## a[39] -0.203974300 0.31656824 -0.7101172  0.304267662 3231.4960 1.0002434
## a[40] -0.253483103 0.26548315 -0.6728011  0.166551135 3347.9711 0.9992766
## a[41] -0.207278993 0.33516375 -0.7363900  0.347980768 3861.8788 0.9990686
## a[42] -0.241089006 0.42271039 -0.9031746  0.432129350 3777.8368 0.9991861
## a[43] -0.040450898 0.26175582 -0.4630550  0.376060558 3699.2265 0.9987211
## a[44] -0.963000844 0.34439568 -1.5160571 -0.426446562 3212.8710 0.9989371
## a[45] -0.661494374 0.28761782 -1.1276809 -0.209979250 3697.7297 0.9995253
## a[46] -0.001687546 0.19642147 -0.3034124  0.306184450 3357.8349 0.9999781
## a[47] -0.360475605 0.38112211 -0.9745867  0.243352853 3905.2743 0.9984677
## a[48] -0.075963933 0.27343119 -0.5188097  0.356964135 3094.1405 0.9988609
## a[49] -0.882507707 0.50021925 -1.6799347 -0.105474140 2744.2875 0.9998255
## a[50] -0.313694962 0.35654708 -0.8973272  0.252118847 3091.5987 1.0002829
## a[51] -0.286652644 0.28693525 -0.7411200  0.178711975 4460.8683 0.9983270
## a[52] -0.303181632 0.22354007 -0.6556683  0.044824474 3699.8775 0.9986071
## a[53] -0.427027041 0.34582707 -0.9818526  0.124256429 3135.9405 0.9996112
## a[54] -0.797310599 0.45925308 -1.5390220 -0.067739525 2329.3885 1.0010647
## a[55]  0.092210763 0.26894999 -0.3342494  0.515998226 3138.7371 0.9994400
## a[56] -1.069475274 0.35058940 -1.6543960 -0.523929690 2425.5346 0.9988758
## a[57] -0.309627000 0.28859718 -0.7882513  0.155072681 3206.6723 0.9986294
## a[58] -1.022354685 0.42895315 -1.7207608 -0.371480942 2217.5349 0.9996018
## a[59] -0.996772453 0.33093146 -1.5422902 -0.484825022 2759.7663 0.9999807
## a[60] -1.054489173 0.30273684 -1.5565004 -0.588944830 2995.6794 1.0006467
## a_bar -0.544725228 0.08952100 -0.6905118 -0.406168951 1518.9611 1.0025979
## sigma  0.518890758 0.08188134  0.3990366  0.655443747  772.9948 1.0033020</code></pre>
</div>
<div id="inla-6" class="section level3">
<h3>2.2 inla</h3>
<pre class="r"><code>m2.2.i &lt;- inla(use.contraception ~ f(district_id, model=&quot;iid&quot;), data= d2.i, family = &quot;binomial&quot;, 
              Ntrials = 1, #Ntrials = 1 for bernoulli
              control.family = list(control.link=list(model=&quot;logit&quot;)),
              control.fixed = list(
        mean.intercept=  0 ,
        prec.intercept= 1/(1.5^2)),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T, waic= TRUE))
summary(m2.2.i)</code></pre>
<pre><code>## 
## Call:
##    c(&quot;inla(formula = use.contraception ~ f(district_id, model = \&quot;iid\&quot;), &quot;, &quot; family = \&quot;binomial\&quot;, data = d2.i, Ntrials 
##    = 1, control.compute = list(config = T, &quot;, &quot; waic = TRUE), control.predictor = list(link = 1, compute = T), &quot;, &quot; 
##    control.family = list(control.link = list(model = \&quot;logit\&quot;)), &quot;, &quot; control.fixed = list(mean.intercept = 0, 
##    prec.intercept = 1/(1.5^2)))&quot; ) 
## Time used:
##     Pre = 1.67, Running = 1.42, Post = 0.31, Total = 3.4 
## Fixed effects:
##               mean    sd 0.025quant 0.5quant 0.975quant   mode kld
## (Intercept) -0.532 0.084     -0.701   -0.531     -0.371 -0.528   0
## 
## Random effects:
##   Name     Model
##     district_id IID model
## 
## Model hyperparameters:
##                           mean   sd 0.025quant 0.5quant 0.975quant mode
## Precision for district_id 4.72 1.64       2.38     4.43       8.72 3.95
## 
## Expected number of effective parameters(stdev): 33.99(4.22)
## Number of equivalent replicates : 56.89 
## 
## Watanabe-Akaike information criterion (WAIC) ...: 2514.73
## Effective number of parameters .................: 33.32
## 
## Marginal log-Likelihood:  -1278.85 
## Posterior marginals for the linear predictor and
##  the fitted values are computed</code></pre>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                         mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.4799258 0.07896337 0.3386648 0.4748904 0.6490116 0.4657571</code></pre>
<p>Side note: this is how you calculate the sd from the hyperprior (<span class="math inline">\(\sigma\)</span>)</p>
<pre class="r"><code>bri.hyperpar.summary(m2.2.i)</code></pre>
<pre><code>##                         mean         sd    q0.025      q0.5    q0.975      mode
## SD for district_id 0.4799258 0.07896337 0.3386648 0.4748904 0.6490116 0.4657571</code></pre>
<pre class="r"><code># hyperparameter of the precision
m2.2.i.prec &lt;- m2.2.i$internal.marginals.hyperpar

#transform precision to sd using inla.tmarginal
#m2.2.i.prec[[1]] is used to access the actual values inside the list
m2.2.i.sd &lt;- inla.tmarginal(function(x) sqrt(exp(-x)), m2.2.i.prec[[1]])
#plot the post of the sd per district (sigma)
plot(m2.2.i.sd)</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.2%20inla%20hyper%20sd-1.png" width="672" /></p>
<pre class="r"><code>#summary stats for the sd 
m2.2.i.sd.sum &lt;- inla.zmarginal(m2.2.i.sd)</code></pre>
<pre><code>## Mean            0.479926 
## Stdev           0.0789634 
## Quantile  0.025 0.338665 
## Quantile  0.25  0.424458 
## Quantile  0.5   0.47489 
## Quantile  0.75  0.529788 
## Quantile  0.975 0.649012</code></pre>
<pre class="r"><code># this coincides perfectly with the result from bri.hyperpar.summary</code></pre>
</div>
</div>
<div id="plot-of-posterior-mean-probabilities-in-each-district" class="section level2">
<h2>2.3 plot of posterior mean probabilities in each district</h2>
<p>Now let’s extract the samples, compute posterior mean probabilities in each district, and plot it all:</p>
<div id="plot-rethinking" class="section level3">
<h3>plot rethinking</h3>
<pre class="r"><code>post1 &lt;- extract.samples( m2.1 ) 
post2 &lt;- extract.samples( m2.2 )
p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) 
p2 &lt;- apply( inv_logit(post2$a) , 2 , mean )
nd &lt;- max(dat_list$did)
plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; )
points( 1:nd , p1 , pch=16 , col=rangi2 ) 
points( 1:nd , p2 )
abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20rethinking-1.png" width="672" /></p>
</div>
<div id="plot-inla" class="section level3">
<h3>plot inla</h3>
<p><a href="https://people.bath.ac.uk/jjf23/inla/oneway.html" class="uri">https://people.bath.ac.uk/jjf23/inla/oneway.html</a></p>
<p><a href="https://people.bath.ac.uk/jjf23/brinla/reeds.html" class="uri">https://people.bath.ac.uk/jjf23/brinla/reeds.html</a></p>
<p><strong>posterior mean for each district a for the idex fixed effect model m2.1:</strong></p>
<pre class="r"><code># m2.2.i$summary.fixed[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

inverse_logit &lt;- function (x){
    p &lt;- 1/(1 + exp(-x))
    p &lt;- ifelse(x == Inf, 1, p)
    p }

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
# we eliminate the first element of this list, the intercept.
m2.1.i.fix&lt;- lapply(m2.1.i$marginals.fixed, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))[-1]</code></pre>
<p><strong>posterior mean for each district a for the varying intercept model m2.2:</strong></p>
<pre class="r"><code># m2.2.i$summary.random[[1]] would gives us the summary we want but not in the response scale, we need to  transform it using the inverse logit 

#inla.tmarginal : apply inverse logit to all district marginals 
#inla.zmarginal : summary of the logit-transformed marginals 
m2.2.i.rand&lt;- lapply(m2.2.i$marginals.random$district_id, function (x) inla.zmarginal( inla.tmarginal (inverse_logit, x )))</code></pre>
<pre class="r"><code># sapply(m2.2.i.rand, function(x) x[1]) extracts the first element (the mean) from the summary of the posterior of each district
m2.i.mean &lt;- bind_cols(district= 1:60,mean.m2.1= unlist(sapply(m2.1.i.fix, function(x) x[1])), mean.m2.2=unlist(sapply(m2.2.i.rand, function(x) x[1])))

m2.2.i.abar &lt;- inla.zmarginal( inla.tmarginal (inverse_logit, m2.2.i$marginals.fixed[[&quot;(Intercept)&quot;]] ))</code></pre>
<pre><code>## Mean            0.370149 
## Stdev           0.0193718 
## Quantile  0.025 0.331663 
## Quantile  0.25  0.357164 
## Quantile  0.5   0.37022 
## Quantile  0.75  0.383175 
## Quantile  0.975 0.407995</code></pre>
<pre class="r"><code>m2.i.district.plot &lt;- ggplot() +
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.1), color= &quot;blue&quot;, alpha= 0.5)+
  geom_point(data= m2.i.mean, aes(x= district, y= mean.m2.2), color= &quot;black&quot;, alpha= 0.5, shape= 1)+
  geom_hline(yintercept=m2.2.i.abar[[1]], linetype=&#39;longdash&#39;) +
  ylim(0,1)+
  labs(y = &quot;prob use contraception&quot;)+
  theme_bw()
  

m2.i.district.plot</code></pre>
<p><img src="rethinkingINLA_HW8_files/figure-html/8.2.3%20plot%20inla-1.png" width="672" /></p>
<p>The blue points are the fixed estimations. The open points are the varying effects. As you’d expect, they are shrunk towards the mean (the dashed line). Some are shrunk more than others. The third district from the left shrunk a lot.</p>
<p>Let’s look at the sample size in each district:</p>
<pre class="r"><code> table(d$district_id)</code></pre>
<pre><code>## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33 
## 117  20   2  30  39  65  18  37  23  13  21  29  24 118  22  20  24  47  26  15  18  20  15  14  67  13  44  49  32  61  33  24  14 
##  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  35  48  17  13  14  26  41  26  11  45  27  39  86  15  42   4  19  37  61  19   6  45  27  33  10  32  42</code></pre>
<p>District 3 has only 2 women sampled. So it shrinks a lot. There are couple of other districts, like 49 and 54, that also have very few women sampled. But their fixed estimates aren’t as extreme, so they don’t shrink as much as district 3 does. All of this is explained by partial pooling, of course.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
