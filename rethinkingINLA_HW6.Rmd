---
title: "Statistical Rethinking 2nd edition Homework 6 in INLA"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r libraries, message= FALSE}
library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)
```

# Intro to link functions from Statistical Rethinking 2nd edition Chapter.10

## Logit link: 

The logit link maps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this:

yi ∼ Binomial(n, pi) 

logit(pi) = α + βxi

And the logit function itself is defined as the log-odds: 

logit(pi) = log (pi/(1 − pi))

The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is:

log (pi/(1 − pi)) = α + βxi

So to figure out the definition of pi implied here, just do a little algebra and solve the above equation for pi:

pi = exp(α + βxi) / 1 + exp(α + βxi)

The above function is usually called the **logistic**. In this context, it is also commonly called the **inverse-logit**, because it inverts the logit transform.

No longer does a unit change in a predictor variable produce a constant change in the mean of the outcome variable. Instead, a unit change in xi may produce a larger or smaller change in the probability pi, depending upon how far from zero the log-odds are. For example, in Figure 10.7, when x = 0 the linear model has a value of zero on the log-odds scale. A half- unit increase in x results in about a 0.25 increase in probability. But each addition half-unit will produce less and less of an increase in probability, until any increase is vanishingly small.

The key lesson for now is just that no regression coefficient, such as β, from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 7) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change. More generally, every predic- tor variable effectively interacts with every other predictor variable, whether you explicitly model them as interactions or not. This fact makes the visualization of counter-factual pre- dictions even more important for understanding what the model is telling you.

## Log link: 

This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation σ of a Gaussian distribution so it is a function of a predictor variable x. The parameter σ must be positive, because a standard deviation cannot be negative nor can it be zero. The model might look like:

yi ∼ Normal(μ, σi) 

log(σi) = α + βxi

In this model, the mean μ is constant, but the standard deviation scales with the value xi. A log link is both conventional and useful in this situation. It prevents σ from taking on a negative value.
What the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving log(σi) = α + βxi for σi yields the inverse link:

σi = exp(α + βxi)

Using a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right). Another way to think of this relationship is to remember that logarithms are magnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the un- transformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot of Figure 10.8.
While using a log link does solve the problem of constraining the parameter to be posi- tive, it may also create a problem when the model is asked to predict well outside the range of data used to fit it. Exponential relationships grow, well, exponentially. Just like a lin- ear model cannot be linear forever, an exponential model cannot be exponential forever. Human height cannot be linearly related to weight forever, because very heavy people stop getting taller and start getting wider. Likewise, the property damage caused by a hurricane may be approximately exponentially related to wind speed for smaller storms. But for very big storms, damage may be capped by the fact that everything gets destroyed.

# HOMEWORK 6

## 1. 
**The data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 2010–2012 (see van der Lee and Ellemers doi:10.1073/pnas.1510159112). These data have a very similar structure to the UCBAdmit data discussed in Chapter 11.
I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question.
What is your causal interpretation? If NWO’s goal is to equalize rates of funding between the genders, what type of intervention would be most effective?**

The implied DAG is:

```{r 6.1 dag}

hw6.1dag <- dagitty("dag{
                  A <- G
                  A<- D
                  D <- G
                  }")
plot(hw6.1dag)
```

G is gender, D is discipline, and A is award. The direct causal effect of gender is the path G → A. The total effect includes that path and the indirect path G → D → A. 

## 1.a total causal effect of gender

We can estimate the total causal influence (assuming this DAG is correct) with a model that conditions only on gender. I’ll use a N(-1,1) prior for the intercepts, because we know from domain knowledge that less than half of applicants get awards.

### 1.a rethinking

```{r 6.1.a re}
data(NWOGrants)
d <- NWOGrants
dat_list <- list(
awards = as.integer(d$awards),
apps = as.integer(d$applications),
gid = ifelse( d$gender=="m" , 1L , 2L )
)

m1_total <- ulam(
alist(
awards ~ binomial( apps , p ), 
logit(p) <- a[gid],
a[gid] ~ normal(-1,1)
), data=dat_list , chains=4 ) 

precis(m1_total,2)
```


Gender 1 here is male and 2 is female. So males have higher rates of award, on average. How big is the difference? Let’s look at the contrast on absolute (penguin) scale:

```{r 6.1.a re diff}
post <- extract.samples(m1_total)
diff <- inv_logit( post$a[,1] ) - inv_logit(post$a[,2]) 
precis( list( diff=diff ) )
```


So a small 3% difference on average. Still, with such low funding rates (in some disciplines), 3% is a big advantage.

### 1.a inla

```{r 6.1.a inla }
data(NWOGrants)
d <- NWOGrants

d1.i <- d %>% 
  mutate(g_val= 1, 
         d= paste("d", as.integer(discipline), sep = "."),
         d_val= 1) %>% 
  spread(gender, g_val) %>% 
  spread(d, d_val)

# number of trials is d1.i$applications

m1a.i <- inla(awards ~ -1 + m + f , data= d1.i, family = "binomial", 
              Ntrials = applications, 
              control.family = list(control.link=list(model="logit")),
              control.fixed = list(
        mean= -1, 
        prec= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m1a.i)

#computed in the outcome scale 
m1a.i$summary.fitted.values


#contrast between male and female

n.samples = 1000

#pi = exp(α + βxi) / 1 + exp(α + βxi)

inverse_logit <- function (x){
    p <- 1/(1 + exp(-x))
    p <- ifelse(x == Inf, 1, p)
    p }

m1a.i.samples = inla.posterior.sample(n.samples, result = m1a.i)

m1a.i.contrast1 <- inla.posterior.sample.eval(function(...) {
    inverse_logit(m) - 
    inverse_logit(f)
},
   m1a.i.samples)
summary(as.vector(m1a.i.contrast1))

```

## 1.b direct effect of gender 

Now for the direct influence of gender, we condition on discipline as well:

### 1.b rethinking
```{r 6.1.b re}
dat_list$disc <- as.integer(d$discipline) 

m1_direct <- ulam(
alist(
awards ~ binomial( apps , p ), 
logit(p) <- a[gid] + d[disc], 
a[gid] ~ normal(-1,1),
d[disc] ~ normal(0,1)
),
data=dat_list , chains=4 , cores=4 , iter=3000 ) 

precis(m1_direct,2)
```

Those chains didn’t sample very efficiently. This likely because the model is over- parameterized—it has more parameters than absolutely necessary. This doesn’t break it. It just makes the sampling less efficient. Anyway, now we can compute the gender difference again. On the relative scale:

```{r 6.1.b re diff}
post <- extract.samples(m1_direct) 
diff_a <- post$a[,1] - post$a[,2] 
precis( list( diff_a=diff_a ) )

```

Still an advantage for the males, but reduced and overlapping zero a bit. To see this difference on the absolute scale, we need to account for the base rates in each discipline as well. If you look at the postcheck(m1_direct) display, you’ll see the predictive difference is very small. There are also several disciplines that reverse the advantage. If there is a direct influence of gender here, it is small, much smaller than before we accounted for discipline. Why? Because again the disciplines have different funding rates and women apply more to the disciplines with lower funding rates. But it would be hasty, I think, to conclude there are no other influences. There are after all lots of unmeasured confounds...

```{r 6.1.b postcheck}
postcheck(m1_direct)
```

### 1.b inla

```{r 6.1.b inla }
data(NWOGrants)
d <- NWOGrants

d1.i <- d %>% 
  mutate(g_val= 1, 
         d= paste("d", as.integer(discipline), sep = "."),
         d_val= 1) %>% 
  spread(gender, g_val) %>% 
  spread(d, d_val)

# number of trials is d1.i$applications

d_names <- paste("d", 1:9, sep = ".")

m1b.i <- inla(awards ~ -1 + m + f + d.1+d.2+d.3+d.4+d.5+d.6+d.7+d.8+d.9, data= d1.i, family = "binomial", 
              Ntrials = applications, 
              control.family = list(control.link=list(model="logit")),
              control.fixed = list(
        mean= list(m=-1,f=-1, d.1=0, d.2=0, d.3=0, d.4=0,d.5=0,d.6=0,d.7=0,d.8=0,d.9),
        prec= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m1b.i)

#contrast between male and female

n.samples = 1000

m1b.i.samples = inla.posterior.sample(n.samples, result = m1b.i)

#On the relative scale:
m1b.i.contrast1 <- inla.posterior.sample.eval(function(...) {
    m - f
},
   m1b.i.samples)
summary(as.vector(m1b.i.contrast1))

```



## 2. 
**Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant.
Add these influences to your DAG from Problem 1. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the back-door criterion. Hint: This is structurally a lot like the grandparents-parents- children-neighborhoods example from a previous week.
If you have trouble thinking this though, try simulating fake data, assuming your DAG is true. Then analyze it using the model from Problem 1. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence?**

```{r 6.2 dag}

hw6.2dag <- dagitty("dag{
                  A <- G
                  A<- D
                  D <- G
                  D <- S
                  A <- S
                  }")
plot(hw6.2dag)
```

S is stage of career (unobserved). This DAG has the same structure as the grandparents-parents-children-neighborhoods example from earlier in the course. When we condition on discipline D it opens a backdoor path through S to A. 

**It is not possible here to get an unconfounded estimate of gender on awards.**

Here’s a simulation to demonstrate the potential issue: This code simulates 1000 applicants. There are 2 genders (G 0/1), 2 stages of career (S 0/1), and 2 disciplines (D 0/1). Discipline 1 is chosen more by gender 1 and career stage 1. So that could mean more by males and later stage of career. Then awards A have a consistent bias towards gender 1, and discipline 1 has a higher award rate, and stage 1 also a higher award rate. If we analyze these data:

### 2.a rethinking 
```{r 6.2 sim}

set.seed(1913)
N <- 1000
G <- rbern(N)
S <- rbern(N)
D <- rbern( N , p=inv_logit( G + S ) )
A <- rbern( N , p=inv_logit( 0.25*G + D + 2*S - 2 ) ) 
dat_sim <- list( G=G , D=D , A=A )

m2_sim <- ulam( alist(
A ~ bernoulli(p), 
logit(p) <- a + d*D + g*G, 
c(a,d,g) ~ normal(0,1)
), data=dat_sim , chains=4 , cores=4 ) 

precis(m2_sim)

```

The parameter g is the advantage of gender 1. It is smaller than the true advantage and the estimate straddles zero quite a lot, even with 1000 applicants. 

### 2.a INLA

Reference: https://rpubs.com/corey_sparks/431920
```{r 6.2 inla}

set.seed(1913)
N <- 1000
G <- rbern(N)
S <- rbern(N)
D <- rbern( N , p=inv_logit( G + S ) )
A <- rbern( N , p=inv_logit( 0.25*G + D + 2*S - 2 ) ) 
dat_sim <- list( G=G , D=D , A=A )

m2a.i <- inla(A ~ D+G, data= dat_sim, family = "binomial", 
              Ntrials = 1, #Ntrials=1 for bernoulli, logistic regression
              control.family = list(control.link=list(model="logit")),
              control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m2a.i)

```

**It is also possible to have no gender influence and infer it by accident.**
Try these settings:
### 2.b rethinking 
```{r 6.2 sim2}
set.seed(1913)
N <- 1000
G <- rbern(N)
S <- rbern(N)
D <- rbern( N , p=inv_logit( 2*G - S ) )
A <- rbern( N , p=inv_logit( 0*G + D + S - 2 ) ) 
dat_sim2 <- list( G=G , D=D , A=A )
m2_sim_2 <- ulam( m2_sim , data=dat_sim2 , chains=4 , cores=4 ) 

precis(m2_sim_2,2)
```

Now it looks like gender 1 has a consistent advantage, but in fact there is no advan- tage in the simulation.

### 2.b INLA
```{r 6.2 sim}

set.seed(1913)
N <- 1000
G <- rbern(N)
S <- rbern(N)
D <- rbern( N , p=inv_logit( 2*G - S ) )
A <- rbern( N , p=inv_logit( 0*G + D + S - 2 ) ) 
dat_sim2 <- list( G=G , D=D , A=A )

m2b.i <- inla(A ~ D+G, data= dat_sim2, family = "binomial", 
              Ntrials = 1, #Ntrials=1 for bernoulli, logistic regression
              control.family = list(control.link=list(model="logit")),
              control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1),
              control.predictor=list(link=1, compute=T),
              control.compute=list(config=T))
summary(m2b.i)

```

## 3. 

**The data in data(Primates301) were first introduced at the end of Chapter7. In this problem, you will consider how brain size is associated with social learning. There are three parts.**

## 3.1
**First, model the number of observations of social_learning for each species as a function of the log brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the resulting posterior.** 

Now we first want a model with social learning as the outcome and brain size as a predictor. For this Poisson GLM, I’m going to use a N(0,1) prior on the intercept, since we know the counts should be small.

### 3.1.a rethinking

```{r 6.3.1 re}

library(rethinking)
data(Primates301)
d <- Primates301
d3 <- d[ complete.cases( d$social_learning , d$brain , d$research_effort),]

dat <- list(
soc_learn = d3$social_learning,
log_brain = standardize( log(d3$brain) ), log_effort = log(d3$research_effort)
)

m3_1 <- ulam( alist(
soc_learn ~ poisson( lambda ), log(lambda) <- a + bb*log_brain, a ~ normal(0,1),
bb ~ normal(0,0.5)
), data=dat , chains=4 , cores=4 ) 

precis( m3_1 )

postcheck(m3_1,window=50)
```

### 3.1.a INLA

```{r 6.3.1 inla}

library(rethinking)
data(Primates301)
d <- Primates301
d3.i <- d[ complete.cases( d$social_learning , d$brain , d$research_effort),] %>% 
  mutate(soc_learn = social_learning,
log_brain = standardize( log(brain) ), 
log_effort = log(research_effort), 
case= row_number())

m3.1a.i <- inla( soc_learn~ log_brain, family='poisson',
   data=d3.i,
   control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(0.5^2)),
   control.family=list(link='log'),
   control.predictor=list(link=1, compute=TRUE),
   control.compute=list(dic=TRUE, cpo=TRUE, waic=TRUE))

summary(m3.1a.i)
```

inla postcheck

```{r inla 6.3.1 postcheck}

m3.1.i.postcheck <- bind_cols(d3.i[, c("case", "soc_learn")], m3.1.i$summary.fitted.values)

names(m3.1.i.postcheck) <- c("case", "soc_learn", "mean", "sd", "LCI", "MCI", 
"UCI", "mode")

m3.1.i.postcheck.plot <- ggplot() +
  geom_point(data= m3.1.i.postcheck, aes(x= case, y= soc_learn), color= "blue", alpha= 0.5)+
  geom_pointrange(data= m3.1.i.postcheck, aes(x= case, y= mean, ymin= LCI, ymax= UCI ), color= "red", alpha= 0.3, size= 0.15)+
  theme_bw()
  

m3.1.i.postcheck.plot 

```

The blue points are the raw data, recall. These are not great posterior predictions. Clearly other factors are in play.

Let’s try the research effort variable now. 

## 3.2
**Second, some species are studied much more than others. So the number of reported instances of social_learning could be a product of research effort. Use the research_effort variable, specifically its logarithm, as an additional predictor variable. Interpret the coefficient for log research_effort. Does this model disagree with the previous one?**

### 3.2 rethinking

```{r 6.3.2 rethinking}

library(rethinking)
data(Primates301)
d <- Primates301
d3 <- d[ complete.cases( d$social_learning , d$brain , d$research_effort),]

dat <- list(
soc_learn = d3$social_learning,
log_brain = standardize( log(d3$brain) ), log_effort = log(d3$research_effort)
)

m3_2 <- ulam( alist(
soc_learn ~ poisson( lambda ),
log(lambda) <- a + be*log_effort + bb*log_brain, 
a ~ normal(0,1),
c(bb,be) ~ normal(0,0.5)
), data=dat , chains=4 , cores=4 ) 
precis( m3_2 )

precis( m3_2)

postcheck(m3_2,window=50)
```


Brain size bb is still positively associated, but much less. Research effort be is strongly associated. To see how these models disagree, let’s use pointwise WAIC to see which cases each predicts well.


```{r re waic}
waic1 <- WAIC( m3_1 , pointwise=TRUE )
waic2 <- WAIC( m3_2 , pointwise=TRUE )
plot( waic1 - waic2 , dat$log_effort , col=rangi2 , pch=16 ) 
identify( waic1-waic2 , dat$log_effort , d2$genus , cex=0.8 ) abline(v=0,lty=2,lwd=0.5)

```



### 3.2 INLA

```{r 6.3.2 inla}

library(rethinking)
data(Primates301)
d <- Primates301
d3.i <- d[ complete.cases( d$social_learning , d$brain , d$research_effort),] %>% 
  mutate(soc_learn = social_learning,
log_brain = standardize( log(brain) ), 
log_effort = log(research_effort), 
case= row_number())

m3.2.i <- inla( soc_learn~ log_brain + log_effort, family='poisson',
   data=d3.i,
   control.fixed = list(
        mean= 0,
        prec= 1, 
        mean.intercept= 0, 
        prec.intercept= 1/(0.5^2)),
   control.family=list(link='log'),
   control.predictor=list(link=1, compute=TRUE),
   control.compute=list(dic=TRUE, cpo=TRUE, waic=TRUE))

summary(m3.2.i)

```

```{r inla 6.3 waic compare}

waic.i1 <- m3.1.i$waic$local.waic
waic.i2 <- m3.2.i$waic$local.waic

d3.i <- d3.i %>% 
  mutate(waic.i1= waic.i1a, 
         waic.i2= waic.i1b, 
         waic.diff= waic.i1a-waic.i1b)

waic.i.3plot <- ggplot(data= d3.i, aes(x= waic.diff, y= log_effort, label= genus))+
  geom_point()+
  geom_text(aes(label=genus),hjust=0, vjust=0)+
  geom_vline(xintercept=0, linetype='longdash') +
    theme_bw()
  
  
waic.i.3plot
```
Species on the right of the vertical line fit better for model m3_2, the model with research effort. These are mostly species that are studied a lot, like chimpanzees (Pan) and macaques (Macaca). The genus Pan especially has been a focus on social learning research, and its counts are inflated by this.
This is a good example of how the nature of measurement influences inference. There are likely a lot of false zeros in these data, species that are not studied often enough to get a good idea of their learning tendencies. Meanwhile every time a chimpanzee sneezes, someone writes a social learning paper.

**3. Third, draw a DAG to represent how you think the variables social_learning, brain, and research_effort interact. Justify the DAG with the measured associations in the two models above (and any other models you used).**


The implied DAG is:

```{r 6.3.3 dag}

hw6.3dag <- dagitty("dag{
                  S <- E
                  S<- B
                  E <- B
                  }")
plot(hw6.3dag)
```
B is brain size, E is research effort, and S is social learning. Research effort doesn’t actually influence social learning, but it does influence the value of the variable. The model results above are consistent with this DAG in the sense that including E reduced the association with B, which we would expect when we close the indirect path through E. If researchers choose to look for social learning in species with large brains, this leads to an exaggerated estimate of the association between brains and social learning
