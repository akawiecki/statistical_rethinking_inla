---
title: "INLArethinking_HW4"
author: "Ania Kawiecki"
date: "8/3/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

* [Statistical rethinking homework solutions] (https://github.com/rmcelreath/statrethinking_winter2019/tree/master/homework)

INLA book: https://becarioprecario.bitbucket.io/inla-gitbook/ch-intro.html

```{r libraries, message= FALSE}
library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)

```

# HOMEWORK 4

## 1. 
Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important birb species:

```{r birb spp table}

IB1 <- c( 0.2 , 0.2 , 0.2 , 0.2 , 0.2 )
IB2<- c( 0.8 , 0.1 , 0.05 , 0.025 , 0.025 )
IB3 <- c( 0.05 , 0.15 , 0.7 , 0.05 , 0.05 )

df <- data.frame(IB1, IB2, IB3)

rownames(df) <- c("A", "B", "C", "D", "E")

kable(df)

```

Notice that each column sums to 1, all the birbs. This problem has two parts. It is not computationally complicated. But it is conceptually tricky.

First, compute the entropy of each island’s birb distribution. Interpret these entropy values.

Second, use each island’s birb distribution to predict the other two. This means to compute the K-L Divergence of each island from the others, treat- ing each island as if it were a statistical model of the other islands. You should end up with 6 different K-L Divergence values. Which island predicts the others best? Why?


SOL: 
To compute the entropies,we just need a function to compute the entropy.Information entropy, as defined in lecture and the book, is simply:
 $H(p) = - \sum_{i}p_i log(p_i)

where p is a vector of probabilities summing to 1. In R code this would look like:
```{r hw4.1.1}

H <- function(p) -sum(p*log(p))

IB <- list()
IB[[1]] <- c( 0.2 , 0.2 , 0.2 , 0.2 , 0.2 )
IB[[2]] <- c( 0.8 , 0.1 , 0.05 , 0.025 , 0.025 )
IB[[3]] <- c( 0.05 , 0.15 , 0.7 , 0.05 , 0.05 )
sapply( IB , H )

```

The first island has the largest entropy, followed by the third, and then the second in last place. Why is this? Entropy is a measure of the evenness of a distribution. The first islands has the most even distribution of birbs. This means you wouldn’t be very surprised by any particular birb. The second island, in contrast, has a very uneven distribution of birbs. If you saw any birb other than the first species, it would be surprising.

Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution.This is often known as Kullback-Leibler divergence or simply K-L divergence. In plainer language, the divergence is the average difference in log probability between the target (p) and model (q). This divergence is just the difference between two entropies: The entropy of the target distribution p and the cross entropy arising from using q to predict p. When p = q, we know the actual probabilities of the events and the K-L distance = 0. What divergence can do for us now is help us contrast different approximations to p. As an approximating function q becomes more accurate, DKL(p, q) will shrink. So if we have a pair of candidate distributions, then the candidate that minimizes the divergence will be closest to the target. Since predictive models specify probabilities of events (observations), we can use divergence to compare the accuracy of models.

Now we need K-L distance, so let’s write a function for it:

```{r hw4.1.2.1}
DKL <- function(p,q) sum( p*(log(p)-log(q)) )
```

This is the distance from q to p, regarding p as true and q as the model. Now to use each island as a model of the others, we need to consider the different ordered pairings. I’ll just make a matrix and loop over rows and columns:
```{r hw4 1.2.2}


Dm <- matrix( NA , nrow=3 , ncol=3 )

for ( i in 1:3 ) for ( j in 1:3 ) Dm[i,j] <- DKL( IB[[j]] , IB[[i]])

#test <- DKL( IB[[1]] , IB[[2]])
#0.2*(log(0.2)- log(0.8)) + 0.2*(log(0.2)- log(0.1))+ 0.2*(log(0.2)- log(0.05))+0.2*(log(0.2)- log(0.025))+0.2*(log(0.2)-log(0.025))
  
round( Dm , 2 )


```

The way to read this is each row as a model and each column as a true distribution. So the first island, the first row, has the smaller distances to the other islands. This makes sense, since it has the highest entropy. Why does that give it a shorter distance to the other islands? Because it is less surprised by the other islands, due to its high entropy.

##2. 
Recall the marriage,age,and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree? 

SOL: So let’s consider a multiple regression model aimed at inferring the influence of age on happiness, while controlling for marriage status. This is just a plain multiple regression, like the others in this and the previous chapter. The linear model is this: μi = α mid[i] + βAAi where mid[i] is an index for the marriage status of individual i, with 1 meaning single and 2 meaning married. It’s easier to make priors, when we use multiple intercepts, one for each category, than when we use indicator variables.

Now we should do our duty and think about the priors. 
-Let’s consider the slope βA first, because how we scale the predictor A will determine the meaning of the intercept. We’ll focus only on the adult sample, those 18 or over. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. It’ll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. 
-Happiness is on an arbitrary scale, in these data, from −2 to +2. So our imaginary strongest relationship, taking happiness from maximum to minimum, has a slope with rise over run of (2 − (−2))/1 = 4. Remember that 95% of the mass of a normal distribution is contained within 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we are saying that we expect 95% of plausible slopes to be less than maximally strong. That isn’t a very strong prior, but again, it at least helps bound inference to realistic ranges. 
-Now for the intercepts. Each α is the value of μi when Ai = 0. In this case, that means at age 18. So we need to allow α to cover the full range of happiness scores. Normal(0, 1) will put 95% of the mass in the −2 to +2 interval.
-We need to construct the marriage status index variable, as well.
Finally, let’s approximate the posterior.  

```{r hw4.2 rethinking data}
d <- sim_happiness( seed=1977 , N_years=1000 )
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )
d2$mid <- d2$married + 1
```

Model m6.9 contains both marriage status and age. Model m6.10 contains only age. Model m6.9 produces a confounded inference about the relationship between age and happiness, due to opening a collider path. To compare these models using WAIC:

```{r hw4.2.2 m6.9 rethinking}
#contains both marriage status and age

m6.9 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a[mid] + bA*A,
        a[mid] ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.9,depth=2)

```

In order to code a separate intercept for being married/not, we need to reformat the data

```{r hw4.2.2 m6.9 INLA}

d3 <- d2 %>% 
  mutate(i_notmarried= na_if(if_else(married==0, 1, 0), 0), 
         i_married= na_if(married, 0)) 

m6.9.inla.prior <- inla(happiness~ -1 +i_notmarried+i_married + A , data= d3, control.fixed = list(
        mean= list(0), 
        prec= list(i_notmarried=2, i_married= 1, A=2)
))
summary(m6.9.inla.prior )

```

```{r hw4.2.2 m6.10 rethinking}

#this model to a model that omits marriage status.
m6.10 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.10)

 compare( m6.9 , m6.10 )
```


3. Reconsider the urban fox analysis from last week’s homework. Use WAIC or LOO based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:
(1) avgfood + groupsize + area 
(2) avgfood + groupsize
(3) groupsize + area
(4) avgfood
(5) area
Can you explain the relative differences in WAIC scores, using the fox DAG from last week’s homework? Be sure to pay attention to the standard error of the score differences (dSE).


```{r hw4 3}

library(rethinking)
data(foxes)
d <- foxes
d$W <- standardize(d$weight)
d$A <- standardize(d$area)
d$F <- standardize(d$avgfood)
d$G <- standardize(d$groupsize)
m1 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G + bA*A,
        a ~ dnorm(0,0.2),
        c(bF,bG,bA) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
m2 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F + bG*G,
        a ~ dnorm(0,0.2),
        c(bF,bG) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
m3 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bG*G + bA*A,
        a ~ dnorm(0,0.2),
        c(bG,bA) ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
m4 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bF*F,
        a ~ dnorm(0,0.2),
        bF ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=d )
m5 <- quap(
    alist(
        W ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm(0,0.2),
        bA ~ dnorm(0,0.5),
        sigma ~ dexp(1)
), data=d )


 compare( m1 , m2 , m3 , m4 , m5 )

```

avgfood is a confounder of groupsize --> weight, so models that include avgfood and groupsize are causally correct 


(1) avgfood + groupsize + area: 
 don't really need ara 


(2) avgfood + groupsize

(3) groupsize + area
 confounded 
(4) avgfood
(5) area