---
title: "Statistical Rethinking 2nd edition Homework 5 in INLA"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r libraries, message= FALSE}
library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)
```

# HOMEWORK 5

## 1. 
**Consider the data (Wines2012) data table.These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model score, the subjective rating assigned by each judge to each wine. I recommend standardizing it.**

In this first problem, consider only variation among judges and wines. Construct index variables of judge and wine and then use these index variables to construct a linear regression model. Justify your priors. You should end up with 9 judge parameters and 20 wine parameters. Use ulam instead of quap to build this model, and be sure to check the chains for convergence. If you’d rather build the model directly in Stan or PyMC3, go ahead. I just want you to use Hamiltonian Monte Carlo instead of quadratic approximation.
How do you interpret the variation among individual judges and individual wines? Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/ best on average?

```{r HW5 1.1}
library(rethinking)
data(Wines2012)
d <- Wines2012

dat_list <- list(
    S = standardize(d$score),
    jid = as.integer(d$judge),
    wid = as.integer(d$wine)
)

str(dat_list)

```

The model is straightforward. The only issue is the priors. Since I’ve standardized the outcome, we can use the ordinary N(0,0.5) prior from the examples in the text with standardized outcomes. Then the prior outcomes will stay largely within the possible outcome space. A bit more regularization than that wouldn’t be a bad idea either.

diagnostics that precis provides: The n_eff values are all actually higher than the number of samples (2000), and all the Rhat values at exactly 1. Looks good so far. These diagnostics can mislead, however, so let’s look at the trace plots too: These pass the hairy-caterpillar-ocular-inspection-test: all the chains mix in the same region, and they move quickly through it, not getting stuck anyplace. 
Now let’s plot these parameters so they are easier to interpret:

### 1. rethinking
```{r hw5.1 re}

#How do you interpret the variation among individual judges and indi- vidual wines?
#Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/ best on average?

m1 <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a[jid] + b[wid], 
    a[jid] ~ dnorm(0, 0.5), 
    b[wid] ~ dnorm(0, 0.5), 
    sigma ~dexp(1)
    
  ), data=dat_list, chains=4 , cores=4)

precis(m1, 2)
traceplot(m1)

plot(precis(m1, 2))
```

### 1. INLA

In order to code a separate intercept for each judge and wine, we need to reformat the data so that there are separate variables for each intercept, with 1s for a given value of the variable we are basing the intercept on and NAs for all other values. 

```{r hw5.1 INLA}

d.i <- d %>% 
  mutate(S = standardize(d$score),
    jid = paste("j",as.integer(d$judge), sep= "."),
    wid =  paste("w",as.integer(d$wine), sep= "."), 
    j.value= 1, 
    w.value= 1) %>% 
  spread(jid, j.value) %>% 
  spread(wid, w.value)

j <- paste("j", 1:9, sep=".")

w <- paste("w", 1:20, sep=".")

est <- c(j,w)

m1.i<- inla(S~ -1 +j.1+ j.2+j.3+j.4+j.5+j.6+j.7+j.8+j.9+
                          w.1+w.2+ w.3+w.4+w.5+w.6+w.7+w.8+w.9+w.10+w.11+w.12+w.13+w.14+w.15+ w.16+w.17+ w.18+w.19+w.20, data= d.i,
                        control.fixed = list(
        mean= 0, 
        prec= 0.5),
        control.compute = list(dic=TRUE, waic= TRUE)
)

summary(m1.i )

## Pull out summaries from the model object
m1.i.sum <- summary(m1.i)
 
## Summarise results
m1.i.df  <- data.frame(mean = m1.i.sum$fixed[,"mean"],
                       lower = m1.i.sum$fixed[,"0.025quant"],
                       upper = m1.i.sum$fixed[,"0.975quant"],
                       stringsAsFactors = FALSE)

mi.i.df <- bind_cols(as.factor(est), m1.i.df)

m1.i.plot <- mi.i.df %>% 
  mutate(est= factor(est, levels= c("j.1",  "j.2" , "j.3" , "j.4" , "j.5"  ,"j.6" , "j.7" , "j.8" , "j.9" , "w.1" , "w.2" , "w.3" , "w.4" , "w.5" , "w.6" , "w.7" ,"w.8" , "w.9"  ,"w.10" ,"w.11", "w.12" ,"w.13" ,"w.14", "w.15" ,"w.16", "w.17", "w.18", "w.19", "w.20"))) %>% 
  ggplot() + 
    geom_pointrange(aes(x = factor(est), y = mean, ymin = lower, ymax = upper), position = position_dodge(0.5)) +
    xlab("coefficient") +
    ylab("Estimate")+
  coord_flip()+
    theme_bw()

m1.i.plot 
```

The a/j parameters are the judges. Each represents an average deviation of the scores. So judges with lower values are harsher on average. Judges with higher values liked the wines more on average. There is some noticeable variation here. It is fairly easy to tell the judges apart.

The w parameters are the wines. Each represents an average score across all judges. Except for wine 18 (a New Jersey red I think), there isn’t that much variation. These are good wines, after all. Overall, there is more variation from judge than from wine.

## 2.
**Now consider three features of the wines and judges:**

1. flight: Whether the wine is red or white.

2. wine.amer: Indicator variable for American wines. 

3. judge.amer: Indicator variable for American judges.

**Use indicator or index variables to model the influence of these features on the scores. Omit the individual judge and wine index variables from Problem 1. Do not include interaction effects yet. Again use ulam, justify your priors, and be sure to check the chains. What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in Problem 1.**

The easiest way to code the data is to use indicator variables. Let’s look at that approach first. I’ll do an index variable version next. I’ll use the three indicator variables W (NJ wine), J (American NJ), and R (red wine).

## 2.a indicator variables

### 2.a rethinking
```{r HW5 2a re}

dat_list2 <- list(
    S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    F= if_else(d$flight == "white", 0, 1))

m2a <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a + bW*W + bJ*J + bF*F, 
    a ~ dnorm(0, 0.2), 
    bW~ dnorm(0, 0.5),
    bJ~ dnorm(0, 0.5),
    bF~ dnorm(0, 0.5),
  sigma ~ dexp(1)
    ), data=dat_list2, chains=4 , cores=4)

precis(m2a)
traceplot(m2a)
plot(precis(m2a, 2))


```

### 2.a INLA

```{r HW5 2a INLA}


d.i2a <- d %>% 
  mutate(S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0))

m2a.i <- inla(S~W+J+R, data= d.i2a, control.fixed = list(
        mean= 0, 
        prec= list(R=0.5, W=0.5, J= 0.5), 
        mean.intercept= 0, 
        prec.intercept= 0.2
), 
control.compute = list(waic= TRUE))

summary(m2a.i)


## Pull out summaries from the model object
m1.i.sum <- summary(m1.i)
 
## Summarise results
m1.i.df  <- data.frame(mean = m1.i.sum$fixed[,"mean"],
                       lower = m1.i.sum$fixed[,"0.025quant"],
                       upper = m1.i.sum$fixed[,"0.975quant"],
                       stringsAsFactors = FALSE)

mi.i.df <- bind_cols(as.factor(est), m1.i.df)

m1.i.plot <- mi.i.df %>% 
  mutate(est= factor(est, levels= c("j.1",  "j.2" , "j.3" , "j.4" , "j.5"  ,"j.6" , "j.7" , "j.8" , "j.9" , "w.1" , "w.2" , "w.3" , "w.4" , "w.5" , "w.6" , "w.7" ,"w.8" , "w.9"  ,"w.10" ,"w.11", "w.12" ,"w.13" ,"w.14", "w.15" ,"w.16", "w.17", "w.18", "w.19", "w.20"))) %>% 
  ggplot() + 
    geom_pointrange(aes(x = factor(est), y = mean, ymin = lower, ymax = upper), position = position_dodge(0.5)) +
    xlab("coefficient") +
    ylab("Estimate")+
  coord_flip()+
    theme_bw()

m1.i.plot 

```
As expected, red and wines are on average the same—bR is right on top of zero. American judges seem to be more on average slightly more generous with ratings—bJ is slightly but reliably above zero. American wines have slightly lower average ratings than French wines—bW is mostly below zero, but not very large in absolute size.

## 2.b index variables

Okay, now for an index variable version. The thing about index variables is that you can easily end up with more parameters than in an equivalent indicator variable model. But it’s still the same posterior distribution. You can convert from one to the other (if the priors are also equivalent). 

We’ll need three index variables: wid, jid, fid. Now wid is 1 for a French wine and 2 for a NJ wine,jid is 1 for a French judge and 2 for an American judge, and fid is 1 for red and 2 for white. Those 1L numbers are just the R way to type the number as an integer—“1L” is the integer 1, while “1” is the real number 1. We want integers for an index variable.

Now let’s think about priors for the parameters that correspond to each index value. Now the question isn’t how big the difference could be, but rather how far from the mean an indexed category could be. If we use Normal(0,0.5) priors, that would make a full standard deviation difference from the global mean rare. It will also match what we had above, in a crude sense. Again, I’d be tempted to something narrow, for the sake of regularization. But certainly something like Normal(0,10) is flat out silly, because it makes impossible values routine. Let’s see what we get:


### 2.b rethinking

```{r hw5.2b index}

dat_list2b <- list(
    S = standardize(d$score),
    wid = d$wine.amer + 1,
    jid = d$judge.amer + 1, 
    fid= if_else(d$flight=="red",1L,2L)
    )

m2b <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- w[wid]+j[jid]+f[fid], 
    w[wid]~ dnorm(0, 0.5),
    j[wid]~ dnorm(0, 0.5),
    f[wid]~ dnorm(0, 0.5),
  sigma ~ dexp(1)
    ), data=dat_list2b, chains=4 , cores=4
  )

precis(m2b, depth=2)
```


To see that this model is the same as the previous, let’s compute contrasts. The contrast between American and French wines is:

```{r hw5 2 contrast}

post <- extract.samples(m2b)
diff_w <- post$w[,2] - post$w[,1]
precis( diff_w )

```

That’s almost exactly the same mean and standard deviation as bW in the first model. The other contrasts match as well.

Something to notice about the two models is that the second one does sample less efficiently. The n_eff values are lower. This isn’t a problem, but it is a consequence of the higher correlations in the posterior, a result of the redundant parameterization. If you look at the pairs(m2b) plot, you’ll see tight correlations for each pair of index parameters of the same type. This is because really it is a difference that matters, and many combinations of two numbers can produce the same difference. But the priors keep this from ruining our inference. if you tried the same thing without priors, it would likely fall apart and return very large standard errors.

```{r hw5 2b }
pairs(m2b)
```

### 2.b INLA 

Note that here we have used the selection argument to keep just the sampled values of the coefficients of wine.amer.no and wine.amer.yes. Note that the expression wine.amer.no = 1 means that we want to keep the first element in effect wine.amer.no. Note that in this case there is only a single value associated with wine.amer.no (i.e., the coefficient) but this can be extended to other latent random effects with possibly more values.

The object returned by inla.posterior.sample() is a list of length 100, where each element contains the samples of the different effects in the model in a name list.

Finally, function inla.posterior.sample.eval() is used to compute the product of the two coefficients. The output is a matrix with 1 row and 100 columns so that summary statistics can be computed from the posterior as usual:

```{r hw5.2b INLA}

d.i2b <- d %>% 
  mutate(S = standardize(d$score),
   wine.amer.no= na_if(if_else(wine.amer==0, 1, 0), 0), 
         wine.amer.yes= na_if(wine.amer, 0), 
         judge.amer.no= na_if(if_else(judge.amer==0, 1, 0), 0), 
         judge.amer.yes= na_if(judge.amer, 0), 
         f.red= na_if(if_else(flight=="red", 1, 0), 0), 
   f.white= na_if(if_else(flight=="white", 1, 0), 0)
         ) 


m2b.i<- inla(S~ -1 + wine.amer.no + wine.amer.yes + judge.amer.no+ judge.amer.yes + f.red + f.white, data= d.i2b,
                        control.fixed = list(
        mean= 0, 
        prec= 0.5),
        control.compute = list(
          dic=TRUE, waic= TRUE, config=TRUE)
)

summary(m2b.i )

## Pull out summaries from the model object
m2b.i.sum <- summary(m2b.i)
 
m2b.i.samp <- inla.posterior.sample(100, m2b.i , selection = list(wine.amer.no = 1, wine.amer.yes = 1))

m2b.i.contrast <- inla.posterior.sample.eval(function(...) {wine.amer.yes - wine.amer.no},
   m2b.i.samp)

summary(as.vector(m2b.i.contrast))

```

## 3.
**Now consider two-way interactions among the three features.You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator variables. Again use ulam, justify your priors, and be sure to check the chains. Explain what each interaction means. Be sure to interpret the model’s predictions on the outcome scale (mu, the expected score), not on the scale of individual parameters. You can use link to help with this, or just use your knowledge of the linear model instead.
What do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from Problem 1?**

Again I’ll show both the indicator approach and the index variable approach.

## 3.a indicator variables

### 3.a rethinking

For the indicator approach, we can use the same predictor variables as before, it's the model that's different. 
I used the same priors as before for the main effects. I used tighter priors for the interactions. Why? Because interactions represent sub-categories of data, and if we keep slicing up the sample, differences can’t keep getting bigger. Again, the most important thing is not to use flat priors like Normal(0,10) that produce impossible outcomes.


```{r hw5.3a rethinking}

library(rethinking)
data(Wines2012)
d <- Wines2012

dat_list2 <- list(
    S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0))

m3a <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a + bW*W + bJ*J + bR*R + 
      bWJ*W*J + bWR*W*R + bJR*J*R, 
    a ~ dnorm(0, 0.2), 
   c(bW,bJ,bR) ~ dnorm(0,0.5),
    c(bWJ,bWR,bJR) ~ dnorm(0,0.25),
  sigma ~ dexp(1)
    ), data=dat_list2, chains=4 , cores=4)

precis(m3a)

```

Reading the parameters this way is not easy. But right away you might notice that bW is now close to zero and overlaps it a lot on both sides. NJ wines are no longer on average worse. So the interactions did something. Glancing at the interaction parameters, you can see that only one of them has much mass away from zero, bWR, the interaction between NJ wines and red flight, so red NJ wines. To get the predicted scores for red and white wines from both NJ and France, for both types of judges, we can use link:

```{r hw5 3.a plot}
pred_dat <- data.frame(
    W = rep( 0:1 , times=4 ),
    J = rep( 0:1 , each=4 ),
    R = rep( c(0,0,1,1) , times=2 )
)
mu <- link( m3a )

row_labels <- paste( ifelse(pred_dat$W==1,"A","F") ,
                 ifelse(pred_dat$J==1,"A","F") ,
                 ifelse(pred_dat$R==1,"R","W") , sep="" )

plot( precis( list(mu=mu) , 2 ) , labels=row_labels )
```


### 3.a INLA

```{r hw5.3a inla}
library(INLA)
library(rethinking)
library(tidyverse)
data(Wines2012)
d <- Wines2012

#we want to predict the score for each combination of factors.
pred_dat <- data.frame(S= NA,
    W = rep( 0:1 , times=4 ),
    J = rep( 0:1 , each=4 ),
    R = rep( c(0,0,1,1) , times=2 )
    )

d.i3a <- d %>% 
  mutate(S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0)) %>% 
  select(c("S", "W", "J", "R")) %>% 
  rbind(pred_dat)
  

#indices of the scores with missing values 
d.i.3na <- which(is.na(d.i3a$S))

m3a.i <- inla(S~W+J+R+ W*J + W*R + J*R, data= d.i3a, 
              
              control.fixed = list(
        mean= 0, 
        prec= list(R=1/(0.5^2), W=1/(0.5^2), J= 1/(0.5^2), WJ = 1/(0.25^2), WR=1/(0.25^2), JR= 1/(0.25^2)), 
        mean.intercept= 0, 
        prec.intercept= 1/(0.2^2)
), 
control.compute = list(config= TRUE, waic= TRUE),
control.predictor=list(compute=TRUE),
)

summary(m3a.i)

#names of predicted scores
row_labels <- paste( ifelse(pred_dat$W==1,"Aw","Fw") ,
ifelse(pred_dat$J==1,"Aj","Fj") ,
ifelse(pred_dat$R==1,"Red","Wh") , sep="" )

m3a.i.postmean <- bind_cols(label= row_labels, m3a.i$summary.linear.predictor[d.i.3na, ]) %>% 
  select(c("label", "mean", "sd", "0.025quant",  "0.975quant"))

names(m3a.i.postmean) <- c("label", "mean", "sd", "LCI", "UCI")

m3a.i.postmean.plot <-  ggplot(data= m3a.i.postmean, aes(y=label, x=mean, label=label)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=LCI, xmax=UCI), height=.3) +
    coord_fixed(ratio=.3) +
    geom_vline(xintercept=0, linetype='longdash') +
    theme_bw()

m3a.i.postmean.plot

```


## 3.b index variables
Now let’s do an index version. The way to think of this is to make unique parameters for each combination. If we consider all the interactions—including a three-way interaction between nation, judge and flight—there would be 8 combinations and so 8 parameters to estimate. Let’s go ahead and do that, so we can simultaneously consider the 3-way interaction.

### 3.b rethinking

There are several ways to go about coding this. I’m going to use a trick and make an array of parameters. An array is like a matrix, but can have more than 2 dimensions. If we make a 2-by-2-by-2 array of parameters, then there will be 8 parameters total and we can access each by just using these index variables.

```{r hw5.3b , eval=FALSE}
dat_list2b <- list(
    S = standardize(d$score),
    wid = d$wine.amer + 1,
    jid = d$judge.amer + 1, 
    fid= if_else(d$flight=="red",1L,2L)
    )

d3 <- data.frame(dat_list2b)
```

** in rethinking::ulam** 
The '2,2,2' literal is not very elegant, so I’m likely to improve this is a later version. Either way, the result is displayed on the next page (for sake of line breaks).
```{r in ulam}

#in ulam 

m3b <- ulam(
    alist(
        S ~ dnorm( mu , sigma ),
        mu <- w[wid,jid,fid],
        real['2,2,2']:w ~ normal(0,0.5),
        sigma ~ dexp(1)
    ), data=dat_list2b , chains=4 , cores=4 )

precis(m3b, depth=3)

row_labels = c("FFR","FFW","FAR","FAW","AFR","AFW","AAR","AAW" ) 
plot( precis( m3b , 3 , pars="w" ) , labels=row_labels )

```

### 3.b INLA 

Note that here we have used the selection argument to keep just the sampled values of the coefficients of wine.amer.no and wine.amer.yes. Note that the expression wine.amer.no = 1 means that we want to keep the first element in effect wine.amer.no. Note that in this case there is only a single value associated with wine.amer.no (i.e., the coefficient) but this can be extended to other latent random effects with possibly more values.

The object returned by inla.posterior.sample() is a list of length 100, where each element contains the samples of the different effects in the model in a name list.

Finally, function inla.posterior.sample.eval() is used to compute the product of the two coefficients. The output is a matrix with 1 row and 100 columns so that summary statistics can be computed from the posterior as usual:

```{r hw5.3b INLA}
library(INLA)
library(rethinking)
library(tidyverse)
data(Wines2012)
d <- Wines2012

#names of predicted scores
row_labels <- paste( ifelse(pred_dat$W==1,"Aw","Fw") ,
ifelse(pred_dat$J==1,"Aj","Fj") ,
ifelse(pred_dat$R==1,"Red","Wh") , sep="" )

d3b.i <- d %>% 
  mutate(S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0)) %>% 
  mutate(index= paste(ifelse(W==1,"Aw","Fw") ,
ifelse(J==1,"Aj","Fj") ,
ifelse(R==1,"Red","Wh"), sep="" ), 
value= 1) %>% 
  spread(index, value)
  
         
labels <- paste( ifelse(d3b.i$W==1,"Aw","Fw"),
ifelse(d3b.i$J==1,"Aj","Fj") ,
ifelse(d3b.i$R==1,"Red","Wh") , sep="" )


m3b.i<- inla(S~ -1 + FwFjRed +FwFjWh+ FwAjRed+ FwAjWh + AwFjRed + AwFjWh +AwAjRed + AwAjWh, data= d3b.i,
                        control.fixed = list(
        mean= 0, 
        prec= 1/(0.5^2)),
        control.compute = list(
          dic=TRUE, waic= TRUE, config=TRUE)
)

summary(m3b.i)


m3b.i.postmean <- m3b.i$summary.fixed %>% 
  rownames_to_column( "label") %>% 
  select(c("label", "mean", "sd", "0.025quant",  "0.975quant"))

names(m3b.i.postmean) <- c("label", "mean", "sd", "LCI", "UCI")

m3b.i.postmean.plot <-  ggplot(data= m3b.i.postmean, aes(y=label, x=mean, label=label)) +
    geom_point(size=4, shape=19) +
    geom_errorbarh(aes(xmin=LCI, xmax=UCI), height=.3) +
    coord_fixed(ratio=.3) +
    geom_vline(xintercept=0, linetype='longdash') +
    theme_bw()

m3b.i.postmean.plot


```
The previous predictions, for comparison:

```{r inla 3a}
m3a.i.postmean.plot
```
The most noticeable change is that FFW (French wines, French judges, white) have a lower expected rating in the full interaction model. There are some other minor differences as well. What has happened? The three way interaction would be, in the first model’s indicator terms, when a wine is American, the judge is American, and the flight is red. In the first model, a prediction for such a wine is just a sum of parameters:
μi =α+βW +βJ +βR +βWJ +βWR +βJR
This of course limits means that these parameters have to account for the AAR wine. In the full interaction mode, an AAR wine gets its own parameter, as does every other combination. None of the parameters get polluted by averaging over different combinations. Of course, there isn’t a lot of evidence that prediction is improved much by allowing this extra parameter. The differences are small, overall. These wines are all quite good. But it is worth understand how the full interaction model gains additional flexibility. This additional flexibility typically requires some addition regularization. When we arrive at multilevel models later, you’ll see how we can handle regularization more naturally inside of a model.
