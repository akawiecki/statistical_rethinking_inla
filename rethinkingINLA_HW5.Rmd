---
title: "rethinkingINLA_HW5"
author: "Ania Kawiecki"
date: "8/3/2020"
output: html_document
---

[Homework Solutions of Statistical Rethinking by Richard McElreath](https://github.com/rmcelreath/statrethinking_winter2019/tree/master/homework)

[Bayesian inference with INLA by Virgilio Gómez-Rubio](https://becarioprecario.bitbucket.io/inla-gitbook/ch-intro.html)

```{r libraries, message= FALSE}
library(tidyverse)
library(rethinking)
library(dagitty)
library(INLA)
library(knitr)
library(stringr)
```

# HOMEWORK 5

## 1. 
Consider the data (Wines2012) data table.These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model score, the subjective rating assigned by each judge to each wine. I recommend standardizing it.

In this first problem, consider only variation among judges and wines. Construct index variables of judge and wine and then use these index variables to construct a linear regression model. Justify your priors. You should end up with 9 judge parameters and 20 wine parameters. Use ulam instead of quap to build this model, and be sure to check the chains for convergence. If you’d rather build the model directly in Stan or PyMC3, go ahead. I just want you to use Hamiltonian Monte Carlo instead of quadratic approximation.
How do you interpret the variation among individual judges and individual wines? Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/ best on average?

```{r HW5 1.1}
library(rethinking)
data(Wines2012)
d <- Wines2012

dat_list <- list(
    S = standardize(d$score),
    jid = as.integer(d$judge),
    wid = as.integer(d$wine)
)

str(dat_list)

```

The model is straightforward. The only issue is the priors. Since I’ve standardized the outcome, we can use the ordinary N(0,0.5) prior from the examples in the text with standardized outcomes. Then the prior outcomes will stay largely within the possible outcome space. A bit more regularization than that wouldn’t be a bad idea either.

diagnostics that precis provides: The n_eff values are all actually higher than the number of samples (2000), and all the Rhat values at exactly 1. Looks good so far. These diagnostics can mislead, however, so let’s look at the trace plots too: These pass the hairy-caterpillar-ocular-inspection-test: all the chains mix in the same region, and they move quickly through it, not getting stuck anyplace. 
Now let’s plot these parameters so they are easier to interpret:

### 1. rethinking
```{r hw5.1 re}

#How do you interpret the variation among individual judges and indi- vidual wines?
#Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/ best on average?

m1 <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a[jid] + b[wid], 
    a[jid] ~ dnorm(0, 0.5), 
    b[wid] ~ dnorm(0, 0.5), 
    sigma ~dexp(1)
    
  ), data=dat_list, chains=4 , cores=4)

precis(m1, 2)
traceplot(m1)

plot(precis(m1, 2))
```

### 1. INLA

In order to code a separate intercept for each judge and wine, we need to reformat the data so that there are separate variables for each intercept, with 1s for a given value of the variable we are basing the intercept on and NAs for all other values. 

```{r hw5.1 INLA}

d.i <- d %>% 
  mutate(S = standardize(d$score),
    jid = paste("j",as.integer(d$judge), sep= "."),
    wid =  paste("w",as.integer(d$wine), sep= "."), 
    j.value= 1, 
    w.value= 1) %>% 
  spread(jid, j.value) %>% 
  spread(wid, w.value)

j <- paste("j", 1:9, sep=".")

w <- paste("w", 1:20, sep=".")

est <- c(j,w)

m1.i<- inla(S~ -1 +j.1+ j.2+j.3+j.4+j.5+j.6+j.7+j.8+j.9+
                          w.1+w.2+ w.3+w.4+w.5+w.6+w.7+w.8+w.9+w.10+w.11+w.12+w.13+w.14+w.15+ w.16+w.17+ w.18+w.19+w.20, data= d.i,
                        control.fixed = list(
        mean= 0, 
        prec= 0.5),
        control.compute = list(dic=TRUE, waic= TRUE)
)

summary(m1.i )

## Pull out summaries from the model object
m1.i.sum <- summary(m1.i)
 
## Summarise results
m1.i.df  <- data.frame(mean = m1.i.sum$fixed[,"mean"],
                       lower = m1.i.sum$fixed[,"0.025quant"],
                       upper = m1.i.sum$fixed[,"0.975quant"],
                       stringsAsFactors = FALSE)

mi.i.df <- bind_cols(as.factor(est), m1.i.df)

m1.i.plot <- mi.i.df %>% 
  mutate(est= factor(est, levels= c("j.1",  "j.2" , "j.3" , "j.4" , "j.5"  ,"j.6" , "j.7" , "j.8" , "j.9" , "w.1" , "w.2" , "w.3" , "w.4" , "w.5" , "w.6" , "w.7" ,"w.8" , "w.9"  ,"w.10" ,"w.11", "w.12" ,"w.13" ,"w.14", "w.15" ,"w.16", "w.17", "w.18", "w.19", "w.20"))) %>% 
  ggplot() + 
    geom_pointrange(aes(x = factor(est), y = mean, ymin = lower, ymax = upper), position = position_dodge(0.5)) +
    xlab("coefficient") +
    ylab("Estimate")+
  coord_flip()+
    theme_bw()

m1.i.plot 
```

The a/j parameters are the judges. Each represents an average deviation of the scores. So judges with lower values are harsher on average. Judges with higher values liked the wines more on average. There is some noticeable variation here. It is fairly easy to tell the judges apart.

The w parameters are the wines. Each represents an average score across all judges. Except for wine 18 (a New Jersey red I think), there isn’t that much variation. These are good wines, after all. Overall, there is more variation from judge than from wine.

## 2.
Now consider three features of the wines and judges:

1. flight: Whether the wine is red or white.

2. wine.amer: Indicator variable for American wines. 

3. judge.amer: Indicator variable for American judges.

Use indicator or index variables to model the influence of these features on the scores. Omit the individual judge and wine index variables from Problem 1. Do not include interaction effects yet. Again use ulam, justify your priors, and be sure to check the chains. What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in Problem 1.

The easiest way to code the data is to use indicator variables. Let’s look at that approach first. I’ll do an index variable version next. I’ll use the three indicator variables W (NJ wine), J (American NJ), and R (red wine).

## 2.a indicator variables

### 2.a rethinking
```{r HW5 2a re}

dat_list2 <- list(
    S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    F= if_else(d$flight == "white", 0, 1))

m2a <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a + bW*W + bJ*J + bF*F, 
    a ~ dnorm(0, 0.2), 
    bW~ dnorm(0, 0.5),
    bJ~ dnorm(0, 0.5),
    bF~ dnorm(0, 0.5),
  sigma ~ dexp(1)
    ), data=dat_list2, chains=4 , cores=4)

precis(m2a)
traceplot(m2a)
plot(precis(m2a, 2))


```

### 2.a INLA

```{r HW5 2a INLA}


d.i2a <- d %>% 
  mutate(S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0))

m2a.i <- inla(S~W+J+R, data= d.i2a, control.fixed = list(
        mean= 0, 
        prec= list(R=0.5, W=0.5, J= 0.5), 
        mean.intercept= 0, 
        prec.intercept= 0.2
), 
control.compute = list(waic= TRUE))

summary(m2a.i)


## Pull out summaries from the model object
m1.i.sum <- summary(m1.i)
 
## Summarise results
m1.i.df  <- data.frame(mean = m1.i.sum$fixed[,"mean"],
                       lower = m1.i.sum$fixed[,"0.025quant"],
                       upper = m1.i.sum$fixed[,"0.975quant"],
                       stringsAsFactors = FALSE)

mi.i.df <- bind_cols(as.factor(est), m1.i.df)

m1.i.plot <- mi.i.df %>% 
  mutate(est= factor(est, levels= c("j.1",  "j.2" , "j.3" , "j.4" , "j.5"  ,"j.6" , "j.7" , "j.8" , "j.9" , "w.1" , "w.2" , "w.3" , "w.4" , "w.5" , "w.6" , "w.7" ,"w.8" , "w.9"  ,"w.10" ,"w.11", "w.12" ,"w.13" ,"w.14", "w.15" ,"w.16", "w.17", "w.18", "w.19", "w.20"))) %>% 
  ggplot() + 
    geom_pointrange(aes(x = factor(est), y = mean, ymin = lower, ymax = upper), position = position_dodge(0.5)) +
    xlab("coefficient") +
    ylab("Estimate")+
  coord_flip()+
    theme_bw()

m1.i.plot 

```
As expected, red and wines are on average the same—bR is right on top of zero. American judges seem to be more on average slightly more generous with ratings—bJ is slightly but reliably above zero. American wines have slightly lower average ratings than French wines—bW is mostly below zero, but not very large in absolute size.

## 2.b index variables

Okay, now for an index variable version. The thing about index variables is that you can easily end up with more parameters than in an equivalent indicator variable model. But it’s still the same posterior distribution. You can convert from one to the other (if the priors are also equivalent). 

We’ll need three index variables: wid, jid, fid. Now wid is 1 for a French wine and 2 for a NJ wine,jid is 1 for a French judge and 2 for an American judge, and fid is 1 for red and 2 for white. Those 1L numbers are just the R way to type the number as an integer—“1L” is the integer 1, while “1” is the real number 1. We want integers for an index variable.

Now let’s think about priors for the parameters that correspond to each index value. Now the question isn’t how big the difference could be, but rather how far from the mean an indexed category could be. If we use Normal(0,0.5) priors, that would make a full standard deviation difference from the global mean rare. It will also match what we had above, in a crude sense. Again, I’d be tempted to something narrow, for the sake of regularization. But certainly something like Normal(0,10) is flat out silly, because it makes impossible values routine. Let’s see what we get:


### 2.b rethinking

```{r hw5.2b index}

dat_list2b <- list(
    S = standardize(d$score),
    wid = d$wine.amer + 1,
    jid = d$judge.amer + 1, 
    fid= if_else(d$flight=="red",1L,2L)
    )

m2b <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- w[wid]+j[jid]+f[fid], 
    w[wid]~ dnorm(0, 0.5),
    j[wid]~ dnorm(0, 0.5),
    f[wid]~ dnorm(0, 0.5),
  sigma ~ dexp(1)
    ), data=dat_list2b, chains=4 , cores=4
  )

precis(m2b, depth=2)
```


To see that this model is the same as the previous, let’s compute contrasts. The contrast between American and French wines is:

```{r hw5 2 contrast}

post <- extract.samples(m2b)
diff_w <- post$w[,2] - post$w[,1]
precis( diff_w )

```

That’s almost exactly the same mean and standard deviation as bW in the first model. The other contrasts match as well.

Something to notice about the two models is that the second one does sample less efficiently. The n_eff values are lower. This isn’t a problem, but it is a consequence of the higher correlations in the posterior, a result of the redundant parameterization. If you look at the pairs(m2b) plot, you’ll see tight correlations for each pair of index parameters of the same type. This is because really it is a difference that matters, and many combinations of two numbers can produce the same difference. But the priors keep this from ruining our inference. if you tried the same thing without priors, it would likely fall apart and return very large standard errors.

```{r hw5 2b }
pairs(m2b)
```

### 2.b INLA 

Note that here we have used the selection argument to keep just the sampled values of the coefficients of wine.amer.no and wine.amer.yes. Note that the expression wine.amer.no = 1 means that we want to keep the first element in effect wine.amer.no. Note that in this case there is only a single value associated with wine.amer.no (i.e., the coefficient) but this can be extended to other latent random effects with possibly more values.

The object returned by inla.posterior.sample() is a list of length 100, where each element contains the samples of the different effects in the model in a name list.

Finally, function inla.posterior.sample.eval() is used to compute the product of the two coefficients. The output is a matrix with 1 row and 100 columns so that summary statistics can be computed from the posterior as usual:

```{r hw5.2b INLA}

d.i2b <- d %>% 
  mutate(S = standardize(d$score),
   wine.amer.no= na_if(if_else(wine.amer==0, 1, 0), 0), 
         wine.amer.yes= na_if(wine.amer, 0), 
         judge.amer.no= na_if(if_else(judge.amer==0, 1, 0), 0), 
         judge.amer.yes= na_if(judge.amer, 0), 
         f.red= na_if(if_else(flight=="red", 1, 0), 0), 
   f.white= na_if(if_else(flight=="white", 1, 0), 0)
         ) 


m2b.i<- inla(S~ -1 + wine.amer.no + wine.amer.yes + judge.amer.no+ judge.amer.yes + f.red + f.white, data= d.i2b,
                        control.fixed = list(
        mean= 0, 
        prec= 0.5),
        control.compute = list(
          dic=TRUE, waic= TRUE, config=TRUE)
)

summary(m2b.i )

## Pull out summaries from the model object
m2b.i.sum <- summary(m2b.i)
 
m2b.i.samp <- inla.posterior.sample(100, m2b.i , selection = list(wine.amer.no = 1, wine.amer.yes = 1))

m2b.i.contrast <- inla.posterior.sample.eval(function(...) {wine.amer.yes - wine.amer.no},
   m2b.i.samp)

summary(as.vector(m2b.i.contrast))

```

# 3.
**Now consider two-way interactions among the three features.You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator variables. Again use ulam, justify your priors, and be sure to check the chains. Explain what each interaction means. Be sure to interpret the model’s predictions on the outcome scale (mu, the expected score), not on the scale of individual parameters. You can use link to help with this, or just use your knowledge of the linear model instead.
What do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from Problem 1?**

Again I’ll show both the indicator approach and the index variable approach.

## 3.a indicator variables

### 3.a rethinking

For the indicator approach, we can use the same predictor variables as before, it's the model that's different. 
I used the same priors as before for the main effects. I used tighter priors for the interactions. Why? Because interactions represent sub-categories of data, and if we keep slicing up the sample, differences can’t keep getting bigger. Again, the most important thing is not to use flat priors like Nor- mal(0,10) that produce impossible outcomes.


```{r hw5.3a rethinking}

library(rethinking)
data(Wines2012)
d <- Wines2012

dat_list2 <- list(
    S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0))

m3a <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a + bW*W + bJ*J + bR*R + 
      bWJ*W*J + bWR*W*R + bJR*J*R, 
    a ~ dnorm(0, 0.2), 
   c(bW,bJ,bR) ~ dnorm(0,0.5),
    c(bWJ,bWR,bJR) ~ dnorm(0,0.25),
  sigma ~ dexp(1)
    ), data=dat_list2, chains=4 , cores=4)

precis(m3a)

```

Reading the parameters this way is not easy. But right away you might notice that bW is now close to zero and overlaps it a lot on both sides. NJ wines are no longer on average worse. So the interactions did something. Glancing at the interaction parameters, you can see that only one of them has much mass away from zero, bWR, the interaction between NJ wines and red flight, so red NJ wines. To get the predicted scores for red and white wines from both NJ and France, for both types of judges, we can use link:

```{r hw5 3.a plot}
pred_dat <- data.frame(
    W = rep( 0:1 , times=4 ),
    J = rep( 0:1 , each=4 ),
    R = rep( c(0,0,1,1) , times=2 )
)
mu <- link( m3a )

row_labels <- paste( ifelse(pred_dat$W==1,"A","F") ,
                 ifelse(pred_dat$J==1,"A","F") ,
                 ifelse(pred_dat$R==1,"R","W") , sep="" )

plot( precis( list(mu=mu) , 2 ) , labels=row_labels )
```


### 3.a INLA

```{r hw5.3a inla}
d.i3a <- d %>% 
  mutate(S = standardize(d$score),
    W = d$wine.amer,
    J = d$judge.amer,
    R= if_else(d$flight == "red", 1, 0))

m3a.i <- inla(S~W+J+R+ W*J + W*R + J*R, data= d.i3a, 
              
              control.fixed = list(
        mean= 0, 
        prec= list(R=0.5, W=0.5, J= 0.5, WJ = 0.25, WR=0.25, JR= 0.25), 
        mean.intercept= 0, 
        prec.intercept= 0.2
), 
control.compute = list(waic= TRUE))

summary(m3a.i)

#m3a.i.predicted.scores <- 
```

Overthinking: How link works. The function link is not really very sophisticated. All it is doing is using the formula you provided when you fit the model to compute the value of the linear model. It does this for each sample from the posterior distribution, for each case in the data. You could accomplish the same thing for any model, fit by any means, by performing these steps yourself. This is how it’d look for m4.3.
```{r code 4.58, eval=FALSE}
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b*( weight - xbar ) weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link )
mu.mean <- apply( mu , 2 , mean )
mu.CI <- apply( mu , 2 , PI , prob=0.89 )

```
And the values in mu.mean and mu.CI should be very similar (allowing for simulation variance) to what you got the automated way, using link.
Knowing this manual method is useful both for (1) understanding and (2) sheer power. What- ever the model you find yourself with, this approach can be used to generate posterior predictions for any component of it. Automated tools like link save effort, but they are never as flexible as the code you can write yourself.



```{r hw5.3b , eval=FALSE}
dat_list2b <- list(
    S = standardize(d$score),
    wid = d$wine.amer + 1,
    jid = d$judge.amer + 1, 
    fid= if_else(d$flight=="red",1L,2L)
    )

# define an array
# in STAN
 mcode <- "
data{
    vector[180] S;


m2b <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- w[wid]+j[jid]+f[fid], 
    w[wid]~ dnorm(0, 0.5),
    j[wid]~ dnorm(0, 0.5),
    f[wid]~ dnorm(0, 0.5),
  sigma ~ dexp(1)
    ), data=dat_list2b, chains=4 , cores=4
  )
   int fid[180];
    int jid[180];
    int wid[180];
}
parameters{
    real w[2,2,2];
    real<lower=0> sigma;
}
model{
    vector[180] mu;
    sigma ~ exponential( 1 );
    for ( i in 1:2 )
        for ( j in 1:2 )
            for ( k in 1:2 )
                w[i,j,k] ~ normal( 0 , 0.5 );
    for ( i in 1:180 ) {
        mu[i] = w[wid[i], jid[i], fid[i]];
    }
    S ~ normal( mu , sigma );
}
"


#in ulam 

m3b <- ulam(
    alist(
        S ~ dnorm( mu , sigma ),
        mu <- w[wid,jid,fid],
        real['2,2,2']:w ~ normal(0,0.5),
        sigma ~ dexp(1)
    ), data=dat_list2b , chains=4 , cores=4 )
precis(m3b, depth=3)




```
